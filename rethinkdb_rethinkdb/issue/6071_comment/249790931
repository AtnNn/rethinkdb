IssueComment
  { issueCommentUpdatedAt = 2016 (-09) (-27) 07 : 55 : 57 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 372365
        , simpleUserLogin = N "analytik"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/372365?v=3"
        , simpleUserUrl = "https://api.github.com/users/analytik"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/249790931"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/6071#issuecomment-249790931"
  , issueCommentCreatedAt = 2016 (-09) (-27) 07 : 47 : 05 UTC
  , issueCommentBody =
      "We got hit by this bug again yesterday pretty hard in production. I tried to add a new read-only node in a different datacenter, but even when I specified new server tags, the new server of course also had the `default` tag, and RethinkDB started copying things there almost immediately. This wasn't a problem, just an incovenience. However, while it was backfilling about 12 tables onto the new server, I tried resharding some tables to have x copies in the main cluster and 1 copy in the new one. This triggered the backfill+traffic bug over and over again, so I started slowly killing nodes one by one, and then decided to stop it by halting the new node completely. This somehow caused a few tables to be unreadable, even when they had all shards available, for example one table had 3 shards, 4 replicas, and one of the replica had statuses like this:\r\n\r\n* replica 1 - primary replica - green\r\n* replica 2 - acting primary replica - green\r\n* replica 3 - acting primary replica - green\r\n* replica 4 - secondary replica - green\r\n\r\nAnd even when all other shards were green, it still wouldn't read until I did emergency repair on that table (it doesn't seem like any data was lost). The potential for data loss is worrying though.\r\n\r\nAlso, at that point, we got hit by a kernel bug on one server, which breaks Docker, so we tried to restart the physical node, but our server provider had some issues, so at this point, one of the regular nodes, rethinkdb2 was down for an extended period, but we still had everything replicated to at least 3:3.\r\n\r\nWhat makes it more peculiar, that during the problematic backfill, one db node started sending 150MB/s (yes, MB), but those were going to PROXIES. All 5 of them - although sometimes only to 4 of them (see graph below). And from proxies, the traffic went.............. nowhere. There literally wasn't any app server that would receive anywhere that amount of data (most were in 100kB/s range). I even killed all services with changefeeds to see if accidentally someone was siphoning that kind of data, and rethinkdb accidentally sent updates when tables was re-sharded, or something like that. Alas, no.\r\n\r\nWhy would a database node send that amount of data to proxies?\r\n\r\nOne of the spikes from yesterday, stopped only when I killed that particular instance sending out the data. Later we just stopped all proxies and routed the traffic directly to avoid this strange behavior.\r\n\r\n![screenshot 2016-09-27 10 34 16](https://cloud.githubusercontent.com/assets/372365/18864239/0b81442e-849e-11e6-8b6f-98f787c93113.png)\r\n![screenshot 2016-09-27 10 34 22](https://cloud.githubusercontent.com/assets/372365/18864240/0b816d96-849e-11e6-9488-b564feae25c9.png)\r\n\r\nUsually after bringing a node up after a downtime it takes only seconds or minutes to backfill everything. Today we let rethinkdb2 rejoin the crowd after being down for ~20h, and it was all peachy after 5 minutes. However, yesterday the backfills were taking hours. RethinkDB reported writes that seemed like a reasonable amount for the hardware - 5~10k writes per second per node, the backfills just took ages.\r\n\r\nIs it possible that it was backfilling data it actually already had? Does RethinkDB have a well-defined behaviour when it's backfilling, and someone reconfigures table at the same time? If I would change a table in a 5-node cluster from something like 5 shards, 5 replicas to 4 shards, 5 replicas, would it copy a lot of data again, or would it be just a matter of voting on new data ranges for the shards? And until it backfills the new 4 shards, will everything be read from the old 5 shard setup? Will it act correctly if I suddenly reconfigure the table again, to say 5 shards, 1 replica?\r\n\r\nEDIT: Running `2.3.5~0jessie (GCC 4.9.2)`.\r\n\r\nEDIT 2: Is it possible to run some health check on metadata of all tables? At one point we even shut down all proxies and notes at the same time to fix any potential in-memory misconfiguration."
  , issueCommentId = 249790931
  }