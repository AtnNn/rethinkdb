IssueComment
  { issueCommentUpdatedAt = 2016 (-08) (-31) 12 : 55 : 19 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 372365
        , simpleUserLogin = N "analytik"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/372365?v=3"
        , simpleUserUrl = "https://api.github.com/users/analytik"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/243754658"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/6071#issuecomment-243754658"
  , issueCommentCreatedAt = 2016 (-08) (-31) 12 : 55 : 19 UTC
  , issueCommentBody =
      "Oops, turns out I was wrong about the \"traffic going nowhere\" part. What I thought was combined in/out traffic was a graph for only outgoing traffic, and we didn't have a graph for incoming traffic, so I fixed that. All the traffic displayed is internal (except for graph (5)), we serve public requests from other nodes.\r\n\r\nWhat I described in the original post was about Friday 2016-08-19, but I think it's more visible from the following day, Saturday 20th. The \"traffic going nowhere\" was actually going from rethinkdb1 to the other four, however there was no need for it - as I said, after a few restarts, things settled down and general query speed was back to normal.\r\n\r\nWe don't have TCP dumps from that time, so it's hard to prove any traffic was between rethinkdb nodes other than correlating and guessing. Also the same physical nodes host rethinkdb proxies and app servers, however we can see from other reports that at least none of our app servers have any kind of significant traffic, likely not more than 10MB/s for all app servers across all nodes combined. I could provide graphs for \"boring\" periods if that would help.\r\n\r\nI don't have a drawing tablet here to annotate the graphs aesthetically, hopefully it will be readable enough:\r\n\r\n![screenshot 2016-08-31 14 45 09](https://cloud.githubusercontent.com/assets/372365/18127610/29120b3c-6f8a-11e6-9f8f-f8644d4b2f49.png)\r\n\r\nGraphs, in order:\r\n1. RethinkDB container memory, as reported by Docker/Kubernetes. Here you can also see when I killed specific rethinkdb servers.\r\n2. Network sent, for the 5 nodes that have rethinkdb servers.\r\n3. Network received, for 5 nodes\r\n4. Busy backends show the cumulative time of responses, per real life second. The green graph is a service receiving a 3rd party feed that is 95% realtime updates.\r\n5. Bytes received from external sources - again, the green graph is the XML coming in from 3rd party, and processing it generally represents 60-80% of the whole cluster load.\r\n6. \"Finished reports\" graph isn't so important, I'm just including it to show that it caused load only once (green line corresponding to left axis, blue is just count of reports finished at any given time - right axis)\r\n7. \"Odds changed\" - number of inserts and updates in the most active table, as reported by a changefeed. I'm not sure why it spiked to ~300k per minute, given how the following graph,\r\n8. \"RethinkDB writes per second\" is based on a RethinkDB Prometheus Exporter. These numbers represent writes per second at the moment of querying - roughly the same numbers one would see in RethinkDB dashboard.\r\n\r\nMy recollection of exact order of things is vague, but\r\n\r\n- from before 11.00 to 12.25, green graph on (2) and cyan on (3), we had a running script that was cleaning up the most used table, active_odds - deleting about 6~10k documents at a time, sleeping 30 seconds.\r\n- at 12.01, a report finished after 6 minutes, which always uses a lot of network traffic. Nothing unusual here.\r\n- at 12.25, I run `table('active_odds').rebalance()` which started the problems (as on the day before)\r\n- The table seemed rebalanced quite quickly, but the 80MB/s sent traffic was the highest we've ever seen in our cluster (pink spike from 12.29 to 12.36 on (2))\r\n- I think 4 broken, halted backfill jobs (for table active_odds) were reported at this point, two of them at 0, some of them at something like 0.16, but not moving\r\n- it ended when I restarted `rethinkdb1`, because the whole site became unusable\r\n- `rethinkdb5` outbound traffic went up right after I restarted `rethinkdb1` (cyan graph on (2))\r\n- I restarted `rethinkdb5` at 12.53, and the outbound traffic went down by a 1/3 - from 37 to 26\r\n- now there were 3 frozen backfill jobs reported, I think they were 5->2, 5->4 and 4->5.\r\n- I restarted `rethinkdb4` at 12.59\r\n- I restarted `rethinkdb2` at 13.01\r\n- two backfill jobs were still there I think\r\n- I restarted `rethinkdb5` again at 13.04\r\n- only now all the backfill jobs disappeared\r\n- from 13.05 onward, the network traffic is low, and site worked well. The \"Busy backends\" never goes higher than 8s of requests per 1 second for the most complex services.\r\n\r\n\r\nHere's a similar graph for Friday 19th August, although I don't dare narrate it. It might be interesting to note `rethinkdb3` growing in memory use rapidly to 40GB at 14.37, in relation to #5865 perhaps. It could be the turnover report running (two started executing at 14.32 and 14.33), or it could be also related to this backfill issue, as I restarted `rethinkdb2` at 14.37, exactly when the `rethinkdb3` memory usage skyrocketed.\r\n\r\n![screenshot 2016-08-31 15 42 14](https://cloud.githubusercontent.com/assets/372365/18129197/536f6c0a-6f92-11e6-85cd-0bd38cfab37c.png)\r\n\r\nAll graphs are from the official 2.3.4~0 debian build, running with the default `cluster-reconnect-timeout` setting, and cache size set to 5120MB (not fully utilized)."
  , issueCommentId = 243754658
  }