IssueComment
  { issueCommentUpdatedAt = 2014 (-03) (-28) 20 : 03 : 18 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 91193
        , simpleUserLogin = N "mglukhovsky"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/91193?v=3"
        , simpleUserUrl = "https://api.github.com/users/mglukhovsky"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/38962110"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/2189#issuecomment-38962110"
  , issueCommentCreatedAt = 2014 (-03) (-28) 20 : 03 : 18 UTC
  , issueCommentBody =
      "Thanks for the bug report! @Tryneus, could you take a look at this?\r\n\r\n> On Mar 28, 2014, at 11:19 AM, Graeme Nordgren <notifications@github.com> wrote:\r\n> \r\n> Tried to dump my test database to upgrade to 1.12, but the dump ran for > 2 days, and then failed as followed:\r\n> \r\n> NOTE: 'rethinkdb-dump' only dumps data and does *not* dump secondary indexes or\r\n>  cluster metadata.  You will need to recreate your secondary indexes and cluster\r\n>  setup yourself after you run 'rethinkdb-restore'.\r\n> Exporting to directory...\r\n> [===================                     ]  48%\r\n> Traceback: [('/usr/lib64/python2.7/site-packages/rethinkdb/_export.py', 281, 'export_table', 'read_table_into_queue(conn, db, table, task_queue, progress_info, exit_event)'), ('/usr/lib64/python2.7/site-packages/rethinkdb/_export.py', 190, 'read_table_into_queue', 'for row in r.db(db).table(table).run(conn, time_format=\"raw\"):'), ('/usr/lib64/python2.7/site-packages/rethinkdb/net.py', 48, '__iter__', 'self.conn._continue_cursor(self)'), ('/usr/lib64/python2.7/site-packages/rethinkdb/net.py', 207, '_continue_cursor', 'self._async_continue_cursor(cursor)'), ('/usr/lib64/python2.7/site-packages/rethinkdb/net.py', 216, '_async_continue_cursor', 'self._send_query(query, cursor.term, cursor.opts, async=True)'), ('/usr/lib64/python2.7/site-packages/rethinkdb/net.py', 293, '_send_query', 'self._sock_sendall(query_header + query_protobuf)'), ('/usr/lib64/python2.7/site-packages/rethinkdb/net.py', 165, '_sock_sendall', 'return self.socket.sendall(data)'), ('/usr/lib64/python2.7/socket.py', 224\r\n>  , 'meth', 'return getattr(self._sock,name)(*args)')]\r\n> error: [Errno 110] Connection timed out\r\n> Errors occurred during export\r\n> Error: rethinkdb-export failed\r\n> The bulk of the data is in a single 400GB table. I watched IO rates during the dump, and it was consistently doing well under 1 MB / sec. Per IRC, I confirmed with @atnnn that I'm already running the C++ protobuf library:\r\n> \r\n> $ python2.7 -c 'import rethinkdb; print rethinkdb.protobuf_implementation'\r\n> cpp\r\n> \8212\r\n> Reply to this email directly or view it on GitHub."
  , issueCommentId = 38962110
  }