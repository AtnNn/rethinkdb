IssueComment
  { issueCommentUpdatedAt = 2013 (-08) (-27) 23 : 29 : 20 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 583919
        , simpleUserLogin = N "underrun"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/583919?v=3"
        , simpleUserUrl = "https://api.github.com/users/underrun"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/23379562"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1368#issuecomment-23379562"
  , issueCommentCreatedAt = 2013 (-08) (-27) 23 : 29 : 20 UTC
  , issueCommentBody =
      "when you say \"which this could be used to optimize\", that actually depends on what you are already doing to implement groupedMapReduce. when i say \"fields that benefit from an index\" i mean in other queries that use an index. if a grouped map reduce is like a r.table.getAll(value, {index: 'idx'}).map().reduce() for all possible values of index 'idx', then it seems like indexes would help every groupedMapReduce where the grouping function was just returning a field that is already indexed... if it's more like actual hadoopish mapreduce then it really wouldn't matter since you would be processing the entire table.\r\n\r\nmostly i've just been doing counts of distinct values of fields per index value. "
  , issueCommentId = 23379562
  }