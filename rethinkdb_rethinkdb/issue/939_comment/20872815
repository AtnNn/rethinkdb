IssueComment
  { issueCommentUpdatedAt = 2013 (-07) (-12) 12 : 10 : 05 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 48436
        , simpleUserLogin = N "coffeemug"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/48436?v=3"
        , simpleUserUrl = "https://api.github.com/users/coffeemug"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/20872815"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/939#issuecomment-20872815"
  , issueCommentCreatedAt = 2013 (-07) (-12) 12 : 09 : 29 UTC
  , issueCommentBody =
      "Awesome! A couple of questions:\r\n\r\n* Is the throughput problem coming from the fact that we're copying things we used to not copy before, or from the fact that copying that was done on the buffer cache threads before is now being done on the serializer thread? If we do copying we didn't do before, what is it? Can we eliminate it?\r\n* Do we still allocate 4KB blocks in RAM despite writing smaller blocks to disk?\r\n"
  , issueCommentId = 20872815
  }