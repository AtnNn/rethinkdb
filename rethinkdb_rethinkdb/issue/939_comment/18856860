IssueComment
  { issueCommentUpdatedAt = 2013 (-06) (-03) 17 : 26 : 50 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 258437
        , simpleUserLogin = N "srh"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/258437?v=3"
        , simpleUserUrl = "https://api.github.com/users/srh"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/18856860"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/939#issuecomment-18856860"
  , issueCommentCreatedAt = 2013 (-06) (-03) 17 : 26 : 50 UTC
  , issueCommentBody =
      "We were talking about this in the office and came to the conclusion that instead the serializer (and buffer cache) should support variable-sized blocks, that is, any block size less than 4096 or some other fixed value that's small relative to extent size.\r\n\r\nThis solves the underused blob problem and also makes more feasible other easy disk usage optimizations such as gzip compression of blocks, or more mundane truncation of underfull btree blocks.\r\n\r\nChanging the MAX_IN_NODE_VALUE_SIZE to something larger than half a block would make btree leveling/splitting/merging logic more complicated -- insertions would now have to accept creating underfull nodes in certain circumstances, without trying to level.  And it would have worse performance, disk-usage-wise, than having the serializer support arbitrary block sizes."
  , issueCommentId = 18856860
  }