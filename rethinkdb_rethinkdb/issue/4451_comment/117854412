IssueComment
  { issueCommentUpdatedAt = 2015 (-07) (-01) 23 : 47 : 27 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/117854412"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/4451#issuecomment-117854412"
  , issueCommentCreatedAt = 2015 (-07) (-01) 23 : 47 : 27 UTC
  , issueCommentBody =
      "@sebadiaz We're still a bit unsure what's happening in your case. Is it a single document that's more than 200 MB or is that the total file size? The thing is that there's a 64 MB limit on the size of data that can be imported into a table in a single query, so we would expect the import of such a document to fail.\r\nTheoretically if the JSON encoded document has a lot of whitespaces in it (e.g. if it's indented), that might explain why a much larger JSON file can still be imported though.\r\n\r\nIn general I would still prefer a solution that doesn't add additional parameters to the import tool though."
  , issueCommentId = 117854412
  }