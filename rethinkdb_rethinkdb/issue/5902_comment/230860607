IssueComment
  { issueCommentUpdatedAt = 2016 (-07) (-06) 18 : 22 : 46 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/230860607"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/5902#issuecomment-230860607"
  , issueCommentCreatedAt = 2016 (-07) (-06) 18 : 22 : 46 UTC
  , issueCommentBody =
      "@ttmc \"the total data set\" here is referring to only the data stored on the particular server.\r\nHence 10 GB of RAM spread across the cluster are enough to operate on a Terabyte of data, as long as the data is not replicated. If you replicate the data, the amount of data per server increases accordingly, because multiple copies of the same data will be held by different servers in the cluster.\r\n\r\nFor reasonable performance, you should probably aim at something closer to 5-10% of the data size. The 1% is the bare minimum and doesn't include any caching. If you want to run near the minimum, you'll also need to manually lower RethinkDB's cache size through the `--cache-size` parameter to free up enough RAM for the metadata overhead (or by adding `cache-size=<size in MB>` to the configuration file in /etc/rethinkdb/instances.d if you're starting RethinkDB through the init script)."
  , issueCommentId = 230860607
  }