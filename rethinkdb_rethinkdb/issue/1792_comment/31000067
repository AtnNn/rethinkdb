IssueComment
  { issueCommentUpdatedAt = 2013 (-12) (-20) 10 : 10 : 19 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 48436
        , simpleUserLogin = N "coffeemug"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/48436?v=3"
        , simpleUserUrl = "https://api.github.com/users/coffeemug"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/31000067"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1792#issuecomment-31000067"
  , issueCommentCreatedAt = 2013 (-12) (-20) 10 : 10 : 19 UTC
  , issueCommentBody =
      "> 1) Are rethinkdb admin commands idempotent?  (as we are shelling out, it's less reliable, so are we able to just re-execute the script if it fails?)\r\n\r\nIt depends on the command. For setting the replication count -- yes. For others, we can discuss it command by command if you'd like (just ask).\r\n\r\n> 2) I can't see a few things from the documentation from --help ... is there fuller documentation anywhere? In the meantime ...\r\n\r\nNot at the moment. Please open a github issue if there is specific info missing in `help`.\r\n\r\n> - The admin commands all seem to take a Table ... but not a Database to qualify the table with ... is there a way to specify which Table or which Database?\r\n\r\nThe admin tool predates the grouping by databases, so it currently resolves name conflicts by forcing the user to specify IDs. If you have two tables `foo`, one in database `test`, and another in database `test2`, here is an example session:\r\n\r\n```\r\nlocalhost:29015> set replicas foo 2\r\n'foo' not unique, possible objects:\r\ntable (r) 114a1e95\r\ntable (r) af9388f3\r\n\r\nlocalhost:29015> set replicas af9388f3 2\r\nthe number of replicas cannot be more than the number of machines in the datacenter\r\n```\r\n\r\nYou can check which table belongs to which database as follows:\r\n\r\n```\r\nlocalhost:29015> ls tables --long\r\nuuid                                  name  shards  replicas  primary                               database                              durability\r\n114a1e95-4356-4a55-a10c-4313295b30b1  foo   1       1         00000000-0000-0000-0000-000000000000  332a67d4-2f84-43a7-9f5c-f13ef2fb7fee  hard\r\naf9388f3-5bf7-4ae2-90f8-e124380efa73  foo   1       1         00000000-0000-0000-0000-000000000000  25d4bbb9-1539-4fee-a446-a1ff795f674a  hard\r\n```\r\n\r\nThis is definitely less than ideal and we'll provide better tools.\r\n\r\n> - Can sharding be done via rethinkdb admin ?   (I cant see a way, I may be missing it)\r\n\r\nYes, run `help split` and `help merge` to get more information (also `help pin` for pinning shards to specific machines). Current to split shards in the admin CLI you have to explicitly specify a split point (i.e. an primary key prefix to split on). You can't just specify the number of shards like you can in the web UI. The manual split point functionality will go away once #364 is done, so you'll be able to specify the number of shards like you can in the web UI.\r\n\r\n\r\n> For a production system these are usually the first questions asked (at least by an ops team), so some viable options really need to be in place now.\r\n\r\nI understand. Your feedback is invaluable. We're working as fast as we can to fix all these issues, but we have to work within the confines of our resources.\r\n\r\nI agree with most of your suggestions, with two caveats:\r\n\r\n* We can't automatically default to multiple replicas, because people often start with a single node. One thing we can do, is add the option to specify the number of replicas directly to the `tableCreate` command (or add a syntax like `r.table('foo').setReplicas(5)`). How does that sound?\r\n* Similarly, I don't think automatically sharding to all machines in the primary datacenter is a good idea. Sharding isn't free -- if you automatically add a shard every time a user adds a machine, there is an associated cost in the system. Similarly, you might want to have, say two machines per table to spread the load across your cluster. What about something like `r.table('foo').setShards(5)`?"
  , issueCommentId = 31000067
  }