IssueComment
  { issueCommentUpdatedAt = 2013 (-12) (-20) 05 : 04 : 50 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 371348
        , simpleUserLogin = N "jakcharlton"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/371348?v=3"
        , simpleUserUrl = "https://api.github.com/users/jakcharlton"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/30989155"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1792#issuecomment-30989155"
  , issueCommentCreatedAt = 2013 (-12) (-20) 05 : 04 : 50 UTC
  , issueCommentBody =
      "That's a high end use case?\r\n\r\nThis is basic redundancy, no table should ever be created that does not\r\nreplicate to your disaster recover cluster/data centre immediately\r\n\r\nI can understand (a little more) not making sharding automatic (though even\r\nthis is a fairly standard use case) - but replicas are disaster recovery,\r\nmaking this a manual process is unviable in a production system\r\n\r\nIt takes half the advantages of a NoSQL database and disregards them -\r\nmaking you effectively design and manage your schema up front\r\n\r\nThe focus on things as being at table level, means you could end up\r\nmanaging hundreds, if not thousands of 'things'\r\n\r\nThis problem is further compounded by the fact that the Web UI is not\r\ncapable of clearly telling you which servers are dealing with which tables\r\nand which tables are not replicated\r\n\r\n\r\nIs there any way of mitigating this? Or managing this?  Even such a simple\r\nthing as 'replicate Database' (which then replicates all the tables) would\r\nmassively improve the current situation.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nOn Fri, Dec 20, 2013 at 3:19 PM, coffeemug <notifications@github.com> wrote:\r\n\r\n> You're right that this currently isn't possible (or at least isn't easy)\r\n> in RethinkDB. This will eventually be fixed, but there is a really good\r\n> reason for the way things are right now.\r\n>\r\n> Designing an automatically managed sharding/replication system is\r\n> notoriously difficult. There is no perfect system that will work 100% of\r\n> the time without human intervention. So we decided to design a lower level\r\n> system that will do sharding and replication on command, and expose a\r\n> simple way of using it to users.\r\n>\r\n> Now that this system is in place (and getting more mature in preparation\r\n> for the LTS release) we'll soon be in a position to write a higher level\r\n> component to automate some of these operations. There are multiple ways to\r\n> do it -- for example, we can offer settings on a database level, or let an\r\n> administrator set general metrics (i.e. shard a table if some conditions\r\n> are met, etc.) However, this functionality is likely six months away (or\r\n> more).\r\n>\r\n> In the meantime, you can shard and replicate programmatically by using rethinkdb\r\n> admin. You can write scripts to perform sharding and replication\r\n> operations when certain conditions are met. We don't make it easy (for\r\n> example, there is currently no well documented monitoring API -- see #1392<https://github.com/rethinkdb/rethinkdb/issues/1392>),\r\n> but it can be done with some effort.\r\n>\r\n> This is bubbling up against high end use cases, and unfortunately Rethink\r\n> isn't quite ready for that. If I had to guess, I'd say this will be\r\n> bulletproof about a year from now.\r\n>\r\n> Moving to backlog.\r\n>\r\n> \8212\r\n> Reply to this email directly or view it on GitHub<https://github.com/rethinkdb/rethinkdb/issues/1792#issuecomment-30988043>\r\n> .\r\n>"
  , issueCommentId = 30989155
  }