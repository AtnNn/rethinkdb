IssueComment
  { issueCommentUpdatedAt = 2013 (-12) (-20) 22 : 38 : 33 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 371348
        , simpleUserLogin = N "jakcharlton"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/371348?v=3"
        , simpleUserUrl = "https://api.github.com/users/jakcharlton"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/31047080"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1792#issuecomment-31047080"
  , issueCommentCreatedAt = 2013 (-12) (-20) 22 : 38 : 33 UTC
  , issueCommentBody =
      "Thanks for the updates - really appreciated ...\r\n\r\n\"If you have two tables foo, one in database test, and another in database\r\ntest2, here is an example session:\"\r\n\r\nDoes this prevent running the whole command from the shell? It seems to,\r\nespecially with the commands required to figure out what table is in which\r\nDB\r\n\r\nThe need to use the shell already means my web server will need to call a\r\nweb service on the database server, for it to then shell out to the admin\r\ntool - but again in my use case, there is no administrator interaction, so\r\nI can't run queries to get database IDs and then use those as far as I can\r\ntell. Can these IDs be found from the client ReQL ?\r\n\r\n\r\n\"One thing we can do, is add the option to specify the number of replicas\r\ndirectly to the tableCreate command (or add a syntax like\r\nr.table('foo').setReplicas(5)). How does that sound?\"\r\n\r\nThat's actually a pretty good idea, but it seems you also need to be able\r\nto identify datacentres/clusters as Replicas - so that when you setup your\r\nEast and West coast datacentres/clusters, you can tell them they are\r\nmasters or slaves, so that the '5' replicas get put in the right location\r\n\r\nQuick question here: You put '5' replicas - which would seem excessive. It\r\nmay be a random number you picked, but generally replicas are disaster\r\nrecovery, so you more or less need just one - is there a reason you chose\r\n5, like replicas in Rethink being actively queried?\r\n\r\n\r\n\"Sharding isn't free -- if you automatically add a shard every time a user\r\nadds a machine,\"\r\n\r\nThe general reason for adding another machine would be that your load needs\r\nto be spread. If you had say two machines, and both were under excessive\r\nload (say 50% utilisation), you might add two more machines.\r\n\r\nNow we have (in this example) 500 tables on the first two instances - and\r\nnothing on the other two.  We now have a manual task to go into 500 tables\r\nand tell them to use the other two boxes. So the first part of the problem\r\nis that the Web UI really needs a one-click 'add this server to the cluster\r\nand automatically rebalance all tables', which I would guess would be the\r\ndefault for 90% of use cases  (even if you have just 5 badly behaving\r\ntables, you dont care about sharding the other 495 for the one off cost to\r\nrebalance, especailly as load for them will be spread after)\r\n\r\n\r\n\r\n\r\nThis does bring up the other possible reason for sharding or replication in\r\nother databases, geolocation .... where I want some web servers in Sydney,\r\nsome in US-West, and some in Europe - each with their own associated local\r\ndatabase servers, all synchronised for performance reasons, but only\r\nquerying locally except in case of outages.  How does Rethink handle this?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nOn Fri, Dec 20, 2013 at 9:10 PM, coffeemug <notifications@github.com> wrote:\r\n\r\n> 1) Are rethinkdb admin commands idempotent? (as we are shelling out, it's\r\n> less reliable, so are we able to just re-execute the script if it fails?)\r\n>\r\n> It depends on the command. For setting the replication count -- yes. For\r\n> others, we can discuss it command by command if you'd like (just ask).\r\n>\r\n> 2) I can't see a few things from the documentation from --help ... is\r\n> there fuller documentation anywhere? In the meantime ...\r\n>\r\n> Not at the moment. Please open a github issue if there is specific info\r\n> missing in help.\r\n>\r\n>\r\n>    - The admin commands all seem to take a Table ... but not a Database\r\n>    to qualify the table with ... is there a way to specify which Table or\r\n>    which Database?\r\n>\r\n>  The admin tool predates the grouping by databases, so it currently\r\n> resolves name conflicts by forcing the user to specify IDs. If you have two\r\n> tables foo, one in database test, and another in database test2, here is\r\n> an example session:\r\n>\r\n> localhost:29015> set replicas foo 2\r\n> 'foo' not unique, possible objects:\r\n> table (r) 114a1e95\r\n> table (r) af9388f3\r\n>\r\n> localhost:29015> set replicas af9388f3 2\r\n> the number of replicas cannot be more than the number of machines in the datacenter\r\n>\r\n> You can check which table belongs to which database as follows:\r\n>\r\n> localhost:29015> ls tables --long\r\n> uuid                                  name  shards  replicas  primary                               database                              durability\r\n> 114a1e95-4356-4a55-a10c-4313295b30b1  foo   1       1         00000000-0000-0000-0000-000000000000  332a67d4-2f84-43a7-9f5c-f13ef2fb7fee  hard\r\n> af9388f3-5bf7-4ae2-90f8-e124380efa73  foo   1       1         00000000-0000-0000-0000-000000000000  25d4bbb9-1539-4fee-a446-a1ff795f674a  hard\r\n>\r\n> This is definitely less than ideal and we'll provide better tools.\r\n>\r\n>\r\n>    - Can sharding be done via rethinkdb admin ? (I cant see a way, I may\r\n>    be missing it)\r\n>\r\n>  Yes, run help split and help merge to get more information (also help pinfor pinning shards to specific machines). Current to split shards in the\r\n> admin CLI you have to explicitly specify a split point (i.e. an primary key\r\n> prefix to split on). You can't just specify the number of shards like you\r\n> can in the web UI. The manual split point functionality will go away once\r\n> #364 <https://github.com/rethinkdb/rethinkdb/issues/364> is done, so\r\n> you'll be able to specify the number of shards like you can in the web UI.\r\n>\r\n> For a production system these are usually the first questions asked (at\r\n> least by an ops team), so some viable options really need to be in place\r\n> now.\r\n>\r\n> I understand. Your feedback is invaluable. We're working as fast as we can\r\n> to fix all these issues, but we have to work within the confines of our\r\n> resources.\r\n>\r\n> I agree with most of your suggestions, with two caveats:\r\n>\r\n>    - We can't automatically default to multiple replicas, because people\r\n>    often start with a single node. One thing we can do, is add the option to\r\n>    specify the number of replicas directly to the tableCreate command (or\r\n>    add a syntax like r.table('foo').setReplicas(5)). How does that sound?\r\n>    - Similarly, I don't think automatically sharding to all machines in\r\n>    the primary datacenter is a good idea. Sharding isn't free -- if you\r\n>    automatically add a shard every time a user adds a machine, there is an\r\n>    associated cost in the system. Similarly, you might want to have, say two\r\n>    machines per table to spread the load across your cluster. What about\r\n>    something like r.table('foo').setShards(5)?\r\n>\r\n> \8212\r\n> Reply to this email directly or view it on GitHub<https://github.com/rethinkdb/rethinkdb/issues/1792#issuecomment-31000067>\r\n> .\r\n>"
  , issueCommentId = 31047080
  }