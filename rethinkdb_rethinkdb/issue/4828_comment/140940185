IssueComment
  { issueCommentUpdatedAt = 2015 (-09) (-17) 01 : 35 : 26 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/140940185"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/4828#issuecomment-140940185"
  , issueCommentCreatedAt = 2015 (-09) (-17) 01 : 35 : 26 UTC
  , issueCommentBody =
      "@stephanbuys The general rule of thumb for the memory overhead at the moment is that you'll need 5% of your total data size times the replication factor to be available as RAM in your cluster.\r\n\r\nIf for example you're storing 1 TB of data, and you want to replicate your data to two servers, you should have a total of 50*2=100 GB of RAM available in your cluster. For example four servers with at least 25 GB each, or two servers with at least 50 GB each.\r\n(We're soon going to reduce this requirement a bit.)\r\n\r\nFor analytics data specifically, I strongly recommend using SSD storage rather than rotational drives. Analytical queries can become very slow in RethinkDB when run against data stored on rotational drives."
  , issueCommentId = 140940185
  }