IssueComment
  { issueCommentUpdatedAt = 2016 (-07) (-29) 21 : 35 : 10 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/236299597"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/6015#issuecomment-236299597"
  , issueCommentCreatedAt = 2016 (-07) (-29) 21 : 35 : 10 UTC
  , issueCommentBody =
      "This isn't an issue. When you want to store data that is too large to have its size fit into a 64 bit integer you're going to have a whole set of other problems, and things will fail long long before you get anywhere near the blob code.\r\n\r\nFor one thing you can't address this much memory on a 64 bit computer. Even if you could, file access APIs in today's operating systems use 64 bit offsets, so you couldn't access a file like this either.\r\n\r\nRethinkDB rejects queries that are larger than 128 MB. You can write queries that generate data that is larger than this, but those queries would run out of RAM way before you hit the limits of a 64 bit integer."
  , issueCommentId = 236299597
  }