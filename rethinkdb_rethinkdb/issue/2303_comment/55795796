IssueComment
  { issueCommentUpdatedAt = 2014 (-09) (-16) 19 : 01 : 57 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 495718
        , simpleUserLogin = N "bitterjug"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/495718?v=3"
        , simpleUserUrl = "https://api.github.com/users/bitterjug"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/55795796"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/2303#issuecomment-55795796"
  , issueCommentCreatedAt = 2014 (-09) (-16) 19 : 01 : 57 UTC
  , issueCommentBody =
      "I agree; it would be indistinguishable from magic.\r\nOn 16 Sep 2014 19:41, \"roncemer\" <notifications@github.com> wrote:\r\n\r\n> One area where Riak really falls flat, and RethinkDB does as well, is in\r\n> processing HUGE amounts of data in a query. I'm talking potentially\r\n> hundreds of millions of objects, each with nested arrays of child objects,\r\n> where the query iterates over the objects for a range of keys, and for each\r\n> object, iterates over the child objects within that object, to form a\r\n> result set.\r\n>\r\n> The limitation of array sizes to 100,000 isn't going to work for\r\n> real-world number-crunching applications such as this.\r\n>\r\n> RethinkDB needs something like what MySQL has, where if the size of a\r\n> result set is too large to fit into memory, it puts it on disk in a\r\n> temporary file (temp table) and works on it there. RethinkDB's MapReduce\r\n> implementation could especially benefit from this.\r\n>\r\n> My real-world use case: Some 15M objects going into Riak (or RethinkDB)\r\n> per day, each with between 1 and 500 child objects within them. The query\r\n> aggregates child objects and comes up with counts, grouped by some common\r\n> attributes of the child objects, for a 7-day, 3-day, and 1-day ranges. The\r\n> keys are date/time-prefixed, so scanning a date range is easy.\r\n>\r\n> If RethinkDB could handle this massive query, do proper MapReduce and\r\n> aggregation, and return the results, all without complaining about arrays\r\n> being too large, running out of memory, or producing any errors, then it\r\n> would definitely be ahead of the game compared to almost every other\r\n> solution out there.\r\n>\r\n> An example object in the table:\r\n> [\r\n> {\r\n> \"ems\": [\r\n> {\r\n> \"k\": 8 ,\r\n> \"kc\": 2321 ,\r\n> \"kf\": 5656 ,\r\n> \"ks\": 8863 ,\r\n> \"md5\": \"aa1fc5490cbca4a3aa6b6e450952a0c3\" ,\r\n> \"mdom\": \"example.com\"\r\n> }\r\n> ] ,\r\n> \"id\": \"2014-08-28 00:00:00 53feb750a581e2a953feb7500ce856e5\" ,\r\n> \"pc\": 934 ,\r\n> \"rtm\": \"2014-08-28 00:00:00\" ,\r\n> \"urid\": \"53feb750a581e2a953feb7500ce856e5\"\r\n> }\r\n> ]\r\n>\r\n> Note how the key (id) is prefixed with the date/time, which makes it easy\r\n> to do range scans.\r\n>\r\n> My query (covers one hour only, so RethinkDB won't blow up; should be able\r\n> to cover up to 7 days):\r\n> r.db('ecapi').table('ecapilog').between('2014-08-28 00:00:00', '2014-08-28\r\n> 01:00:00')\r\n> .concatMap(function(obj) {\r\n> return obj('ems').merge({pc:obj('pc'), nems:1});\r\n> })\r\n> .group('pc', 'mdom', 'k', 'kc', 'ks', 'kf')\r\n> .sum('nems')\r\n>\r\n> Typical query result:\r\n> [\r\n> {\r\n> \"group\": [\r\n> 1 ,\r\n> \"example.com\" ,\r\n> 17 ,\r\n> 111 ,\r\n> 8793 ,\r\n> 4699\r\n> ] ,\r\n> \"reduction\": 1\r\n> } ,\r\n> {\r\n> \"group\": [\r\n> 1 ,\r\n> \"example.com\" ,\r\n> 25 ,\r\n> 29 ,\r\n> 1366 ,\r\n> 5231\r\n> ] ,\r\n> \"reduction\": 3\r\n> } ,\r\n> ...\r\n> ]\r\n>\r\n> If I try to run the query for 4 hours' time rage, RethinkDB blows up. The\r\n> number of objects in the table should be inconsequential.\r\n>\r\n> RethinkDB should properly handle arrays of any size, using temporary\r\n> tables on disk where needed to avoid generating out-of-memory errors or\r\n> exceeding its in-memory maximum array size.\r\n>\r\n> If you can get this right, you'll be ahead of the game in the querying and\r\n> MapReduce department, compared to nearly every other distributed k/v store\r\n> out there.\r\n>\r\n> \8212\r\n> Reply to this email directly or view it on GitHub\r\n> <https://github.com/rethinkdb/rethinkdb/issues/2303#issuecomment-55792399>\r\n> .\r\n>"
  , issueCommentId = 55795796
  }