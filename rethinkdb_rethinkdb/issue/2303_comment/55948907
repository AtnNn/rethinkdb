IssueComment
  { issueCommentUpdatedAt = 2014 (-09) (-17) 19 : 44 : 07 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 1461947
        , simpleUserLogin = N "neumino"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/1461947?v=3"
        , simpleUserUrl = "https://api.github.com/users/neumino"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/55948907"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/2303#issuecomment-55948907"
  , issueCommentCreatedAt = 2014 (-09) (-17) 19 : 44 : 07 UTC
  , issueCommentBody =
      "You can change the array limit by passing an option to run, but that will\r\nnot really solve your problem I think.\r\n\r\n\r\nIncremental map reduce operations is probably what you want. They are\r\nscheduled for 1.16 (1.15 is going out probably next week).\r\nhttps://github.com/rethinkdb/rethinkdb/issues/1118\r\n\r\n\r\n\r\nYou seem to be dealing with time series data. In this case you can have a\r\ntable with all your raw data (like logs), and have a table of aggregated\r\nresults.\r\nI wrote this gist earlier for someone on IRC, and it probably applies to\r\nyour use case too:\r\nhttps://gist.github.com/neumino/8105b19497b904c4814d\r\n\r\n\r\nBasically you have to tables, one table \"log\", and one table \"aggregated\".\r\nEverytime you save a log, it's going to push a notification of the feed\r\nthat will update the relevant aggregated results.\r\nThe gist just keep track of the number of logs per day, but you could keep\r\ntrack of anything you want. It's not\r\n\r\nFeeds currently do not provide a strong guarantee (like if your server goes\r\ndown). But if it does, you can just run an aggregated query on the logs\r\nfrom today, overwrite the results in \"aggregated\", open your feed, and\r\nresume your inserts.\r\n\r\n\r\n\r\n\r\nLet me know if it helps,\r\n\r\n\r\n\r\nMichel\r\n\r\n2014-09-16 11:41 GMT-07:00 roncemer <notifications@github.com>:\r\n\r\n> One area where Riak really falls flat, and RethinkDB does as well, is in\r\n> processing HUGE amounts of data in a query. I'm talking potentially\r\n> hundreds of millions of objects, each with nested arrays of child objects,\r\n> where the query iterates over the objects for a range of keys, and for each\r\n> object, iterates over the child objects within that object, to form a\r\n> result set.\r\n>\r\n> The limitation of array sizes to 100,000 isn't going to work for\r\n> real-world number-crunching applications such as this.\r\n>\r\n> RethinkDB needs something like what MySQL has, where if the size of a\r\n> result set is too large to fit into memory, it puts it on disk in a\r\n> temporary file (temp table) and works on it there. RethinkDB's MapReduce\r\n> implementation could especially benefit from this.\r\n>\r\n> My real-world use case: Some 15M objects going into Riak (or RethinkDB)\r\n> per day, each with between 1 and 500 child objects within them. The query\r\n> aggregates child objects and comes up with counts, grouped by some common\r\n> attributes of the child objects, for a 7-day, 3-day, and 1-day ranges. The\r\n> keys are date/time-prefixed, so scanning a date range is easy.\r\n>\r\n> If RethinkDB could handle this massive query, do proper MapReduce and\r\n> aggregation, and return the results, all without complaining about arrays\r\n> being too large, running out of memory, or producing any errors, then it\r\n> would definitely be ahead of the game compared to almost every other\r\n> solution out there.\r\n>\r\n> An example object in the table:\r\n> [\r\n> {\r\n> \"ems\": [\r\n> {\r\n> \"k\": 8 ,\r\n> \"kc\": 2321 ,\r\n> \"kf\": 5656 ,\r\n> \"ks\": 8863 ,\r\n> \"md5\": \"aa1fc5490cbca4a3aa6b6e450952a0c3\" ,\r\n> \"mdom\": \"example.com\"\r\n> }\r\n> ] ,\r\n> \"id\": \"2014-08-28 00:00:00 53feb750a581e2a953feb7500ce856e5\" ,\r\n> \"pc\": 934 ,\r\n> \"rtm\": \"2014-08-28 00:00:00\" ,\r\n> \"urid\": \"53feb750a581e2a953feb7500ce856e5\"\r\n> }\r\n> ]\r\n>\r\n> Note how the key (id) is prefixed with the date/time, which makes it easy\r\n> to do range scans.\r\n>\r\n> My query (covers one hour only, so RethinkDB won't blow up; should be able\r\n> to cover up to 7 days):\r\n> r.db('ecapi').table('ecapilog').between('2014-08-28 00:00:00', '2014-08-28\r\n> 01:00:00')\r\n> .concatMap(function(obj) {\r\n> return obj('ems').merge({pc:obj('pc'), nems:1});\r\n> })\r\n> .group('pc', 'mdom', 'k', 'kc', 'ks', 'kf')\r\n> .sum('nems')\r\n>\r\n> Typical query result:\r\n> [\r\n> {\r\n> \"group\": [\r\n> 1 ,\r\n> \"example.com\" ,\r\n> 17 ,\r\n> 111 ,\r\n> 8793 ,\r\n> 4699\r\n> ] ,\r\n> \"reduction\": 1\r\n> } ,\r\n> {\r\n> \"group\": [\r\n> 1 ,\r\n> \"example.com\" ,\r\n> 25 ,\r\n> 29 ,\r\n> 1366 ,\r\n> 5231\r\n> ] ,\r\n> \"reduction\": 3\r\n> } ,\r\n> ...\r\n> ]\r\n>\r\n> If I try to run the query for 4 hours' time rage, RethinkDB blows up. The\r\n> number of objects in the table should be inconsequential.\r\n>\r\n> RethinkDB should properly handle arrays of any size, using temporary\r\n> tables on disk where needed to avoid generating out-of-memory errors or\r\n> exceeding its in-memory maximum array size.\r\n>\r\n> If you can get this right, you'll be ahead of the game in the querying and\r\n> MapReduce department, compared to nearly every other distributed k/v store\r\n> out there.\r\n>\r\n> \8212\r\n> Reply to this email directly or view it on GitHub\r\n> <https://github.com/rethinkdb/rethinkdb/issues/2303#issuecomment-55792399>\r\n> .\r\n>"
  , issueCommentId = 55948907
  }