IssueComment
  { issueCommentUpdatedAt = 2014 (-03) (-02) 05 : 20 : 51 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/36446553"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1861#issuecomment-36446553"
  , issueCommentCreatedAt = 2014 (-03) (-02) 05 : 20 : 51 UTC
  , issueCommentBody =
      "Hi @wiremine,\r\n\r\nthe theoretic overhead for changing the cluster configuration (e.g. creating a table, resharding etc.) grows in O(n^2 * s) where n is the number of nodes in the cluster, and s is the number of shards (which is roughly equivalent to the number of tables).\r\n\r\nUnless you have a very large number of nodes, the overhead for an additional table is usually higher than the one for an additional node though (the quadratic component kicks in much later).\r\n\r\nIf you have a small number of tables (say less than 10), scaling to more than 100 nodes is probably possible with RethinkDB version 1.12, which we will soon release.\r\nI'm saying probably because we have not actually tested clusters with more than 64 nodes. We plan to do these tests at some point, but it might have to wait for the LTS release a couple of months from now."
  , issueCommentId = 36446553
  }