IssueComment
  { issueCommentUpdatedAt = 2013 (-06) (-17) 07 : 58 : 40 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 646357
        , simpleUserLogin = N "wmrowan"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/646357?v=3"
        , simpleUserUrl = "https://api.github.com/users/wmrowan"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/19530861"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/897#issuecomment-19530861"
  , issueCommentCreatedAt = 2013 (-06) (-17) 07 : 58 : 40 UTC
  , issueCommentBody =
      "@fuwaneko Thanks for your work! From what I can tell there's basically an obvious slow algorithm for protobuf serialization and a non-obvious fast algorithm. I'm reasonably convinced that a properly implemented JS only protobuf serializer could be quite fast but wrapping the C++ protobuf lib should be a gold standard for node. We can rip out closure entirely once we have a new protobuf implementation in place but it will have to be a pure JS one to support the web UI.\r\n\r\nThe 4 byte offsets you see exist because both requests and responses are prefixed with a 4 byte little endian length field so that we know... and @coffeemug just beat me to it. Every time man, every time."
  , issueCommentId = 19530861
  }