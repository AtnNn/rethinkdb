Issue
  { issueClosedAt = Just 2014 (-01) (-23) 20 : 43 : 44 UTC
  , issueUpdatedAt = 2014 (-02) (-13) 05 : 22 : 12 UTC
  , issueEventsUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/1820/events"
  , issueHtmlUrl =
      Just "https://github.com/rethinkdb/rethinkdb/issues/1820"
  , issueClosedBy = Nothing
  , issueLabels =
      [ IssueLabel
          { labelColor = "e102d8"
          , labelUrl =
              "https://api.github.com/repos/rethinkdb/rethinkdb/labels/tp:bug"
          , labelName = "tp:bug"
          }
      , IssueLabel
          { labelColor = "444444"
          , labelUrl =
              "https://api.github.com/repos/rethinkdb/rethinkdb/labels/tp:performance"
          , labelName = "tp:performance"
          }
      ]
  , issueNumber = 1820
  , issueAssignee =
      Just
        SimpleUser
          { simpleUserId = Id 505365
          , simpleUserLogin = N "danielmewes"
          , simpleUserAvatarUrl =
              "https://avatars.githubusercontent.com/u/505365?v=3"
          , simpleUserUrl = "https://api.github.com/users/danielmewes"
          , simpleUserType = OwnerUser
          }
  , issueUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueTitle = "Extremely slow gets during insert workload"
  , issuePullRequest = Nothing
  , issueUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/1820"
  , issueCreatedAt = 2014 (-01) (-06) 19 : 09 : 29 UTC
  , issueBody =
      Just
        "@underrun reports simple point gets taking up to 6 minutes. However this is not consistent, apparently the time for a get varies between 16ms and 6 minutes.\r\nHe has an insert workload running in the background.\r\n\r\nI'll try to reproduce it. Only idea I have out of my head is that it is some kind of locking issue, or starvation issue.\r\n\r\nHere is the IRC log for details:\r\n```\r\n12:41 <underrun> hey guys ... i just did a .get on a table with about 80M docs and it took almost 6 minutes ...\r\n12:43 <underrun> the swing is 16ms to 6 minutes ... which seems pretty bad\r\n13:38 <neumino> Did it go in swap?\r\n13:38 <underrun> no\r\n13:39 <underrun> i have swappiness set to 0\r\n13:39 <underrun> and 100GB of free ram\r\n13:39 <underrun> rethink is using ~160 GB\r\n13:40 <underrun> i have 4 shards running on the same box assigned to my 4 different numa nodes\r\n13:41 <danielmewes> underrun: Just to clarify: By .get you mean a point get of a single document?\r\n13:41 <danielmewes> 6 minutes sounds like something is really really wrong\r\n13:42 <underrun> yes\r\n13:42 <danielmewes> Also: If you get the same document multiple times in a row, do you still get some slow gets or is only the first one ever slow?\r\n13:42 <underrun> subsequent gets are also slo\r\n13:43 <underrun> sometimes getting a doc more than once will be fast the first time and slow subsequently\r\n13:43 <underrun> it also seemed transient as it is now more responsive\r\n13:43 <danielmewes> I see. Doesn't seem to be a (out-of) cache issue then\r\n13:43 <underrun> but for about an hour there it was very non-optimal\r\n13:44 <underrun> i looked at the query profile in the data explorer\r\n13:44 <danielmewes> non-optimal is clearly a euphemism ;-)\r\n13:44 <underrun> heh\r\n13:45 <danielmewes> Which client are you using btw? Is it running on the same machine as the server?\r\n13:46 <underrun> i'm using the data explorer from my box and my server is in a datacenter\r\n13:46 <underrun> so js client\r\n13:46 <underrun> not on the same machine\r\n13:47 <danielmewes> ok\r\n13:47 <danielmewes> Hmm...\r\n13:48 <underrun> it looks like the profile says \"Perform read.\" took the time. but none of its sub tasks took very long at all. \"perform read on shard.\" took 77ms which is still higher than i would like but certainly more reasonable than 6 minutes.\r\n13:48 <danielmewes> I'm just looking for clues in the dark here, so please excuse me asking lots of odd questions: Does the web interface in general feel fast?\r\n13:48 <danielmewes> Oh ok\r\n13:49 <danielmewes> Never mind about that then\r\n13:49 <underrun> yes\r\n13:49 <danielmewes> So the profile does account for the time, which means that it is spent on the server/cluster side\r\n13:49 <danielmewes> perform read on shard would be the actual read operation\r\n13:49 <danielmewes> The rest I believe is transmitting the result to whichever node you are connected to\r\n13:49 <underrun> interesting\r\n13:50 <underrun> i'm inserting data through and reading from the same node\r\n13:50 <danielmewes> Were any backfilling or other background tasks going on after you started up the cluster?\r\n13:51 <underrun> no - i was on vacation last week and i left it up\r\n13:51 <underrun> so its been idle for many days\r\n13:51 <underrun> but i did start a job that is inserting data using soft durability\r\n13:53 <danielmewes> Ok and that was ongoing while you tested the gets?\r\n13:53 <underrun> yes\r\n13:54 <danielmewes> Can you tell me a bit more about the insert task? How many concurrent clients were you using (if any)? Did you use batched inserts? If yes, how many documents per batch?\r\n13:54 <underrun> 8 threads inserting batches of 100 docs where each doc is about 0.5k\r\n13:55 <underrun> plus or minus ~200 bytes\r\n13:55 <danielmewes> Ok thanks\r\n13:55 <danielmewes> And are your tables stored on rotational disks or SSDs?\r\n13:55 <underrun> ssd\r\n13:55 <danielmewes> Hmm ok\r\n13:56 <underrun> raid 0\r\n13:56 <danielmewes> I thought I was onto something, but no ;-)\r\n13:56 <underrun> 4 disks\r\n13:57 <danielmewes> So what you saw definitely should not happen. Unless there is a problem with your hardware or network or something, this is probably a bug in our server. I have no real clue yet what might cause this.\r\n13:57 <danielmewes> underrun, I'll try to re-create your scenario on a test server and see if I can reproduce the issue\r\n13:58 <danielmewes> How many servers do you have in your cluster?\r\n13:58 <underrun> cool\r\n13:59 <underrun> 1 physical box with 16 cores (4 numa nodes) run 4 instances of rethinkdb with 256GB RAM\r\n13:59 <underrun> amd hardware if that matters\r\n14:00 <underrun> using nuamctl to limit each rethinkdb instance to a single node\r\n14:00 <danielmewes> Ok thanks for the info. Any special file systems or special system configuration?\r\n14:00 <danielmewes> Oh ok hmm\r\n14:00 <underrun> ext4\r\n14:00 <danielmewes> Wonder if that might confuse our internal threading logic somehow. I will test it\r\n14:01 <underrun> table has 32GB table cac\r\n14:01 <underrun> cache\r\n14:01 <danielmewes> RethinkDB 1.11?\r\n14:01 <underrun> yes\r\n14:01 <underrun> 1.11.2\r\n14:02 <danielmewes> Ok one (I hope) last thing, just to make sure: Could you check the memory utilization of the RethinkDB server instances?\r\n14:02 <danielmewes> (top or htop output is sufficient)\r\n14:02 <underrun> each server is using ~42GB\r\n14:02 <danielmewes> ok\r\n14:02 <underrun> plus or minus a couple hundred MB\r\n14:04 <underrun> i am always inserting to and reading from a single instance\r\n 14:05 <underrun> this is how i'm running the instances - for the node I'm inserting to i'm using default ports: numactl -N 0 -m 0 rethinkdb -n dbeval1 -c 4 --bind all --pid-file /space/rethinkdb/rethinkdb1.pid --log-file /space/rethinkdb/log/rethinkdb1.log --canonical-address 127.0.0.1 --daemon -d /space/rethinkdb/data1\r\n14:05 <underrun> other nodes look like this: numactl -N 1 -m 1 rethinkdb -n dbeval2 -c 4 --bind all --no-http-admin -j localhost --pid-file /space/rethinkdb/rethinkdb2.pid --log-file /space/rethinkdb/log/rethinkdb2.log --canonical-address 127.0.0.1 --daemon -d /space/rethinkdb/data2 -o 10\r\n14:06 <underrun> with different port offsets and numbers in filenames accordingly \r\n```"
  , issueState = "closed"
  , issueId = Id 25122662
  , issueComments = 17
  , issueMilestone =
      Just
        Milestone
          { milestoneCreator =
              SimpleUser
                { simpleUserId = Id 48436
                , simpleUserLogin = N "coffeemug"
                , simpleUserAvatarUrl =
                    "https://avatars.githubusercontent.com/u/48436?v=3"
                , simpleUserUrl = "https://api.github.com/users/coffeemug"
                , simpleUserType = OwnerUser
                }
          , milestoneDueOn = Just 2014 (-03) (-13) 07 : 00 : 00 UTC
          , milestoneOpenIssues = 0
          , milestoneNumber = 53
          , milestoneClosedIssues = 203
          , milestoneDescription = Just ""
          , milestoneTitle = "1.12"
          , milestoneUrl =
              "https://api.github.com/repos/rethinkdb/rethinkdb/milestones/53"
          , milestoneCreatedAt = 2013 (-11) (-19) 09 : 47 : 10 UTC
          , milestoneState = "closed"
          }
  }