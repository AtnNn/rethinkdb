IssueComment
  { issueCommentUpdatedAt = 2016 (-08) (-25) 19 : 00 : 19 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/242501459"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/5897#issuecomment-242501459"
  , issueCommentCreatedAt = 2016 (-08) (-25) 19 : 00 : 19 UTC
  , issueCommentBody =
      "> how does it connect - which piece of data is cached? The DNS name, the IP address of the service, or the IP address of the pod itself?\r\n\r\nThe IP address.\r\n\r\n> Could this message be improved somehow? Is there a way to tell if the connection is coming from a client, proxy, or server?\r\n\r\nNot easily. The message means that something tried to connect to the cluster port (e.g. 29015) that was not speaking the RethinkDB cluster protocol. So it was most likely *not* another RethinkDB server or proxy. It might also be displayed if a RethinkDB server has the `--cluster-tls` option enabled and tries to connect to a server without TLS on the cluster port.\r\n\r\n> What is the safe way to run production nodes until this gets resolved?\r\n\r\nNot sure out of my head. @encryptio might have some insights into this.\r\n\r\n> Is there any way, even if hacky one, to flush this \"cache\" the former IP addresses of nodes that got killed? Will restarting servers one by one solve the issue?\r\n\r\nYou can make it forget the IP addresses more quickly by setting the `--cluster-reconnect-timeout <seconds>` option when starting the servers. The default is 24 hours. The other option is restarting the servers as you say (one by one should be enough I think)."
  , issueCommentId = 242501459
  }