IssueComment
  { issueCommentUpdatedAt = 2014 (-07) (-22) 01 : 06 : 46 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 17789
        , simpleUserLogin = N "gchpaco"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/17789?v=3"
        , simpleUserUrl = "https://api.github.com/users/gchpaco"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/49686533"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/2731#issuecomment-49686533"
  , issueCommentCreatedAt = 2014 (-07) (-22) 01 : 06 : 46 UTC
  , issueCommentBody =
      "I asked @mlucy to break this out of the array size limit issue, as this was a different bug that didn't need to be part of the size limit solution.  From code review:\r\n\r\n> Actually, I think this is one of the few really good use cases for setting the array size limit lower in production. If you open a bunch of changefeeds and then your network gets stopped up, buffering 100,000 rows **each** is really pathological behavior. (I.e., your server will run out of memory and crash if you have large rows, and there's no easy way to prevent it.)\r\n>\r\n> I think `subscription_t` should have a field for the maximum number of elements it will queue up, which is set by the query (i.e. the `.changes` call) that creates the `subscription_t, and then that field should be used in this `if` statement.\r\n"
  , issueCommentId = 49686533
  }