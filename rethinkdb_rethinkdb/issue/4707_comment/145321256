IssueComment
  { issueCommentUpdatedAt = 2015 (-10) (-04) 06 : 19 : 09 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 2448684
        , simpleUserLogin = N "ember-max"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/2448684?v=3"
        , simpleUserUrl = "https://api.github.com/users/ember-max"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/145321256"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/4707#issuecomment-145321256"
  , issueCommentCreatedAt = 2015 (-10) (-04) 06 : 19 : 09 UTC
  , issueCommentBody =
      "I'm assuming adding the 'diff' flag wouldn't just be a cosmetic wrapper around something like jsondiffpatch? Using an external diff library is simple enough, it seems the issue is the performance involved in actually diff'ing large objects, not to mention the bandwidth consumed. Imagine you have thousands of users connected in a real time environment, and each user document has thousands of rows, and all these users are doing little things to affect themselves and/or other users all the time (read: just a few diffs at a time occurring very frequently)... changefeeds become an amazing tool for propagating those changes to the clients IF you're efficiently able to tell each client about the deltas... it's not clear that that is very feasible if you have to use application server CPU (or even DB server CPU) to compute the deltas. What I would want personally would just be a stream of the individual write commands and their arguments, it seems like that is what is naturally available without doing any more work.\r\n\r\ntl;dr\r\n+1 :)\r\n"
  , issueCommentId = 145321256
  }