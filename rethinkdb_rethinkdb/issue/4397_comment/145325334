IssueComment
  { issueCommentUpdatedAt = 2015 (-10) (-04) 08 : 06 : 19 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 12630927
        , simpleUserLogin = N "tkodw"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/12630927?v=3"
        , simpleUserUrl = "https://api.github.com/users/tkodw"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/145325334"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/4397#issuecomment-145325334"
  , issueCommentCreatedAt = 2015 (-10) (-04) 08 : 06 : 19 UTC
  , issueCommentBody =
      "I don't like the idea of having a table specific of keynames that documents are dependent on. Managing that sort of metadata seems like a lot of complexity that leaves a lot of compression problems unsolved, despite all the effort it requires.\r\n\r\nI don't have a very good idea of the internals of how RethinkDB, so I'll give an account of how I would solve the key compression problem in a simpler system. Imagine a database system that always stores all documents as json plaintext on disk.\r\n\r\n**Case 1: Small documents dominated by keynames** (examples: Scientific timeseries data / Simple user metrics)\r\n\r\nExample:\r\n```\r\n{TestSeries:'Trial1',MuzzleVelocity:136.46,ElapsedTime:120004}\r\n{TestSeries:'Trial1',MuzzleVelocity:134.37,ElapsedTime:120005}\r\n{TestSeries:'Trial1',MuzzleVelocity:132.73,ElapsedTime:120006}\r\n{TestSeries:'Trial1',MuzzleVelocity:131.27,ElapsedTime:120007}\r\n```\r\n\r\nIn these types of cases low entropy is due to repeated keynames, but also common duplicated values. Now we could solve this by making a global table that knows all the common keynames. Lets see what that will look like compressed with a key table:\r\n\r\ncommon key table\r\n```\r\n{0:TestSeries,1:MuzzleVelocity,2:ElapsedTime}\r\n```\r\nnewly compressed data\r\n```\r\n{\\0:'Trial1',\\1:136.46,\\2:120004}\r\n{\\0:'Trial1',\\1:134.37,\\2:120005}\r\n{\\0:'Trial1',\\1:132.73,\\2:120006}\r\n{\\0:'Trial1',\\1:131.27,\\2:120007}\r\n```\r\n\r\nThat's kinda better, but there is still a lot of entropy in the form of common repeated values and  we added a global metadata table to maintain.\r\n\r\nThinking carefully about the data that is in the global metadata table, it doesn't really need to be global, it just needs to be repeated less often to save space. for example if we store documents in groups of 10 and repeat the common tag list every 10 documents thats 90% compression minus the size of the lookup symbols for the common tag list.\r\n\r\nNext for what we actually put in the table, we notice that in a lot of cases, it isn't just tags that are duplicated, but also common values and even the document structure. So I propose another solution, simply pick a whole document and use it as a static dictionary to compress the next few documents.\r\n\r\nProposed static dictionary solution: \r\n\r\ncopy token format: `\\<dict offset>-<copy length>\\`\r\n\r\n```\r\n{TestSeries:'Trial1',MuzzleVelocity:136.46,ElapsedTime:120004}\r\n\\0-38\\4.37\\42-18\\5}\r\n\\0-38\\2.73\\42-18\\6}\r\n\\0-38\\1.27\\42-18\\7}\r\n```\r\n\r\nSince we didn't restrict ourselves to only key compression here we have a section of the document including a key, a repeated value, another key, and part of the next value that turned into a single token. I did not show it in this example, but it also helps with keys that have prefixes or suffixes, since we can copy any part of the dictionary document it does not need to match a whole key.\r\n\r\nThis is not the same thing as simply compressing a group of documents together, since to decode a single document you do not need to interpret or process the tokens for the documents preceding it.\r\n\r\nFor practical purposes implementing this I would probably want to try to always align the static dictionary with the beginning of an IO block and start a new dictionary at the next IO block so this method won't increase the number of IO operations to read a single document. It might be worth it to break that rule for this compression method to work on larger documents, but that will hurt IO if you end up needing to do an extra read operation to fetch a static dictionary that a document is dependent on.\r\n\r\n**Case 2: Large documents dominated by structure and values.** (examples: forum posts, user comments)\r\n\r\nWhen considering whether to implement a table to store common keys, it's important to take note of what percentage of entropy is saved by shortening the keys. Imagine forum posts where the post text is 180 bytes and you have 20 bytes of keys. At best you can save 10% and it only gets worse from there. Most of these larger documents would benefit most from fast compression of the whole document. Large documents that have lots of nested structure also have a lot of repeated keys, but compressing the entire document solves this better without the need for a special key table. \r\n\r\n**Additional note**\r\n\r\nI've already stated this elsewhere, but I really think compression is best suited to being done in a way that won't delay write operations, such as a garbage collection / compacting step. For best compression you would want to group similar documents, which isn't always possible when handling realtime writes."
  , issueCommentId = 145325334
  }