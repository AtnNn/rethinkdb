IssueComment
  { issueCommentUpdatedAt = 2015 (-10) (-07) 05 : 27 : 03 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 12630927
        , simpleUserLogin = N "tkodw"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/12630927?v=3"
        , simpleUserUrl = "https://api.github.com/users/tkodw"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/146080257"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/4397#issuecomment-146080257"
  , issueCommentCreatedAt = 2015 (-10) (-07) 05 : 27 : 03 UTC
  , issueCommentBody =
      "@indiedotkim \r\n\r\nOn read dominated workloads I would expect strictly better I/O performance with a not very noticeable CPU usage increase.\r\n\r\nThe CPU cost of decompression should scale linearly with the number of matches that are copied from the static dictionary. It is an almost negligible cost, just reading a position and length and doing a memcopy for each match. Worst case scenario is a large document with entirely minimum length matches, but additional logic could be placed in the compression stage to prevent that from ever happening. (at the cost of reduced IO performance gains)\r\n\r\nOn write heavy workload it might be risky to always compress all documents, since compression is significantly more CPU intensive than decompression. I think it can be managed in a way that never saturates the CPU and becomes a bottleneck, but the there are cases where the admin of that system might have needed that extra CPU for another process on the system even more than a potentially very large I/O performance increase and size reduction on the database. That said, I highly encourage an effort to come up with a low cost compression that can default to always on.\r\n\r\nThe CPU cost of decompression is usually higher due to the need to check each byte of document to find a match in the dictionary, compared to decompression that never does *any* operations on a per-byte basis. However, if you are building a custom compression method that is aware of your document structure, you can place limitations to compression to create a lower ratio method that can come close to a negligible cost.\r\n\r\nCompression Steps:\r\n\r\n1. Build a hash table to find matches within the static dictionary. This is usually a non-negligible CPU and memory cost, but caching the table or keeping it static allows you to reuse it when compressing multiple documents.\r\n2. For each byte in each compressed document, look into the hash table for a match. If a match is found, insert a token to the output to copy that match. (I've skimmed the details, but these are the main points)\r\n\r\nCompression limitation ideas:\r\n\r\n1. Don't build the whole hash table. You can cheat here by only inserting potential matches into the hash table at the beginning of each key or value. This allows a much smaller hash table and build it in a fraction of the time. You won't lose the ability to find a match that contains multiple identical keys and values, but will lose the ability to match suffixes or find matches within. Clever compromises are also possible such as allowing partial matches on any part of a key but only allowing partial matches on the first 8 bytes of values, etc.\r\n\r\n2. Don't scan the whole document for matches. You can try checking for matches only at the beginning of each key or value. This has similar benefits to not building the whole hash table, but the overall performance benefit is much greater, since it may allow skipping a huge number of failed hash table lookups on value types.\r\n\r\n3. Don't update the hash table to allow matches within each document. Most compression algorithms that use a static dictionary only initially start the hash table with dictionary entries and start inserting matches within the input from there. If matches within a document are not allowed the hash table can stay completely static and won't need to reloaded between documents or update while compressing each document.\r\n\r\n*Note that if you apply the restrictions of only inserting keynames into the hashtable, and only check for matches at the beginning of keynames, you get similar performance and compression as though you had implemented key compression as originally described in this issue.*\r\n\r\nI strongly advocate scalability when handling compression. For example, when inserting documents, do fast compression that doesn't allow partial matches of keys or values, then have the database shrink documents that haven't changed for a while when the database is idle or on user command.\r\n\r\nMy overall thoughts on using compression in a database is that having data in a compressed format is often very beneficial and sometimes critically important, but a decision needs to be made how much resources you are willing to give up to get it compressed.\r\n\r\n@danielmewes\r\n\r\n> We could do this on a per-leaf-node basis for small documents.\r\n\r\nI don't like pushing the static dictionaries up into the leaf nodes, I think they need to be more of a storage layer thing for a lot of reasons.\r\n\r\n1. What happens when you re-balance the leaves? In retrospect I shouldn't have listed any other reasons it shouldn't be in the leaves, because it's probably impossible to re-balance.\r\n\r\n2. The dictionary cannot be changed if you put it in a leaf node unless there is a way to keep two dictionaries at once, and even then it would be an expensive and convoluted process. There is no clean way to atomically change all the documents to migrate to a new dictionary.\r\n\r\n3. Large dictionaries in the leaf nodes are probably really bad and it's very likely that cross-document compression on documents that are larger than can be reasonably inserted into leaf nodes will be eventually implemented, so the algorithms can never assume dictionaries are available in leaf nodes.\r\n\r\n4. The ability to find highly similar documents from different leaves and compress them together with the same dictionary to optimize storage space is lost, since dictionaries are arbitrarily tied to leaves.\r\n\r\nDeleting old dictionaries should be handled normally by garbage collection without any special logic. A \"block\" containing a dictionary and the data compressed using it should always be deleted together by the garbage collection algorithm after relocating and re-compressing the remaining documents with a new dictionary. I don't see how it is any different than deleting or updating a document normally, other than treating a compressed block as a single unit during deletion.\r\n\r\nThe \"problem\" with using dictionary compression on larger documents is that if the dictionaries and documents are far apart, it means it always costs at least one extra IO for scattered reads. Compression -> smaller working set ->  more things stay in cache -> the extra IO might end up staying in cache and not impact performance. Probably better to test before coming up with a convoluted solution for a theoretical problem."
  , issueCommentId = 146080257
  }