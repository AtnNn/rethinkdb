IssueComment
  { issueCommentUpdatedAt = 2015 (-10) (-06) 00 : 40 : 53 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/145709447"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/4397#issuecomment-145709447"
  , issueCommentCreatedAt = 2015 (-10) (-06) 00 : 40 : 53 UTC
  , issueCommentBody =
      "> So I propose another solution, simply pick a whole document and use it as a static dictionary to compress the next few documents.\r\n\r\nThat's a super cool idea. We could do this on a per-leaf-node basis for small documents.\r\nThere are some details we need to figure out about what happens if the reference document gets deleted or changed, but I think there might be some promising solution down this path that's not too complicated to implement and easy to use.\r\n\r\nI think there's still benefit in providing cross-document compression for larger documents. I've often seen large documents that are still dominated by field names rather than by big chunks of value data. However it might indeed work reasonably well in practice to just use per-document compression for those."
  , issueCommentId = 145709447
  }