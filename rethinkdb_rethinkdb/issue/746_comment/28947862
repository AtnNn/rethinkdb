IssueComment
  { issueCommentUpdatedAt = 2013 (-11) (-21) 00 : 37 : 04 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 139396
        , simpleUserLogin = N "wojons"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/139396?v=3"
        , simpleUserUrl = "https://api.github.com/users/wojons"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/28947862"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/746#issuecomment-28947862"
  , issueCommentCreatedAt = 2013 (-11) (-21) 00 : 37 : 04 UTC
  , issueCommentBody =
      "I worked with a lot of databases that include ttl stuff and it can get really really messy and  rechavoc on the system because its so busy trying to delete the object its killing all other operations. To me something that would be really useful if the doc can be marked dirty on the timer. And i would have the timer be dynamic the longer the last pass of the timer took to wrong the longer the next one is defered. Something like if the timer job is every 60 seconds but it took 45 seconds to mark everything dirty then it will run the next check in 38 seconds, adding an extra 23 seconds. (45/2).\r\n\r\nThere can be another timer that runs that will do the deleting using the same sindex that the first timer uses. The only way to see these dirty docs would be using the \"use_outdated\" flag in the run(). If this second timer uses the throttling deleting i was speaking about in #1609 it would be pretty safe to run this in high load. things till get marked as dirty letting the user not have to worry about filtering out the data or deleting it on there own. and it wont kill there normal perfomance since the deletes wont be aggressive the only issue they have to worry about is the fact they may run out of disk space if they are under high load to long"
  , issueCommentId = 28947862
  }