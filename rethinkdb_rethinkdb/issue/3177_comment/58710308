IssueComment
  { issueCommentUpdatedAt = 2014 (-10) (-10) 20 : 16 : 09 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 1777134
        , simpleUserLogin = N "mlucy"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/1777134?v=3"
        , simpleUserUrl = "https://api.github.com/users/mlucy"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/58710308"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/3177#issuecomment-58710308"
  , issueCommentCreatedAt = 2014 (-10) (-10) 20 : 16 : 09 UTC
  , issueCommentBody =
      "> Close to that, but to update/delete a big batch at once would be much more effective\r\n\r\nEven if you do a big batched delete, we break it up into smaller batches internally so that we don't block out other operations while it happens.  I think a parallelized `update` inside `for_each` would be close as fast as a batched `update`.  If we added a way for writes to be squashed together into batched writes while waiting for the btree, I think it would be just as fast.\r\n\r\n> From the ReQL side this this already possible I think, by using variadic getAll.\r\n\r\nIf the body of the `for_each` is simple, that works too, and should parallelize just fine.  The problem with that approach is that you need to evaluate all of the rows you're operating on before you can start doing writes on the first one, which will slow things down a bit."
  , issueCommentId = 58710308
  }