IssueComment
  { issueCommentUpdatedAt = 2016 (-02) (-24) 23 : 10 : 14 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/188504681"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/5438#issuecomment-188504681"
  , issueCommentCreatedAt = 2016 (-02) (-24) 23 : 10 : 14 UTC
  , issueCommentBody =
      "> Oh I see, that's probably a simple way to find which table the problem is in, if there is any.\r\n\r\nRight. I was hoping that once we know the table, you could maybe provide the data to us so we can analyze the issue further on our end.\r\n\r\n> Could something like an invalid utf-8 sequence cause this?\r\n\r\nIt should not. In general there's nothing you can send to the server that *should* cause this, so it's either a server bug (likely) or some form of file system and/or memory corruption (less likely, though it depends a bit on the storage virtualization layer). Your guess as to what might have triggered this is as good as mine.\r\n\r\nIt's plausible that there's an issue with our JSON parser and/or validation that allows illegal data to get through the query layer.\r\n\r\n\r\nThe data size and load that you describe sound comparably light. Which virtualization solution are you using?"
  , issueCommentId = 188504681
  }