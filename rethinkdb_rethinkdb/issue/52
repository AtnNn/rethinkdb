Issue
  { issueClosedAt = Just 2012 (-11) (-26) 18 : 06 : 03 UTC
  , issueUpdatedAt = 2013 (-11) (-07) 19 : 54 : 18 UTC
  , issueEventsUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/52/events"
  , issueHtmlUrl =
      Just "https://github.com/rethinkdb/rethinkdb/issues/52"
  , issueClosedBy = Nothing
  , issueLabels =
      [ IssueLabel
          { labelColor = "e10c02"
          , labelUrl =
              "https://api.github.com/repos/rethinkdb/rethinkdb/labels/pr:high"
          , labelName = "pr:high"
          }
      , IssueLabel
          { labelColor = "e102d8"
          , labelUrl =
              "https://api.github.com/repos/rethinkdb/rethinkdb/labels/tp:bug"
          , labelName = "tp:bug"
          }
      ]
  , issueNumber = 52
  , issueAssignee =
      Just
        SimpleUser
          { simpleUserId = Id 43867
          , simpleUserLogin = N "jdoliner"
          , simpleUserAvatarUrl =
              "https://avatars.githubusercontent.com/u/43867?v=3"
          , simpleUserUrl = "https://api.github.com/users/jdoliner"
          , simpleUserType = OwnerUser
          }
  , issueUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueTitle = "Database corruption (triggers segfault)"
  , issuePullRequest = Nothing
  , issueUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/52"
  , issueCreatedAt = 2012 (-11) (-13) 09 : 28 : 06 UTC
  , issueBody =
      Just
        "I'm getting a crash similar to https://github.com/rethinkdb/rethinkdb/issues/33\r\nExcept that instead of \"Segmentation fault from reading the address (nil).\", I have \"Segmentation fault from reading the address 0xb0.\".\r\nNot sure if it is the same bug or a different one. \r\n\r\nRelease mode output:\r\n\r\n    info: Listening for intracluster connections on port 29016.\r\n    info: Connected to server \"Leshrac\" 98a56f75-29ae-43bc-9b18-4302d706faed\r\n    info: Listening for client driver connections on port 28016.\r\n    info: Listening for administrative HTTP connections on port 8081.\r\n    info: Server ready\r\n    error: Error in arch/runtime/thread_pool.cc at line 323:\r\n    error: Segmentation fault from reading the address 0xb0.\r\n    error: Backtrace:\r\n    error: Tue Nov 13 10:17:15 2012\r\n           \r\n           1: ./rethinkdb() [0x97bd2f]\r\n           2: ./rethinkdb() [0x97d0e4]\r\n           3: ./rethinkdb() [0x97b763]\r\n           4: ./rethinkdb() [0x4d5522]\r\n           5: +0xf8f0 at 0x7fbb65e498f0 (/lib/libpthread.so.0)\r\n           6: ./rethinkdb() [0x9349ae]\r\n           7: ./rethinkdb() [0x92e9aa]\r\n           8: ./rethinkdb() [0x909871]\r\n           9: ./rethinkdb() [0x4d3fbc]\r\n    error: Exiting.\r\n    [2524] worker: Couldn't read job function: end-of-file received\r\n    [2524] worker: Failed to accept job, quitting.\r\n    [2526] worker: Couldn't read job function: end-of-file received\r\n    [2530] worker: Couldn't read job function: end-of-file received\r\n    [2526] worker: Failed to accept job, quitting.\r\n    [2530] worker: Failed to accept job, quitting.\r\n    Segmentation fault\r\n\r\nDebug mode output (waayyy more helpful I assume):\r\n\r\n    info: Our machine ID is e79fe4db-389d-4727-bf68-b86b3ac66eb8\r\n    info: Listening for intracluster connections on port 29016.\r\n    info: Connected to server \"Leshrac\" 98a56f75-29ae-43bc-9b18-4302d706faed\r\n    info: Listening for client driver connections on port 28016.\r\n    info: Listening for administrative HTTP connections on port 8081.\r\n    info: Server ready\r\n    error: Error in serializer/log/log_serializer.cc at line 416:\r\n    error: Assertion failed: [ls_token] \r\n    error: Backtrace:\r\n    error: Tue Nov 13 10:26:38 2012\r\n           \r\n           1: lazy_backtrace_t::lazy_backtrace_t() at backtrace.cc:251\r\n           2: format_backtrace(bool) at backtrace.cc:198\r\n           3: report_fatal_error(char const*, int, char const*, ...) at errors.cc:65\r\n           4: log_serializer_t::block_read(intrusive_ptr_t<ls_block_token_pointee_t> const&, void*, file_account_t*, linux_iocallback_t*) at log_serializer.cc:416\r\n           5: log_serializer_t::block_read(intrusive_ptr_t<ls_block_token_pointee_t> const&, void*, file_account_t*) at log_serializer.cc:392\r\n           6: translator_serializer_t::block_read(intrusive_ptr_t<ls_block_token_pointee_t> const&, void*, file_account_t*) at translator.cc:239\r\n           7: mc_inner_buf_t::load_inner_buf(bool, file_account_t*) at mirrored.cc:166\r\n           8: boost::_mfi::mf2<void, mc_inner_buf_t, bool, file_account_t*>::operator()(mc_inner_buf_t*, bool, file_account_t*) const at mem_fn_template.hpp:275\r\n           9: void boost::_bi::list3<boost::_bi::value<mc_inner_buf_t*>, boost::_bi::value<bool>, boost::_bi::value<file_account_t*> >::operator()<boost::_mfi::mf2<void, mc_inner_buf_t, bool, file_account_t*>, boost::_bi::list0>(boost::_bi::type<void>, boost::_mfi::mf2<void, mc_inner_buf_t, bool, file_account_t*>&, boost::_bi::list0&, int) at bind.hpp:386\r\n           10: boost::_bi::bind_t<void, boost::_mfi::mf2<void, mc_inner_buf_t, bool, file_account_t*>, boost::_bi::list3<boost::_bi::value<mc_inner_buf_t*>, boost::_bi::value<bool>, boost::_bi::value<file_account_t*> > >::operator()() at bind_template.hpp:21\r\n           11: callable_action_instance_t<boost::_bi::bind_t<void, boost::_mfi::mf2<void, mc_inner_buf_t, bool, file_account_t*>, boost::_bi::list3<boost::_bi::value<mc_inner_buf_t*>, boost::_bi::value<bool>, boost::_bi::value<file_account_t*> > > >::run_action() at runtime_utils.hpp:57\r\n           12: callable_action_wrapper_t::run() at runtime_utils.cc:58\r\n           13: coro_t::run() at coroutines.cc:178\r\n    error: Exiting.\r\n    Crashing while already crashed. Printing error message to stderr.\r\n\r\n\r\nThe data on which the crash occurs is the result of running the stress client on a sharded (but non-replicated) database for 6 hours. After that I enabled replication and shut down the second server, because I wanted to test outdated reads. Upon restarting the second server, the crash occurred.\r\n\r\nTo reproduce, please download my RethinkDB data_dirs from here:\r\nhttp://danielmewes.dnsalias.net/~daniel/.private/cluster_data_segfault.tar.bz2\r\n\r\nThen start server 1 (should start up properly):\r\n\r\n    rethinkdb -d rethinkdb_cluster_data\r\n\r\nand server 2 (should crash):\r\n\r\n    rethinkdb serve -d rethinkdb_cluster_data2 -o 1 -j localhost:29015\r\n\r\n"
  , issueState = "closed"
  , issueId = Id 8316383
  , issueComments = 27
  , issueMilestone =
      Just
        Milestone
          { milestoneCreator =
              SimpleUser
                { simpleUserId = Id 43867
                , simpleUserLogin = N "jdoliner"
                , simpleUserAvatarUrl =
                    "https://avatars.githubusercontent.com/u/43867?v=3"
                , simpleUserUrl = "https://api.github.com/users/jdoliner"
                , simpleUserType = OwnerUser
                }
          , milestoneDueOn = Just 2012 (-12) (-17) 08 : 00 : 00 UTC
          , milestoneOpenIssues = 0
          , milestoneNumber = 4
          , milestoneClosedIssues = 79
          , milestoneDescription = Just ""
          , milestoneTitle = "1.3"
          , milestoneUrl =
              "https://api.github.com/repos/rethinkdb/rethinkdb/milestones/4"
          , milestoneCreatedAt = 2012 (-12) (-06) 19 : 46 : 50 UTC
          , milestoneState = "closed"
          }
  }