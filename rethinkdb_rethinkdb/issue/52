Issue
  { issueClosedAt = Just 2012 (-11) (-26) 18 : 06 : 03 UTC
  , issueUpdatedAt = 2013 (-11) (-07) 19 : 54 : 18 UTC
  , issueEventsUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/52/events"
  , issueHtmlUrl =
      Just "https://github.com/rethinkdb/rethinkdb/issues/52"
  , issueClosedBy = Nothing
  , issueLabels =
      [ IssueLabel
          { labelColor = "e10c02"
          , labelUrl =
              "https://api.github.com/repos/rethinkdb/rethinkdb/labels/pr:high"
          , labelName = "pr:high"
          }
      , IssueLabel
          { labelColor = "e102d8"
          , labelUrl =
              "https://api.github.com/repos/rethinkdb/rethinkdb/labels/tp:bug"
          , labelName = "tp:bug"
          }
      ]
  , issueNumber = 52
  , issueAssignee =
      Just
        SimpleUser
          { simpleUserId = Id 43867
          , simpleUserLogin = N "jdoliner"
          , simpleUserAvatarUrl =
              "https://avatars.githubusercontent.com/u/43867?v=3"
          , simpleUserUrl = "https://api.github.com/users/jdoliner"
          , simpleUserType = OwnerUser
          }
  , issueUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueTitle = "Database corruption (triggers segfault)"
  , issuePullRequest = Nothing
  , issueUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/52"
  , issueCreatedAt = 2012 (-11) (-13) 09 : 28 : 06 UTC
  , issueBody =
      Just
        "I'm getting a crash similar to https://github.com/rethinkdb/rethinkdb/issues/33\nExcept that instead of \"Segmentation fault from reading the address (nil).\", I have \"Segmentation fault from reading the address 0xb0.\".\nNot sure if it is the same bug or a different one. \n\nRelease mode output:\n\n```\ninfo: Listening for intracluster connections on port 29016.\ninfo: Connected to server \"Leshrac\" 98a56f75-29ae-43bc-9b18-4302d706faed\ninfo: Listening for client driver connections on port 28016.\ninfo: Listening for administrative HTTP connections on port 8081.\ninfo: Server ready\nerror: Error in arch/runtime/thread_pool.cc at line 323:\nerror: Segmentation fault from reading the address 0xb0.\nerror: Backtrace:\nerror: Tue Nov 13 10:17:15 2012\n\n       1: ./rethinkdb() [0x97bd2f]\n       2: ./rethinkdb() [0x97d0e4]\n       3: ./rethinkdb() [0x97b763]\n       4: ./rethinkdb() [0x4d5522]\n       5: +0xf8f0 at 0x7fbb65e498f0 (/lib/libpthread.so.0)\n       6: ./rethinkdb() [0x9349ae]\n       7: ./rethinkdb() [0x92e9aa]\n       8: ./rethinkdb() [0x909871]\n       9: ./rethinkdb() [0x4d3fbc]\nerror: Exiting.\n[2524] worker: Couldn't read job function: end-of-file received\n[2524] worker: Failed to accept job, quitting.\n[2526] worker: Couldn't read job function: end-of-file received\n[2530] worker: Couldn't read job function: end-of-file received\n[2526] worker: Failed to accept job, quitting.\n[2530] worker: Failed to accept job, quitting.\nSegmentation fault\n```\n\nDebug mode output (waayyy more helpful I assume):\n\n```\ninfo: Our machine ID is e79fe4db-389d-4727-bf68-b86b3ac66eb8\ninfo: Listening for intracluster connections on port 29016.\ninfo: Connected to server \"Leshrac\" 98a56f75-29ae-43bc-9b18-4302d706faed\ninfo: Listening for client driver connections on port 28016.\ninfo: Listening for administrative HTTP connections on port 8081.\ninfo: Server ready\nerror: Error in serializer/log/log_serializer.cc at line 416:\nerror: Assertion failed: [ls_token] \nerror: Backtrace:\nerror: Tue Nov 13 10:26:38 2012\n\n       1: lazy_backtrace_t::lazy_backtrace_t() at backtrace.cc:251\n       2: format_backtrace(bool) at backtrace.cc:198\n       3: report_fatal_error(char const*, int, char const*, ...) at errors.cc:65\n       4: log_serializer_t::block_read(intrusive_ptr_t<ls_block_token_pointee_t> const&, void*, file_account_t*, linux_iocallback_t*) at log_serializer.cc:416\n       5: log_serializer_t::block_read(intrusive_ptr_t<ls_block_token_pointee_t> const&, void*, file_account_t*) at log_serializer.cc:392\n       6: translator_serializer_t::block_read(intrusive_ptr_t<ls_block_token_pointee_t> const&, void*, file_account_t*) at translator.cc:239\n       7: mc_inner_buf_t::load_inner_buf(bool, file_account_t*) at mirrored.cc:166\n       8: boost::_mfi::mf2<void, mc_inner_buf_t, bool, file_account_t*>::operator()(mc_inner_buf_t*, bool, file_account_t*) const at mem_fn_template.hpp:275\n       9: void boost::_bi::list3<boost::_bi::value<mc_inner_buf_t*>, boost::_bi::value<bool>, boost::_bi::value<file_account_t*> >::operator()<boost::_mfi::mf2<void, mc_inner_buf_t, bool, file_account_t*>, boost::_bi::list0>(boost::_bi::type<void>, boost::_mfi::mf2<void, mc_inner_buf_t, bool, file_account_t*>&, boost::_bi::list0&, int) at bind.hpp:386\n       10: boost::_bi::bind_t<void, boost::_mfi::mf2<void, mc_inner_buf_t, bool, file_account_t*>, boost::_bi::list3<boost::_bi::value<mc_inner_buf_t*>, boost::_bi::value<bool>, boost::_bi::value<file_account_t*> > >::operator()() at bind_template.hpp:21\n       11: callable_action_instance_t<boost::_bi::bind_t<void, boost::_mfi::mf2<void, mc_inner_buf_t, bool, file_account_t*>, boost::_bi::list3<boost::_bi::value<mc_inner_buf_t*>, boost::_bi::value<bool>, boost::_bi::value<file_account_t*> > > >::run_action() at runtime_utils.hpp:57\n       12: callable_action_wrapper_t::run() at runtime_utils.cc:58\n       13: coro_t::run() at coroutines.cc:178\nerror: Exiting.\nCrashing while already crashed. Printing error message to stderr.\n```\n\nThe data on which the crash occurs is the result of running the stress client on a sharded (but non-replicated) database for 6 hours. After that I enabled replication and shut down the second server, because I wanted to test outdated reads. Upon restarting the second server, the crash occurred.\n\nTo reproduce, please download my RethinkDB data_dirs from here:\nhttp://danielmewes.dnsalias.net/~daniel/.private/cluster_data_segfault.tar.bz2\n\nThen start server 1 (should start up properly):\n\n```\nrethinkdb -d rethinkdb_cluster_data\n```\n\nand server 2 (should crash):\n\n```\nrethinkdb serve -d rethinkdb_cluster_data2 -o 1 -j localhost:29015\n```\n"
  , issueState = "closed"
  , issueId = Id 8316383
  , issueComments = 27
  , issueMilestone =
      Just
        Milestone
          { milestoneCreator =
              SimpleUser
                { simpleUserId = Id 43867
                , simpleUserLogin = N "jdoliner"
                , simpleUserAvatarUrl =
                    "https://avatars.githubusercontent.com/u/43867?v=3"
                , simpleUserUrl = "https://api.github.com/users/jdoliner"
                , simpleUserType = OwnerUser
                }
          , milestoneDueOn = Just 2012 (-12) (-17) 08 : 00 : 00 UTC
          , milestoneOpenIssues = 0
          , milestoneNumber = 4
          , milestoneClosedIssues = 79
          , milestoneDescription = Just ""
          , milestoneTitle = "1.3"
          , milestoneUrl =
              "https://api.github.com/repos/rethinkdb/rethinkdb/milestones/4"
          , milestoneCreatedAt = 2012 (-12) (-06) 19 : 46 : 50 UTC
          , milestoneState = "closed"
          }
  }