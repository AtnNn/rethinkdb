IssueComment
  { issueCommentUpdatedAt = 2014 (-06) (-20) 02 : 04 : 20 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 1777134
        , simpleUserLogin = N "mlucy"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/1777134?v=3"
        , simpleUserUrl = "https://api.github.com/users/mlucy"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/46638159"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/137#issuecomment-46638159"
  , issueCommentCreatedAt = 2014 (-06) (-20) 02 : 04 : 20 UTC
  , issueCommentBody =
      "> Nobody wants to do up-to-date reads of large files.\r\n\r\nWhy not?  What if you're writing to those files at the same time in another client?  (Assuming you had some form of locking to make sure reads didn't start until the writes had finished.)\r\n\r\nAlso, as a separate point, you might want to distribute the read workload without needing a bajillion copies of the file.  If you have a 16 machine cluster, you might want to split the read load over the whole thing without needing 16 copies of your data."
  , issueCommentId = 46638159
  }