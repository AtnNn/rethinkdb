IssueComment
  { issueCommentUpdatedAt = 2014 (-06) (-20) 01 : 48 : 40 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 258437
        , simpleUserLogin = N "srh"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/258437?v=3"
        , simpleUserUrl = "https://api.github.com/users/srh"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/46637415"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/137#issuecomment-46637415"
  , issueCommentCreatedAt = 2014 (-06) (-20) 01 : 48 : 40 UTC
  , issueCommentBody =
      "> so that up-to-date reads can be distributed over the cluster\r\n\r\nNobody wants to do up-to-date reads of large files.\r\n\r\n> Not to mention, reading through one large file shouldn't kill throughput on one random machine.\r\n\r\nIt wouldn't.\r\n\r\nIn the case where you're supposing this is bad, the user's accessing one or two files significantly more than all the others combined.  If that's the case, the user can solve the problem by caching those files outside of RethinkDB, or we could solve that problem for them by having proxy nodes capable of caching such things.  \r\n"
  , issueCommentId = 46637415
  }