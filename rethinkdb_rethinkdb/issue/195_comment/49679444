IssueComment
  { issueCommentUpdatedAt = 2014 (-07) (-21) 23 : 21 : 09 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 1777134
        , simpleUserLogin = N "mlucy"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/1777134?v=3"
        , simpleUserUrl = "https://api.github.com/users/mlucy"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/49679444"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/195#issuecomment-49679444"
  , issueCommentCreatedAt = 2014 (-07) (-21) 23 : 21 : 09 UTC
  , issueCommentBody =
      "> I don't imagine such an architecture would be doable to for me, though. Am very interested, therefore, if anyone has considered the problem of redundant/distributed tailing of a change feed.\r\n\r\nIf you're worried about having a single point of failure, it's probably best to have your second process reading from a changefeed on a different machine than the first process.  At that point, you need to generate two network messages for every change anyway, so there's no cost to just opening two changefeeds.  (The cluster will dedup messages internally if you create the two changefeeds on the same cluster node.)\r\n\r\nI think long-term we're going to want a way to attach per-shard timestamps to changefeed messages, which people can turn on with an optarg.  That would make it much easier to keep changefeeds coordinated across machines if you're doing that for redundancy.\r\n\r\nAnother option would be to have resumable changefeeds, but we don't really have the infrastructure to support creating and managing long-running jobs right now, so that's probably a ways away."
  , issueCommentId = 49679444
  }