IssueComment
  { issueCommentUpdatedAt = 2013 (-06) (-20) 00 : 06 : 03 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 646357
        , simpleUserLogin = N "wmrowan"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/646357?v=3"
        , simpleUserUrl = "https://api.github.com/users/wmrowan"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/19723447"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1043#issuecomment-19723447"
  , issueCommentCreatedAt = 2013 (-06) (-20) 00 : 06 : 03 UTC
  , issueCommentBody =
      "A long time ago when I was tasked with figuring out why we used so much memory at startup I found that we were instantiating several large buffers for each shard that we spun up. Reducing the number of hash shards from 24 to 4 was one way of reducing the memory used by these objects. I then was able to get rid of most of this memory usage by instantiating these objects on demand. We didn't then re-increase the number of hash shards.\r\n\r\nWhat if we try bumping the number back up to see what effect this has on performance / memory usage? If this doesn't recreate the memory problems then the advantages of parallelization may outweigh the impact on metadata operations."
  , issueCommentId = 19723447
  }