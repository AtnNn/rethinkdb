IssueComment
  { issueCommentUpdatedAt = 2016 (-01) (-17) 01 : 44 : 04 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 478118
        , simpleUserLogin = N "bchavez"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/478118?v=3"
        , simpleUserUrl = "https://api.github.com/users/bchavez"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/172261625"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/pull/5292#issuecomment-172261625"
  , issueCommentCreatedAt = 2016 (-01) (-16) 22 : 03 : 52 UTC
  , issueCommentBody =
      "@pires , no problem m8. I hope the design info helps. BTW, kudos for improving the Java driver. It's a significant contribution for the community.\r\n\r\n##### Regarding Changefeeds in The New Design\r\nChangefeeds use an underlying `Cursor` type. Not much changes in `Cursor` code. At a high level, the client driver is controlling the flow of downstream updates it receives from a changefeed/cursor by sending `CONTINUE` over the wire. When a consumer thread is iterating over a `Cursor` internally the code is giving back values from a buffered list of items. When the buffer of items runs out, the consumer thread inside `getNext()` sends `CONTINUE` over the wire to the server. A new batch arrives and the `Cursor` is extended with the new list of buffered items. In this design, when using `FutureValue<Response>`, it is important to keep track of the currently pending `FutureValue<Response>` `CONTINUE` so it can be awaited on in `getNext()`. When the `CONTINUE` request is fulfilled, the consumer thread can \"extend\" its own buffer list of items.\r\n\r\n`MaybeSendContinue()` has a check to ensure only one `Cursor` `CONTINUE` (per `Cursor` instance) is pending over the wire at any given time. So, I just set the reference to the `awatingContinue`.\r\n\r\n```\r\nprotected internal virtual void MaybeSendContinue()\r\n{\r\n   if( error == null && items.Count < threshold && outstandingRequests == 0 )\r\n   {\r\n      outstandingRequests += 1;\r\n      awaitingContinue = connection.Continue(this);\r\n   }\r\n}\r\n```\r\n\r\nThe relevant .NET reference for the `Cursor` implementation is here:\r\nhttps://github.com/bchavez/RethinkDb.Driver/blob/master/Source/RethinkDb.Driver/Net/Cursor.cs\r\n\r\nI didn't do much to make a `Cursor` by itself intrinsically thread-safe. My assumption in the `Cursor` implementation still assumes one consumer thread iterating over a `Cursor` instance. Conceptually, I didn't think it made much sense to have multiple threads iterating over a single `Cursor` instance. Maybe it could change in the future if someone comes up with a *really* good use case. But even then, I would be reluctant to make `Cursor` thread-safe. Currently, if a developer needed multiple threads on a single `Cursor` instance I'd recommend the developer use a Mutex over the `Cursor`. I hope it would encourage the developer to write code in a way that was more explicitly thread-safe on the eventual consumption of the items being iterated over.\r\n\r\nAlso, there was a proposal somewhere that I encountered that discussed the possibility of pushing changes to client drivers without an explicit `CONTINUE` but I don't think there are any immediate plans for this. If the RethinkDB team wanted to do this in the future then the `ResponsePump` would simply do the cursor extending itself by reaching into the `cursorCache` instead of pushing future values to awaiters. But this was just *proposal* and the design (I think) would still work out.\r\n\r\nOverall, the design works pretty well with **Observable** Reactive Extensions too.\r\n\r\n##### Regarding Connection Pooling\r\nYou'd probably need to talk with the RethinkDB team on this one. I think the API would probably need to go though some proposal process before going public. It would be the first official driver to support connection pooling?\r\n\r\nOnce you have multi-threading working perfectly connection pooling is pretty straight-forward after that.\r\n\r\nFor the .NET driver, one constraint I had for the .NET driver was keeping the code base similarly aligned to the Java driver. I did not want the .NET driver implementation to change too much other than using .NET semantics. So, this constraint somewhat impacted my design of the Connection Pooling code in the .NET driver.\r\n\r\nThe design notes for .NET Connection Pooling are here:\r\nhttps://github.com/bchavez/RethinkDb.Driver/issues/17\r\n\r\nAnd associated documentation for .NET Connection Pooling:\r\nhttps://github.com/bchavez/RethinkDb.Driver/wiki/Connections-&-Pooling\r\n\r\nThe implementation is here:\r\nhttps://github.com/bchavez/RethinkDb.Driver/tree/master/Source/RethinkDb.Driver/Net/Clustering\r\n\r\nIn the design note, I noted it would have been nice to eliminate `ConnectionInstance` or move `SocketWrapper` into `ConnectionInstance`, so that we could keep the # of layers and concepts of a 'connection' to a minimum of two. `Connection -> ConnecitonInstance` or `Connection -> SocketWrapper`. Currently, we have a conceptual mental model of something like `Connection -> ConnectionInstance -> SocketWrapper`. I kinda feel there is code smell here. Maybe it could be a possibility of merging `ConnectionIntance` and `SocketWrapper` now that there's a possibility of `readResponse` being removed from `ConnectionInstance`. If they are merged, I could make improvements on the Connection Pool API and have a cleaner / more robust implementation. It would simplify a connection \"ErrorCallback\" mechanism for connection pooling.\r\n\r\nI used neumino's rethinkdbdash API and GoRethink API as references. All very good. In particular, GoRethink used [go-hostpool](https://github.com/hailocab/go-hostpool) which I used as a starting point to implement the basis of the connection pooling code. The 1-to-1 C# port of go-hostpool had a lot of locking in Go. As a result, the implementation suffered from performance problems due to all the locking going on inside the host pool. I say \"performance problems\" because I unit tested 600K node selections from 6 concurrent threads as a baseline and it took 4 seconds to complete. I wanted to squeeze every ounce of performance out of node selection so 98% of CPU time was spent doing actual driver work not node selection.\r\n\r\nHaving that goal in mind, I refactored the go-hostpool in C# multiple times. Eventually what came out was:\r\n\r\n1. Use monotonically increasing array storage for nodes (IE: never remove nodes if they go offline only mark them as dead). This allows me to never get out of bounds in tight for loops from multiple threads. Updates to the node list are only Adds(), no need for locking the host pool and stopping all threads just to add a single node to the host pool. The new node will eventually be selected by a thread when the host array reference gets updated.\r\n2. Removed virtually all locks on variables in favor of `Interlocked`/.NET OS/CPU primitives to prevent the CPU from pipelining/reordering instructions when updating memory from multiple threads lock-free.\r\n\r\nNow node selection runs in like 100ms instead of 4 full seconds.\r\n\r\nA common interface `IConnection` was found between `Connection` and `ConnectionPool`, and the rest is pretty basic. A discover thread is in charge of discovering new nodes added (sets up a change feed on a system table) and adds them to the node pool. A supervisory thread that periodically scans through the node list and tries to restart failed connections.\r\n\r\nHope that helps,\r\nBrian\r\n"
  , issueCommentId = 172261625
  }