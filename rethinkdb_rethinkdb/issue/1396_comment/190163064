IssueComment
  { issueCommentUpdatedAt = 2016 (-02) (-29) 11 : 22 : 55 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 21650
        , simpleUserLogin = N "kapilt"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/21650?v=3"
        , simpleUserUrl = "https://api.github.com/users/kapilt"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/190163064"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1396#issuecomment-190163064"
  , issueCommentCreatedAt = 2016 (-02) (-29) 11 : 22 : 55 UTC
  , issueCommentBody =
      "use-case -> i have lots of data, and compressing saves me x disk space so i can store 10x more. for me mostly its around any high volume data storing high value archival data in volume with most of the query set against recent data (ie no full scans, either tail cursors or indexed queries).\r\n\r\nconcrete case -> one i'm looking at is cloudtrail data (aws audit log), which are json events dumped with high frequency to s3 as gz compressed keys. in one aws account i work with that's a 1gb of data a day uncompressed. I'd like to be able to query out those records on various attributes within a time window, as well determine api usage rates within a given time window."
  , issueCommentId = 190163064
  }