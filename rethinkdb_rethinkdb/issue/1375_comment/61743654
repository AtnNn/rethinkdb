IssueComment
  { issueCommentUpdatedAt = 2014 (-11) (-05) 01 : 02 : 08 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 1777134
        , simpleUserLogin = N "mlucy"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/1777134?v=3"
        , simpleUserUrl = "https://api.github.com/users/mlucy"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/61743654"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1375#issuecomment-61743654"
  , issueCommentCreatedAt = 2014 (-11) (-05) 01 : 02 : 08 UTC
  , issueCommentBody =
      "The only thing I would say on this is that if we automatically convert streams to arrays, which I think we're still planning to do, then lots and lots of people will start running into situations where they're accidentally trying to load whole tables into memory.  Having some way to catch this case and tell people specifically what part of their query is at fault would be great.\r\n\r\nAn array size limit catches this better than a memory size limit because a memory size limit will be tripped at some random place in the query, whereas the array size limit will be tripped at the point where the table is converted to an array.  So the user will get back an error message saying the actual problem (they're trying to generate a huge array) and with the appropriate part of their query underlined."
  , issueCommentId = 61743654
  }