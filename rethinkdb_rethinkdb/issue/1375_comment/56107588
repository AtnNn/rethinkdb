IssueComment
  { issueCommentUpdatedAt = 2014 (-09) (-18) 21 : 42 : 18 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 48436
        , simpleUserLogin = N "coffeemug"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/48436?v=3"
        , simpleUserUrl = "https://api.github.com/users/coffeemug"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/56107588"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1375#issuecomment-56107588"
  , issueCommentCreatedAt = 2014 (-09) (-18) 21 : 42 : 18 UTC
  , issueCommentBody =
      "Here are my thoughts:\r\n\r\n- We need to solve the problem (more on the specific solution below) sooner rather than later. I talked to a user on Tue who wrote an analytics-style query, ran it on a large dataset, and had RethinkDB crash. The problem was the OOM killer because he was on a small-RAM VM, but he perceived it as RethinkDB crashing. This is a real problem that needs to get solved. Note that a datum limit wouldn't solve this specific problem (it was a non-indexed join that returned a selection; no huge datums involved).\r\n- I agree with @gchpaco that array limits make sense, per-query memory limits make sense, but per-datum memory limit are weird and hard to understand. I also agree that a partial solution might get us 90% of the way there, and we may not necessarily need to thread things through the mailboxes and such.\r\n- I also think that we should have an array limit *and* a per-query memory limit. Both make sense in different contexts. I don't think we should have a datum size limit if at all possible."
  , issueCommentId = 56107588
  }