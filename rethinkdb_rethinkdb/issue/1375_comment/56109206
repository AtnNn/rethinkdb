IssueComment
  { issueCommentUpdatedAt = 2014 (-09) (-18) 21 : 56 : 41 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 1777134
        , simpleUserLogin = N "mlucy"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/1777134?v=3"
        , simpleUserUrl = "https://api.github.com/users/mlucy"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/56109206"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1375#issuecomment-56109206"
  , issueCommentCreatedAt = 2014 (-09) (-18) 21 : 56 : 41 UTC
  , issueCommentBody =
      "> Note that a datum limit wouldn't solve this specific problem (it was a non-indexed join that returned a selection; no huge datums involved).\r\n\r\nI don't think that's true.  A non-indexed join creates an array, so a cumulative datum size limit would solve this.\r\n\r\n> I don't think we should have a datum size limit if at all possible.\r\n\r\nWhy's that?  The array size limit feels sort of arbitrary because you can have a 90,000 element array of integers or a 90,000 element array of nested 90,000 element arrays, and neither trips the limit.\r\n\r\n(Note that when we're talking about \"datum size limit\" in this issue, we mean \"number of elements\", not \"bytes of memory\", so `[{a: 1, b: 2}, {c: 3}]` would have a datum size of `3` because it has one two-element object and one one-element object.)"
  , issueCommentId = 56109206
  }