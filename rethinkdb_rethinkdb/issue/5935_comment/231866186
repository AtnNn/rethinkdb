IssueComment
  { issueCommentUpdatedAt = 2016 (-07) (-11) 21 : 11 : 03 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 7917
        , simpleUserLogin = N "bsharpe"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/7917?v=3"
        , simpleUserUrl = "https://api.github.com/users/bsharpe"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/231866186"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/5935#issuecomment-231866186"
  , issueCommentCreatedAt = 2016 (-07) (-11) 21 : 10 : 30 UTC
  , issueCommentBody =
      "@larkost Thank you for looking at this.\r\n\r\nWe initially had our cache size at the default -- 1/2 of our available 256G.   But we upped it to 200G after a long period of seeing minimal ram use.  Have since (based on https://github.com/rethinkdb/rethinkdb/issues/5934 ) turned it down to 100G.\r\n\r\nThen, we acquired some new customers who's datasets were very large.  We ended having to emergently increase the array_limit to 500,000 from 100,000 for a few specific queries just to get through the onboarding process.   We are currently working on ways around the array limit altogether in those queries.\r\n\r\nSo, perhaps it's just one of those queries that eats up the available ram."
  , issueCommentId = 231866186
  }