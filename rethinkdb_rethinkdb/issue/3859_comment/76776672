IssueComment
  { issueCommentUpdatedAt = 2015 (-03) (-02) 18 : 45 : 10 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 552910
        , simpleUserLogin = N "Tryneus"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/552910?v=3"
        , simpleUserUrl = "https://api.github.com/users/Tryneus"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/76776672"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/3859#issuecomment-76776672"
  , issueCommentCreatedAt = 2015 (-03) (-02) 18 : 45 : 10 UTC
  , issueCommentBody =
      "I think I see the problem.  This didn't actually occur on line '2' - we just iteratively parse JSON rows from the file (to keep track of progress), and it happened on the second line of a parse.  The problem itself appears to be that a single row is larger than 16 MB, and the import script fails in that case.  This could be solved by increasing the maximum size of the buffer in the script, but that will have some performance implications.\r\n\r\nI'll look into solving this without killing performance on very large rows."
  , issueCommentId = 76776672
  }