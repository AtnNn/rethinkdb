IssueComment
  { issueCommentUpdatedAt = 2015 (-03) (-02) 18 : 49 : 20 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 552910
        , simpleUserLogin = N "Tryneus"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/552910?v=3"
        , simpleUserUrl = "https://api.github.com/users/Tryneus"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/76777855"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/3859#issuecomment-76777855"
  , issueCommentCreatedAt = 2015 (-03) (-02) 18 : 49 : 04 UTC
  , issueCommentBody =
      "Ah, I just remembered one of the reasons we limited the maximum buffer size - some users were getting OOM-killed due to the import script using too much memory.  We can't keep arbitrarily reading more data into memory until the parse works - the system will run out of memory (and if the import is running on the same machine as the server, there's a good chance the OOM killer will target the server).  So we need some upper limit on the buffer.  Otherwise, on bad JSON input, we would keep buffering the file until we reach the EOF."
  , issueCommentId = 76777855
  }