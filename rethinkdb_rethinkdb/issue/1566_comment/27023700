IssueComment
  { issueCommentUpdatedAt = 2013 (-10) (-24) 19 : 44 : 31 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 43867
        , simpleUserLogin = N "jdoliner"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/43867?v=3"
        , simpleUserUrl = "https://api.github.com/users/jdoliner"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/27023700"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1566#issuecomment-27023700"
  , issueCommentCreatedAt = 2013 (-10) (-24) 19 : 44 : 31 UTC
  , issueCommentBody =
      "Copy pasted from #1567.\r\n\r\n> I think this is less of a problem than it appears because 99% of apps written with Rethink will open the connection, do some stufff, and close it for each web request\r\n\r\nNo this is just wrong up and down. For one thing we should not bank on one query per connection being a universal enough pattern that we consider leaking memory if people don't use it to be acceptable. For another this is just not true. Even if connection pools are rare right now I guarantee people are going to use them once there are good libraries that make them easy. These actually already exist in a somewhat nascent form. A good example is here: https://github.com/nviennot/nobrainer. This is a library that we recommend on our site and people seem to actually be using, it has a bug report from 10 days ago. It also has this bug. Every time you use `order_by` in nobrainer you'll be using a table's worth of memory on the server until your program terminates.\r\n\r\n> For `order_by`, what do we do about r.table(\"foo\").order_by(\"bar\").limit(5).delete()? It's a very common query, and returning an array would make it impossible (or much more complicated).\r\n\r\nThis isn't true. I'm not sure why you think it is.\r\n\r\nAs for objections above.\r\n\r\n > It's inconsistent. I believe unindexed order_by currently requires loading all data into RAM, but it doesn't return an array.\r\n\r\nObviously this is the whole point of the this bug report so it doesn't apply. As part of this issue we should make everything follow consistent rules.\r\n\r\n> There may be operations that must load all data into RAM to complete, but can't return an array because they have to return a selection (like order_by). This suggests that using arrays to denote operations that have to load all data into RAM isn't the right paradigm.\r\n\r\nAs mentioned above this isn't correct.\r\n\r\nOn the last 2 points I'll concede that this does indeed make the APIs more complicated but it does so in a very consistent way that makes it much safer. I've had users ask me for an easy way to tell which functions can use lots of memory and right now there's no easy way to tell which ones they are. With the API I'm proposing you'll know that queries only consume memory on the server while they're running (disregarding a very small constant overhead for streams). That's a really nice guarantee to have and it's really unsafe not to have it.\r\n\r\n> I would much rather wait to implement per-query memory limit. I think it's a better solution to this problem than returning arrays.\r\n\r\nThis isn't even close to a solution to this problem. Whatever limit we select you can still leak that much memory per query. How is that a solution? What memory limit are we going to select that we're okay leaking on each query and is still high enough to make the query language useful.\r\n\r\n---\r\n\r\nThis isn't how we write software, we're supposed to be the guys who don't leave landmines like this lying around to screw our users over. And if we were discussing this in the context of why a user had hit the oom killer I really doubt I would need to justify fixing it. I actually think there's a very good chance users are hitting this and we just haven't diagnosed it yet OOM related problems are one of the most commonly reported things and at this point we generally just say we have myriad memory problems we know about and we're working on. I really feel pretty strongly that this is something that needs to be fixed."
  , issueCommentId = 27023700
  }