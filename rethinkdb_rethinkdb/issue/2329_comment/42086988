IssueComment
  { issueCommentUpdatedAt = 2014 (-05) (-02) 22 : 53 : 28 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/42086988"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/2329#issuecomment-42086988"
  , issueCommentCreatedAt = 2014 (-05) (-02) 22 : 53 : 28 UTC
  , issueCommentBody =
      "Yeah we are still doing that.\r\n\r\nThis problem is not limited to dropping an index as it turns out. Everytime one starts a RethinkDB node with a sharded table, it runs an erase operation over all secondary indexes to make sure that no data exists that shouldn't be there (that is part of the reactor_be_nothing initialization).\r\n\r\nI currently have a four node cluster with a single table that doesn't start up anymore, because it runs out of memory after about 1-2 minutes.\r\nThe table contains ~900M small documents. While the cluster was running, the memory consumption was 19 GB. But now that it has been shut down once, just starting it up again makes it run out of memory on these 64 GB RAM machines."
  , issueCommentId = 42086988
  }