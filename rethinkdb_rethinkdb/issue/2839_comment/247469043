IssueComment
  { issueCommentUpdatedAt = 2016 (-09) (-15) 22 : 07 : 27 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 220191
        , simpleUserLogin = N "Iiridayn"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/220191?v=3"
        , simpleUserUrl = "https://api.github.com/users/Iiridayn"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/247469043"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/2839#issuecomment-247469043"
  , issueCommentCreatedAt = 2016 (-09) (-15) 22 : 07 : 27 UTC
  , issueCommentBody =
      "Just ran into this while developing a data migration script - currently the largest insert is 117,094 records. Workaround was to chunk them at 60k (70k didn't work) and insert in a loop."
  , issueCommentId = 247469043
  }