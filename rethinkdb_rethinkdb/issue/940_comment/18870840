IssueComment
  { issueCommentUpdatedAt = 2013 (-06) (-03) 21 : 00 : 02 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/18870840"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/940#issuecomment-18870840"
  , issueCommentCreatedAt = 2013 (-06) (-03) 21 : 00 : 02 UTC
  , issueCommentBody =
      "If the real fix doesn't make it into 1.6, I suggest adding a simple guarantee() to check for a block id overflow to the in-memory LBA code as a temporary work around. Crashing \"cleanly\" on an overflow is a lot better than quietly corrupting memory and potentially on-disk data.\r\n\r\nWith more people evaluating RethinkDB on real data, there's a chance that they hit this limit even without #939 (which would not do anything to the number of blocks used for storing a given amount of data anyways, would it?)."
  , issueCommentId = 18870840
  }