IssueComment
  { issueCommentUpdatedAt = 2013 (-05) (-16) 06 : 42 : 19 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 316661
        , simpleUserLogin = N "timmaxw"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/316661?v=3"
        , simpleUserUrl = "https://api.github.com/users/timmaxw"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/17984453"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/739#issuecomment-17984453"
  , issueCommentCreatedAt = 2013 (-05) (-16) 06 : 39 : 17 UTC
  , issueCommentBody =
      "Na na na na na na na na Batman!\r\n\r\nTwo solutions to this problem have been proposed. Here are my thoughts on both of them:\r\n\r\n#### Solution 1: Prevent duplicate machines IDs from ever being in the directory, by having machines die if they are duplicated ###\r\n\r\nSolution 1 can be implemented as follows: Add a \"sanity checker\" that reviews every copy of the directory before it is passed to the things that use the directory. If this \"sanity checker\" ever sees that there is another machine with the same machine ID as this machine, then it prints an error message and shuts down. If it sees that there are multiple other machines with the same machine ID, but neither is this machine, then it silently throws away that copy of the directory rather than passing it on to the things that use the directory.\r\n\r\nThis way, no code that uses the directory will ever see a version of the directory in which there are multiple machines with the same name. So we don't have to audit all the places where machines see duplicate peers.\r\n\r\nIt's true that directory updates won't propagate until one or both machines shut down, but that will happen quickly, and then the rest of the cluster can keep going. A directory update will be sent out when the machine shuts itself down, and that directory update will pass through the sanity checker, so the system will have no trouble \"catching up\" to whatever directory updates it missed.\r\n\r\nIf one of the machines shuts down but the other stays up, that's probably OK. The worst case is when the one that stays up has an old version of the data, and the blueprint specifies that there are no replicas for that namespace; this could lead to data divergence. But it's no worse than if the sysadmin had a master and a replica, then took down the replica and ran some queries, then took down the master and brought back up the replica and made it master.\r\n\r\nI suggest implementing the sanity checker as an object constructed on the stack in `do_serve()` that wraps `directory_read_manager.get_root_view()`. It would have a method called something like `get_sanitized_root_view()` that returned a `clone_ptr_t<watchable_t<std::map<peer_id_t, cluster_directory_metadata_t> > >` just like `get_root_view()`. Probably `network_logger` should use `get_root_view()` directly, because the logs will be necessary to diagnose duplicate-machine errors; we would have to audit `network_logger_t` to make sure it correctly handles duplicate machine IDs. But everything else besides the network logger and the sanity checker should use `get_sanitized_root_view()` rather than `get_root_view()`, so we wouldn't have to audit anything else.\r\n\r\nThis solution should work perfectly; I don't know of any edge cases that it fails to handle. It's also easy to implement. The only problem is that it isn't very user-friendly.\r\n\r\n#### Solution 2: Tolerate duplicate machine IDs in the directory, and show the user an issue ###\r\n\r\nSolution 2 is a lot of work to implement, because we have to audit every piece of code that cross-references machine IDs with peer IDs. But it's not quite as bad as it sounds. Here's a tentative but reasonably thorough list of things that would have to be done:\r\n* In addition to \"machine down\" issues and \"machine ghost\" issues, we need a \"machine duplicate\" issue.\r\n* `/ajax/log`, `/ajax/stat`, `/ajax/directory`, and `/ajax/progress` need to not crash if there are multiple peers with the same machine ID. An easy solution is to make them ignore all machines except for the one with the lowest peer ID. That would minimize changes to the web UI.\r\n* `clustering/administration/suggester.cc` needs to tolerate duplicate machine IDs. I think it can also ignore all but the one with the lowest peer ID.\r\n* I think that `auto_reconnect.cc` and `last_seen_tracker.cc` don't need to be changed, but somebody should read over them to make sure.\r\n* Something needs to change in `reactor_driver.tcc`. The safest option is that if `translate_blueprint()` sees a duplication, then `reactor_driver_t<protocol_t>::on_change()` should skip that namespace and move on to the next one, rather than creating/destroying the reactor or sending along the new blueprint. This means that no administrative operations will work on the namespace while a machine mentioned in the blueprint is duplicated.\r\n* The command-line UI might need to be updated. I'm not sure exactly how.\r\n* If both servers with the same machine ID write to the same vector clock, a value will be chosen arbitrarily. Unfortunately, it might be chosen differently in different places. One solution is to define a `<` operator for each type of metadata and then make the vector clocks always choose the lesser of the two values. That's a lot of work; alternatively, we could also leave it as is, which means that different machines might see different values for the metadata until another write is done to that vector clock. It will be weird, but I don't think it will lead to permanent corruption.\r\n* `machine_id_to_peer_id()` needs to do something other than crash if the map is not a bijection. One option is to throw an exception or return some special value. Alternatively, we could pick the lowest `peer_id_t`, because this is often the desired behavior. We should change the name and/or return type of `machine_id_to_peer_id()` to make sure that we fix every call site.\r\n\r\nThe advantage of solution 2 is that it's user-friendly. The user can easily see what went wrong because we show them an issue in the web UI. But I think the difficulty of implementing solution 2 and the risk of failing to catch some corner case outweigh the advantage of being more user-friendly. Overall, I think solution 1 is much better than solution 2. \r\n"
  , issueCommentId = 17984453
  }