IssueComment
  { issueCommentUpdatedAt = 2014 (-07) (-20) 03 : 15 : 01 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 1777134
        , simpleUserLogin = N "mlucy"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/1777134?v=3"
        , simpleUserUrl = "https://api.github.com/users/mlucy"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/49535854"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/2701#issuecomment-49535854"
  , issueCommentCreatedAt = 2014 (-07) (-20) 03 : 15 : 01 UTC
  , issueCommentBody =
      "I realize this is a point of contention, but I think that for 99% of bugfixes (including #2696) we should just make the fix.  The case you're worried about is one where someone has secondary index that uses `insert_at` to create a 100,001-element array, and then does something meaningful with that array which manages to not trip the array size limit.  I think the chances of such a person existing are so low that we shouldn't adopt the extra maintenance burden.  (Or, to make a utilitarian argument, the time we save from not adopting the extra maintenance burden will allows us to fix other bugs that have a much higher chance of actually affecting people.)\r\n\r\nObviously some other changes **would** require thinking about backwards-compatibility."
  , issueCommentId = 49535854
  }