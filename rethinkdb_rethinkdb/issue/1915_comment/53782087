IssueComment
  { issueCommentUpdatedAt = 2014 (-08) (-28) 19 : 23 : 58 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 316661
        , simpleUserLogin = N "timmaxw"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/316661?v=3"
        , simpleUserUrl = "https://api.github.com/users/timmaxw"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/53782087"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1915#issuecomment-53782087"
  , issueCommentCreatedAt = 2014 (-08) (-28) 19 : 23 : 58 UTC
  , issueCommentBody =
      "Consider switching temporarily to 64-bit offsets just to see how it affects performance. That will give you a sense of whether the multi-format option is worth the complexity. (Unless you're already sure.)\r\n\r\n> Creating a datum_t that exceeds the size limit would trigger a ReQL failure.\r\n\r\nI think this is good in the long term, but the backwards compatibility break is awkward. In fact, I think we should impose a limit for new datums starting soon, to make it easier to make this switch later.\r\n\r\n> The other disadvantage is a waste of 8 bytes of RAM for each datum_t object.\r\n\r\nWe can cache serialized size for arrays, objects, and buffer-backed datums, and compute it on the fly for all other datum types. This would reduce the memory overhead.\r\n"
  , issueCommentId = 53782087
  }