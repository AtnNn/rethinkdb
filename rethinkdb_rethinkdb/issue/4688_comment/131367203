IssueComment
  { issueCommentUpdatedAt = 2015 (-08) (-17) 09 : 36 : 51 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 4540763
        , simpleUserLogin = N "JohnyDays"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/4540763?v=3"
        , simpleUserUrl = "https://api.github.com/users/JohnyDays"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/131367203"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/4688#issuecomment-131367203"
  , issueCommentCreatedAt = 2015 (-08) (-15) 12 : 35 : 30 UTC
  , issueCommentBody =
      "The explanation makes perfect sense! I ended up switching over to a between (indexed by the timestamp) instead of that limit, which did not have the same performance hit. \r\nAlso, I attempted to run the query (the one from my first post) on a server with about 3 million entries, and I ran OOM, is that also expected? I would assume the query would only load the necessary amount to satisfy the 100 per foreign key (which had about 5 entries) so only a total of about 500, but it seemed to load the entire dataset. Probably because I misunderstood how something works"
  , issueCommentId = 131367203
  }