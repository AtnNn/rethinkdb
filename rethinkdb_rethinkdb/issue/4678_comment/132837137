IssueComment
  { issueCommentUpdatedAt = 2015 (-08) (-20) 00 : 35 : 34 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 1276278
        , simpleUserLogin = N "williamstein"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/1276278?v=3"
        , simpleUserUrl = "https://api.github.com/users/williamstein"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/132837137"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/4678#issuecomment-132837137"
  , issueCommentCreatedAt = 2015 (-08) (-20) 00 : 35 : 34 UTC
  , issueCommentBody =
      "I rewrote my code to optionally use the native driver instead of rethinkdbdash (and also just wrote my own threadpool system for better control/monitoring).  I've run a lot of tests in the last day, and I guess the main issues that I reported can be worked around.   I just started hitting this new traceback/crash a few seconds after starting one of the main nodes in my testing cluster:\r\n\r\n```\r\n2015-08-19T23:56:52.203766831 26.607850s error: Error in src/clustering/query_routing/table_query_client.cc at line 89:\r\n2015-08-19T23:56:52.203808424 26.607891s error: Guarantee failed: [!r.route_to_primary()]\r\n2015-08-19T23:56:52.203816429 26.607899s error: Backtrace:\r\n2015-08-19T23:56:52.318109819 26.722195s error: Wed Aug 19 23:56:52 2015\r\n\r\n1: backtrace_t::backtrace_t() at ??:?\r\n2: format_backtrace(bool) at ??:?\r\n3: report_fatal_error(char const*, int, char const*, ...) at ??:?\r\n4: table_query_client_t::read(read_t con\r\nst&, read_response_t*, order_token_t, signal_t*) at ??:?\r\n5: real_table_t::read_with_profile(ql::env_t*, read_t const&, read_response_t*) at ??:?\r\n6: ql::rget_response_reader_t::do_read(ql::env_t*, read_t const&) at ??:?\r\n7: ql::rget_reader_t::do_range_read\r\n(ql::env_t*, read_t const&) at ??:?\r\n8: ql::rget_reader_t::load_items(ql::env_t*, ql::batchspec_t const&) at ??:?\r\n9: ql::rget_response_reader_t::next_batch(ql::env_t*, ql::batchspec_t const&) at ??:?\r\n10: ql::lazy_datum_stream_t::next_batch_impl(ql::env_t*\r\n, ql::batchspec_t const&) at ??:?\r\n11: ql::datum_stream_t::next_batch(ql::env_t*, ql::batchspec_t const&) at ??:?\r\n12: ql::changefeed::splice_stream_t::next_stream_batch(ql::env_t*, ql::batchspec_t const&) at ??:?\r\n13: ql::changefeed::stream_t<ql::changefee\r\nd::range_sub_t>::next_raw_batch(ql::env_t*, ql::batchspec_t const&) at ??:?\r\n14: ql::eager_datum_stream_t::next_grouped_batch(ql::env_t*, ql::batchspec_t const&, std::map<ql::datum_t, std::vector<ql::datum_t, std::allocator<ql::datum_t> >, optional_datum_le\r\nss_t, std::allocator<std::pair<ql::datum_t const, std::vector<ql::datum_t, std::allocator<ql::datum_t> > > > >*) at ??:?\r\n15: ql::eager_datum_stream_t::next_batch_impl(ql::env_t*, ql::batchspec_t const&) at ??:?\r\n16: ql::datum_stream_t::next_batch(ql::env_t\r\n*, ql::batchspec_t const&) at ??:?\r\n17: ql::coro_stream_t::cb(auto_drainer_t::lock_t) at ??:?\r\n18: callable_action_instance_t<ql::coro_stream_t::maybe_launch_read()::{lambda()#2}>::run_action() at ??:?\r\n19: coro_t::run() at ??:?\r\n2015-08-19T23:56:52.318167726 26.722251s error: Exiting.\r\n```\r\n\r\nI'm just testing making one changefeed using a getAll query with an index (over a few hundred keys).\r\nI'm using this binary: rethinkdb_2.1.1+1+g989a97-0vivid_amd64.deb, which was emailed to me. The above crash isn't at all random.  It happens every time I try to do this query (I realize this may mean little without more info about the schema):\r\n\r\n```\r\ntable('file_use').getAll(ids..., index:'project_id').pluck('project_id', 'path', 'users', 'last_edited').changes(includeStates: false).\r\n```\r\n\r\nMy plan at this point is to rewrite my application to use only the simplest features of changefeeds, namely listening for all changes to a table (which works without crashing), and basically re-implement the same idea as changefeeds (but in node.js) in a way that is rich enough to support my application.  Even when things don't crash, the performance of doing certain more complicated queries like above is such that I really have to just move more to be in memory in my web server, unfortunately. (Obviously, it's possible I'm just naive, though I've spent a lot of time now trying a lot of different approaches and benchmarks systematically...)   "
  , issueCommentId = 132837137
  }