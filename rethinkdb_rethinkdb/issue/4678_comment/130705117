IssueComment
  { issueCommentUpdatedAt = 2015 (-08) (-13) 14 : 39 : 40 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 1276278
        , simpleUserLogin = N "williamstein"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/1276278?v=3"
        , simpleUserUrl = "https://api.github.com/users/williamstein"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/130705117"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/4678#issuecomment-130705117"
  , issueCommentCreatedAt = 2015 (-08) (-13) 14 : 39 : 40 UTC
  , issueCommentBody =
      "I switched to 2.0.4 as mentioned above, and was just rudely awaken when my 2.0.4 one-node server (with SSD, 32GB RAM, 8 cores) crashed with:\r\n\r\n```\r\n2015-08-13T04:21:46.948300789 0.239325s notice: Server ready, \"dbstable\" afe6a4c9-92d2-4add-9bf0-1648507b3efc\r\n2015-08-13T13:55:34.076442491 34427.367467s error: Error in src/rdb_protocol/changefeed.cc at line 1476:\r\n2015-08-13T13:55:34.076478044 34427.367503s error: Guarantee failed: [active()]\r\n2015-08-13T13:55:34.076488347 34427.367513s error: Backtrace:\r\n2015-08-13T13:55:34.239396061 34427.530424s error: Thu Aug 13 13:55:34 2015\\n\\n1: backtrace_t::backtrace_t() at ??:?\\n2: format_backtrace(bool) at ??:?\\n3: rep\r\nort_fatal_error(char const*, int, char const*, ...) at ??:?\\n4: ql::changefeed::range_sub_t::apply_ops(ql::datum_t) at ??:?\\n5: ql::changefeed::msg_visitor_t::\r\noperator()(ql::changefeed::msg_t::change_t const&) const::{lambda(ql::changefeed::range_sub_t*)#1}::operator()(ql::changefeed::range_sub_t*) const at ??:?\\n6:\r\nvoid ql::changefeed::feed_t::each_sub_in_vec_cb<ql::changefeed::range_sub_t>(std::function<void (ql::changefeed::range_sub_t*)> const&, std::vector<std::set<ql\r\n::changefeed::range_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::allocator<ql::changefeed::range_sub_t*> >, std::allocator<std::set<ql::changefeed::ra\r\nnge_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::allocator<ql::changefeed::range_sub_t*> > > > const&, std::vector<int, std::allocator<int> > const&,\r\nint) at ??:?\\n7: callable_action_instance_t<pmap_runner_one_arg_t<std::_Bind<std::_Mem_fn<void (ql::changefeed::feed_t::*)(std::function<void (ql::changefeed::\r\nrange_sub_t*)> const&, std::vector<std::set<ql::changefeed::range_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::allocator<ql::changefeed::range_sub_t*>\r\n >, std::allocator<std::set<ql::changefeed::range_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::allocator<ql::changefeed::range_sub_t*> > > > const&, s\r\ntd::vector<int, std::allocator<int> > const&, int)> (ql::changefeed::feed_t*, std::reference_wrapper<std::function<void (ql::changefeed::range_sub_t*)> const>,\r\n std::reference_wrapper<std::vector<std::set<ql::changefeed::range_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::allocator<ql::changefeed::range_sub_t*\r\n> >, std::allocator<std::set<ql::changefeed::range_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::allocator<ql::changefeed::range_sub_t*> > > > const>,\r\nstd::reference_wrapper<std::vector<int, std::allocator<int> > const>, std::_Placeholder<1>)>, long> >::run_action() at ??:?\\n8: coro_t::run() at ??:?\r\n2015-08-13T13:55:34.239486888 34427.530511s error: Exiting.\r\n```\r\n\r\nHere:\r\n\r\n```\r\nsalvus@db-stable:/var/lib/rethinkdb/default/data$ rethinkdb -v\r\nrethinkdb 2.0.4~0vivid (GCC 4.9.2)\r\n```\r\n\r\nTrying to restart again resulted in\r\n\r\n```\r\n2015-08-13T14:10:05.151074210 0.259386s notice: Listening on addresses: 127.0.0.1, 10.240.186.14\r\n2015-08-13T14:10:05.151091344 0.259402s notice: Server ready, \"dbstable\" afe6a4c9-92d2-4add-9bf0-1648507b3efc\r\n2015-08-13T14:10:13.970131861 9.078444s error: accept() failed: Too many open files.\r\n2015-08-13T14:11:58.017734655 113.126046s notice: Server got SIGINT from pid 3901, uid 0; shutting down...\r\n2015-08-13T14:11:58.017819601 113.126130s notice: Shutting down client connections...\r\n2015-08-13T14:11:58.017945535 113.126257s error: Error in src/errors.cc at line 125:\r\n2015-08-13T14:11:58.017971071 113.126282s error: Uncaught exception of type \"interrupted_exc_t\"\\n  what(): interrupted\r\n2015-08-13T14:11:58.017986454 113.126297s error: Backtrace:\r\n2015-08-13T14:11:58.020727180 113.129038s error: Thu Aug 13 14:11:58 2015\\n\\n1: backtrace_t::backtrace_t() at 0xa381f0 (/usr/bin/rethinkdb)\\n2: format_backtrac\r\ne(bool) at 0xa385bc (/usr/bin/rethinkdb)\\n3: report_fatal_error(char const*, int, char const*, ...) at 0xc217f3 (/usr/bin/rethinkdb)\\n4: terminate_handler() at\r\n 0xc21a7e (/usr/bin/rethinkdb)\\n5: /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0x5eee6) [0x7fef60afdee6] at 0x7fef60afdee6 (/usr/lib/x86_64-linux-gnu/libstdc++.s\r\no.6)\\n6: /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0x5ef31) [0x7fef60afdf31] at 0x7fef60afdf31 (/usr/lib/x86_64-linux-gnu/libstdc++.so.6)\\n7: /usr/lib/x86_64-l\r\ninux-gnu/libstdc++.so.6(+0x5f149) [0x7fef60afe149] at 0x7fef60afe149 (/usr/lib/x86_64-linux-gnu/libstdc++.so.6)\\n8: wait_interruptible(signal_t const*, signal_\r\nt const*) at 0xa43620 (/usr/bin/rethinkdb)\\n9: nap(long, signal_t const*) at 0x942999 (/usr/bin/rethinkdb)\\n10: linux_nonthrowing_tcp_listener_t::accept_loop(a\r\nuto_drainer_t::lock_t) at 0x937ec4 (/usr/bin/rethinkdb)\\n11: callable_action_instance_t<std::_Bind<std::_Mem_fn<void (linux_nonthrowing_tcp_listener_t::*)(auto\r\n_drainer_t::lock_t)> (linux_nonthrowing_tcp_listener_t*, auto_drainer_t::lock_t)> >::run_action() at 0x93c3b0 (/usr/bin/rethinkdb)\\n12: coro_t::run() at 0x94d2\r\n18 (/usr/bin/rethinkdb)\r\n2015-08-13T14:11:58.020757903 113.129069s error: Exiting.\r\n```\r\n\r\nThen I tried again (no reboot), and it's now been running fine for 20 minutes.\r\n\r\nRethinkdb itself is not using a lot of CPU (this server is dedicated to running rethinkdb):\r\n\r\n```\r\nsalvus@db-stable:/var/lib/rethinkdb/default/data$ uptime\r\n 14:32:15 up 10:16,  3 users,  load average: 0.01, 0.27, 0.31\r\n```\r\n\r\nAny thoughts?   Note that I haven't tried the new 2.1 binary you sent me as I just woke up.  Also just \"trying it\" isn't straightforward since right now my only way to test is to use it in production...\r\n\r\nIf the fix you used there is the same as what you would use to fix 2.0.4 (since the first error above is basically the same), would it be hard to fix it in a 2.0.4 binary that I could just swap in, try, and report what happens after 24 hours of running it live?  "
  , issueCommentId = 130705117
  }