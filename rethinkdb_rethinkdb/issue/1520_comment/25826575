IssueComment
  { issueCommentUpdatedAt = 2013 (-10) (-07) 17 : 08 : 53 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 43867
        , simpleUserLogin = N "jdoliner"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/43867?v=3"
        , simpleUserUrl = "https://api.github.com/users/jdoliner"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/25826575"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1520#issuecomment-25826575"
  , issueCommentCreatedAt = 2013 (-10) (-07) 17 : 08 : 53 UTC
  , issueCommentBody =
      "There are a couple of possible levels of optimization for `sample`. `sample` in the general case is a linear time operation and right now we only implement the general case.\r\n\r\nThe first optimization we can make is rewriting it in terms of map reduce. This requires some anaphoric macro magic but other than that it's not too hard.\r\n\r\nThe next step would be optimizing `sample` on tables so that it doesn't read data off disk unless it actually selects the row. This might prove to be a little tricky but shouldn't be impossible.\r\n\r\nThe last step would be to do a BTree aware optimization which could sample rows in logarithmic time. This one is quite challenging so I don't think it's worth doing any time too soon."
  , issueCommentId = 25826575
  }