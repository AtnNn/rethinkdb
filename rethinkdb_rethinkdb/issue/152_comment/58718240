IssueComment
  { issueCommentUpdatedAt = 2014 (-10) (-10) 21 : 23 : 49 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 316661
        , simpleUserLogin = N "timmaxw"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/316661?v=3"
        , simpleUserUrl = "https://api.github.com/users/timmaxw"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/58718240"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/152#issuecomment-58718240"
  , issueCommentCreatedAt = 2014 (-10) (-10) 21 : 23 : 49 UTC
  , issueCommentBody =
      "The long-term solution is incremental backfilling. We would transfer data from one shard to another in small chunks; we would lose availability for each chunk as it was being transferred, but that's a small key range for a fraction of a second.\r\n\r\nI think it would be possible to keep `count` up to date in the way you're suggesting. We would update the counts incrementally as queries are performed; count keys transferred during a backfill; and do a linear count if a shard boundary is moved on the same machine. But I think it would be complicated and messy, in part because the proposed solution requires the B-tree logic to know where the shard boundaries are, which it currently doesn't. So I'd rather not implement it that way."
  , issueCommentId = 58718240
  }