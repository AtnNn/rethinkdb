IssueComment
  { issueCommentUpdatedAt = 2013 (-08) (-13) 21 : 40 : 33 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 258437
        , simpleUserLogin = N "srh"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/258437?v=3"
        , simpleUserUrl = "https://api.github.com/users/srh"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/22599798"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1271#issuecomment-22599798"
  , issueCommentCreatedAt = 2013 (-08) (-13) 21 : 40 : 33 UTC
  , issueCommentBody =
      "> Having `r.table('foo').between(null, null, index='bar').count()` be efficient is probably several orders of magnitude harder than implementing incremental map reduce.\r\n\r\nWe can implement it by storing subtree rowcount estimates with upper and lower bounds on a per-node basis in memory, storing exact subtree rowcount values in the LBA (alongside the repli_timestamp_t value), and having a count query block until the count estimates converge.\r\n"
  , issueCommentId = 22599798
  }