IssueComment
  { issueCommentUpdatedAt = 2016 (-01) (-27) 06 : 26 : 47 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/175394540"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/5289#issuecomment-175394540"
  , issueCommentCreatedAt = 2016 (-01) (-27) 04 : 47 : 36 UTC
  , issueCommentBody =
      "# The short version #\r\nWe have found the bug that's causing this! The fix is currently going through final review and testing.\r\n\r\nWe hope to ship RethinkDB 2.2.4 with the bug fix by the end of this week.\r\n\r\n__Workaround (updated):__ While the problem seems to be unlikely to occur in practice, we recommend not performing any cluster reconfiguration while either\r\n* any server of the cluster is down or\r\n* any number of servers are unreachable because of a network partition\r\n\r\nuntil then.\r\n\r\n\r\nBelow you can find the long description of this bug, as we think that some of you might be interested in the technical details of this.\r\n\r\n\r\n# The long version / Introduction #\r\nAs part of @aphyr's continuing work on testing RethinkDB using his [Jepsen](https://github.com/aphyr/jepsen) test framework, we were made aware of an issue that caused RethinkDB to return incorrect results, and to drop already acknowledged writes.\r\n\r\nRethinkDB - to the best of our knowledge - *does* provide full [linearizable consistency](http://rethinkdb.com/docs/consistency/) when using the documented configuration and not performing any manual reconfigurations. These guarantees are upheld under failure of individual servers as well as arbitrary network partitions. A [recent analysis](https://aphyr.com/posts/329-jepsen-rethinkdb-2-1-5) by @aphyr supported RethinkDB's correctness under the tested scenarios.\r\n\r\nThis bug affects scenarios where a user performs reconfiguration of the cluster in the presence of network partitions. Reconfiguration in this context refers to changes to the `r.db('rethinkdb').table('table_config')` system table, or the use of the `reconfigure` or `rebalance` commands.\r\n\r\nUnder the right circumstances, RethinkDB 2.2.3 and earlier can violate the documented consistency and write persistence guarantees.\r\n\r\nWe are not aware of a single user who has been affected by this bug, and the issue requires a particular combination of factors in order to generate incorrect behavior.\r\n\r\nThe following provides an in-depth analysis of the bug.\r\n\r\nWe would like to thank @aphyr for his help in reproducing the issue and in tracking down potential causes.\r\n\r\n# Background #\r\nHere we provide a simplified explanation of RethinkDB's cluster architecture.\r\n\r\nWe try to provide enough information to understand the bug, but will leave out a lot of detail and simplify certain processes for the sake of not letting this become even longer than it already is.\r\n\r\n## RethinkDB's cluster architecture ##\r\nRethinkDB's cluster architecture has three major components.\r\n* A Raft-based system to manage the configuration state of a table\r\n* The \"multi table manager\" to make servers in the cluster aware of their duties, according to the current table configuration from the Raft cluster\r\n* A system to store and replicate the actual data and queries.\r\n\r\nWe'll take a closer look at the first two components here:\r\n\r\n### Raft ###\r\nRethinkDB uses [Raft](https://raft.github.io/) to maintain a consistent configuration state for a given table. Typically, all the replicas you configure for a table will become members of a Raft cluster (sometimes called Raft ensemble) specific to that table. Most importantly, Raft is used in RethinkDB to ensure that the replicas of the table agree on which server should be the current primary. This makes sure that no two servers can successfully operate as a primary at the same time, and this is what allows RethinkDB to provide [linearizable consistency](http://rethinkdb.com/docs/consistency/).\r\n\r\nRaft is structured around the concept of a quorum. If there are 5 replicas for a table for example, at least 3 of them have to agree on a configuration change before it can take effect. This property ensures that no two configuration changes can happen without at least one replica knowing about both of them, even under arbitrary network partitions. If the two configurations would lead to conflicting outcomes (e.g. each one designating a different replica as the primary for the table), that replica would \"veto\" the second one and thereby make sure that no illegal configuration can ever take effect. (In reality replicas don't actually veto a configuration, but instead vote to elect a Raft leader. The result is the same).\r\n\r\nAnother important component of Raft is the concept of a persistent log. At different points during the Raft protocol, the replicas need to persist state to disk, and guarantee that it will still be there at a later time. Similar to the quorum concept, this guarantee is crucial for Raft to function properly.\r\n\r\n### The multi table manager and Raft membership ###\r\nAs long as you don't create or delete tables, or manually [reconfigure](http://rethinkdb.com/api/javascript/reconfigure/) an existing table, all configuration management pretty much happens in the Raft cluster.\r\n\r\nHowever what happens if you for example use `reconfigure` to add a new replica to a table? Let's say a table currently has the replicas `{A, B}` and we now want to add a third replica `C`. The `reconfigure` command will contact one of the existing replicas and ask them to change the table configuration. This configuration change happens through the Raft protocol, and `{A, B}` will communicate to agree on a new configuration that includes `C` to form a new replica set `{A, B, C}`.  However `C` is not currently part of the Raft cluster for the table. Hence it has no way of learning about this change.\r\n\r\nThis is where the multi table manager comes into play. There is always one instance of the multi table manager running on each RethinkDB server. Once the multi table manager on `A` or `B` learns about the new replica `C` from the Raft cluster, it will reach out to the multi table manager on `C` to tell it about that change. `C` can now join the Raft cluster and start serving as a full replica for the table.\r\n\r\nSimilarly, if `C` is currently unreachable (e.g. because of a network failure), and you perform another reconfiguration to reduce the replica set back to `{A, B}`, the multi table manager on `C` will get notified by the ones running on `A` and `B` to stop being a replica for the table once `C` becomes reachable again.\r\n\r\nEvery member of the Raft cluster is identified by a unique \"Raft ID\". When a new replica is made to join the cluster, the current members of the Raft cluster will generate a random ID for the new replica. This member ID is communicated through the multi table manager to the new replica, which then uses it to join the Raft cluster.\r\n\r\nIf a server joins the Raft cluster with a member ID that has been seen in the cluster\r\nbefore, the other replicas in the Raft cluster will assume that the server has participated in the cluster before and is now coming back, with all of its persistent log intact. In contrast a newly added replica will receive a fresh random ID, and hence the other members of the Raft cluster will know that it's a new node without any prior data in its persistent log.\r\n\r\n# Where things went wrong #\r\nTo understand what went wrong, we need to take a closer look at some details of the multi table manager.\r\n\r\nThe multi table manager relies on timestamps to determine which configuration for a table is the most recent. When it receives some new information from another multi table manager (e.g. that the server is now a replica for a table like in our example above), it compares the timestamp of that new piece of information with the timestamp of the table state that it has currently stored. If the received information is older than the currently stored one, it is ignored. Only if it's newer, the locally stored information is replaced and the multi table manager takes additional actions to become a replica for\r\na table or cease being a replica for a table.\r\n\r\nHowever there is one exception to this rule, as you can see in [this part](https://github.com/rethinkdb/rethinkdb/blob/v2.2.3-1/src/clustering/table_manager/multi_table_manager.cc#L330) of its source code.\r\n```C++\r\n/* If we are inactive and are told to become active, we ignore the\r\ntimestamp. The `base_table_config_t` in our inactive state might be\r\na left-over from an emergency repair that none of the currently active\r\nservers has seen. In that case we would have no chance to become active\r\nagain for this table until another emergency repair happened\r\n(which might be impossible, if the table is otherwise still available). */\r\nbool ignore_timestamp =\r\n    table->status == table_t::status_t::INACTIVE\r\n    && action_status == action_status_t::ACTIVE;\r\n```\r\n\r\nWhat that code is saying is that if the multi table manager currently believes that the server it's running on should *not* be a replica for a table (the INACTIVE status), and then it learns from another multi table manager that it *should* be a replica (the ACTIVE status), it accepts that new ACTIVE status even if the new status has an older timestamp than the INACTIVE status that it previously knew about.\r\n\r\nThis special case was added in RethinkDB 2.1.1 as part of [issue #4668](https://github.com/rethinkdb/rethinkdb/issues/4668) to work around a scenario that caused tables to never finish any reconfiguration, if the table had previously been migrated to RethinkDB 2.1.0 from RethinkDB 2.0.x or earlier. The reason this became necessary was because the migration code sometimes generated INACTIVE entries with wrong timestamps that were far in the future, and hence any server with such an entry in its multi table manager could never become ACTIVE again.\r\n\r\nThis so far isn't an issue. Let's however take a closer look at what the multi table manager does if it processes an INACTIVE status. As one part of that process, the multi table manager writes the INACTIVE state to disk by calling the `write_metadata_inactive` function. You can find the full implementation of that function [here](https://github.com/rethinkdb/rethinkdb/blob/v2.2.3-1/src/clustering/administration/persist/table_interface.cc#L240), but note this line in particular:\r\n```C++\r\ntable_raft_storage_interface_t::erase(&write_txn, table_id);\r\n```\r\n\r\nThis line erases the Raft storage of the table, which includes the Raft persistent log among other data.\r\n\r\n## Putting things together ##\r\nWe now have all the ingredients to understand the basic mode of the bug.\r\n\r\nRemember:\r\n* We rely on Raft to ensure consistency of table configurations and, indirectly, of the stored data\r\n* A Raft member is identified by its member ID. For a given member ID, the Raft member must ensure that no entry written to its persistent log gets lost.\r\n* If the multi table manager processes an ACTIVE status, it causes the server to join the Raft cluster for the table with the member ID provided in the status.\r\n* If the multi table manager processes an INACTIVE status, it stops the replica and erases the persistent Raft log.\r\n\r\nAfter processing an INACTIVE status, the only way for a multi table manager to later process an ACTIVE status is if that ACTIVE status has a higher timestamp. The timestamps are generated by the current members of the Raft cluster for the table. The same code generates the Raft member ID that gets put into the ACTIVE status.\r\n\r\nThe code makes sure that it *never* generates a sequence of ACTIVE, INACTIVE, ACTIVE status where each one has a higher timestamp than the previous one, *and* both ACTIVE status have the same Raft member ID. If you reconfigure a table first to remove a replica, and then reconfigure it again to add the same replica back, the second ACTIVE status will have a different Raft member ID. So things should be safe.\r\n\r\n... but wait a minute. We saw that there was *one* exception where the multi table manager *does* process an ACTIVE status even though its timestamp is not higher than that of a previously received INACTIVE status.\r\n\r\nAnd this is indeed where the bug lies. If for whatever reason (network delays, network partitions, etc.) a multi table manager receives an ACTIVE status first, then receives an INACTIVE status with a higher timestamp, and then receives the initial ACTIVE status a second time, it will process the second copy of the ACTIVE status. Both ACTIVE status have the same Raft member ID, but the INACTIVE status in between has wiped out the persistent log. And we know that Raft cannot properly function if a member comes back with the same member ID, but a different (in this case empty) log.\r\n\r\n## Example sequence of events ##\r\nA couple of things have to come together for this to actually matter and cause split-brain behavior (two primaries accepting queries at the same time) and/or data loss.\r\n\r\nSo far we've only come up with scenarios that involve a combination of table reconfigurations and network partitions, though that doesn't mean that no other scenarios exist.\r\n\r\nHere is a rough sketch of one such scenario:\r\nConsider a cluster of give nodes {A, B, C, D, E}, and a table in that cluster.\r\nWe will denote the configuration of the table in terms of its replica set as stored in the Raft cluster, as well as the current connectivity groups of the network where applicable.\r\n\r\n1. Initial configuration. All servers are a replica. The network is fully connected.\r\nReplicas: {A, B, C, D, E}\r\nNetwork: {A, B, C, D, E}\r\n2. The network gets partitioned. {A, B} can no longer see {C, D, E} and vice versa.\r\nReplicas: {A, B, C, D, E}\r\nNetwork: {A, B}, {C, D, E}\r\n3. Reconfigure the table to a new replica set {D, E} with a client connected to the {C, D, E} side of the network partition. Note that {A, B} are not aware of this configuration change at this point, but {C, D, E} can apply the change because they have a quorum (i.e. at least 3 out of 5 replicas).\r\nReplicas: {A, B, C, D, E} as seen from {A, B}, {D, E} as seen from {C, D, E}\r\nNetwork: {A, B}, {C, D, E}\r\n4. As a consequence of the reconfiguration, C receives an INACTIVE status and wipes out its persistent Raft log.\r\nNetwork: {A, B}, {C, D, E}\r\n5. Repartition the network into different connected sets {A, B, C}, {D, E}\r\nNetwork: {A, B, C}, {D, E}\r\n6. Since A and B still believe that the replica set is {A, B, C, D, E}, their multi table manager send an ACTIVE status to C.\r\nNetwork: {A, B, C}, {D, E}\r\n7. Because of the bug, C accepts the ACTIVE status and rejoins the Raft cluster with {A, B} (it also would like to join with {D, E}, but since the network is still partitioned, it can't reach those servers).\r\nNetwork: {A, B, C}, {D, E}\r\n8. In step 3, C had approved the membership change that excluded [A, B] from the replica set. However because its Raft log is now gone, there is no longer any record of that change in the connected set {A, B, C}.\r\nNetwork: {A, B, C}, {D, E}\r\n9. {A, B, C} (incorrectly) believe that they have a Raft quorum. {A, B} still assume the original replica set from step 1, i.e.  {A, B, C, D, E} since they were cut off from the network when the change happened in step 3. Together with C, they can obtain a quorum since they have 3 out of 5 replicas.\r\n10. At the same time, {D, E} (legitimately) believe that they have a quorum as well. In their case their quorum is within the replica set {C, D, E}, i.e. they have 2 out of 3 replicas.\r\n11. Both {A, B, C} and {D, E} can now independently perform subsequent configuration changes. For example they could both elect a primary of their own. Both primaries would independently accept write and read operations on both sides of the netsplit.\r\n\r\n# The fix #\r\nIn this case the fix is rather straight forward. We simply remove the special override for the timestamp comparison in the multi table manager. The multi table manager is only going to process an ACTIVE status if it has a higher timestamp than any previously received status. Together with the way these status are generated, this ensures that any processed ACTIVE status will have a new Raft member ID.\r\n\r\nYou can find the new code [here](https://github.com/rethinkdb/rethinkdb/blob/daniel_small_clustering_fixes/src/clustering/table_manager/multi_table_manager.cc#L343).\r\n\r\nThis introduces a regression for users who migrated to RethinkDB 2.1.0 at some point and either are still running RethinkDB 2.1.0, or haven't reconfigured their tables to utilize all servers in their cluster since the initial migration. We expect that the number of users affected by this will be extremely small.\r\n\r\nIf you observe replicas that never become ready after a reconfiguration, and you find messages of the form `Not adding a replica on this server because the active configuration conflicts with a more recent inactive configuration.` in the server log, you can use the following command to allow the  table to complete the reconfiguration:\r\n```js\r\nr.db(<db>).table(<table>).reconfigure({emergencyRepair: \"_debug_recommit\"})\r\n```\r\nWe highly advise to disconnect any clients before running this command. As with all `emergencyRepair` commands, the `_debug_recommit` command does not guarantee linearizable consistency during the reconfiguration.\r\n\r\n# Lessons learned #\r\nDistributed systems are highly complex and designing them to be safe under any sort of edge case is a difficult undertaking. Incidentally this is the reason for why we decided to base our cluster architecture around the proven (literally, in a mathematical sense) Raft protocol, rather than designing our own consensus protocols from scratch.\r\n\r\nAs we've seen, the bug occurred in an auxiliary component that interacted with the Raft subsystem in a way that we didn't anticipate when we made the change that introduced the bug.\r\n\r\nApart from an increased general caution whenever future changes to one of these systems are necessary, there are three things in particular that we learned while researching this bug. These measures will make a similar bug much less likely to occur again:\r\n\r\n1. More fuzz testing of the clustering architecture.\r\nDesigning distributed systems is hard, but testing them isn't much easier. @aphyr's [Jepsen series](http://aphyr.com/tags/Jepsen) on the correctness of distributed databases has some great insights into what it means to perform sophisticated randomized testing of such systems.\r\nWe have previously used an adapted version of the Jepsen test framework internally, in addition to our own fuzzing tests. Recently, @aphyr wrote about his own [analysis of RethinkDB](https://aphyr.com/posts/329-jepsen-rethinkdb-2-1-5), and we are happy that RethinkDB turned out to be one of the very few systems tested so far that behaved correctly under the tested conditions.\r\nKyle has now extended his test to include configuration changes during network partitions. This is a completely new aspect, and his preliminary results of this test were what made us aware of the existence of this bug. We're going to continue making use of the extended version of the Jepsen test for RethinkDB in the future.\r\n\r\n2. Making better use of runtime guarantees in the code. Our Raft implementation performs a number of consistency checks at runtime, and shuts down the server if it detects any issues. This is an effective safety mechanism to stop invalid state from leading to further consistency and data integrity issues, once it has been detected.\r\nAs part of our work on this bug, we've added additional checks to detect issues like this even earlier in the future.\r\n\r\n3. Priorization of issues found through the previously mentioned methods.\r\nWe got a hint that something unexpected was happening in our cluster implementation in\r\nOctober 2015, when one of our own fuzzing tests failed a runtime guarantee check (see [issue #4979](https://github.com/rethinkdb/rethinkdb/issues/4979)). We investigated the issue, and found a bug that seemed to be the cause of it. We fixed the bug a few days later. However we observed the guarantee check failing again in November, indicating that our original fix didn't provide a full resolution. Unfortunately for our debugging efforts, the guarantee failure was extremely rare. We spent another few weeks validating the involved code paths, but couldn't find a conclusive explanation for the issue at the time. It now appears like the guarantee failures were a consequence of the same bug we describe here.\r\nThe lesson learned is to give even more attention to even the slightest indication of unexpected behavior in the part of the clustering logic that deals with consistency."
  , issueCommentId = 175394540
  }