IssueComment
  { issueCommentUpdatedAt = 2015 (-09) (-18) 17 : 14 : 27 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 151924
        , simpleUserLogin = N "sontek"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/151924?v=3"
        , simpleUserUrl = "https://api.github.com/users/sontek"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/141510548"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/4569#issuecomment-141510548"
  , issueCommentCreatedAt = 2015 (-09) (-18) 17 : 14 : 15 UTC
  , issueCommentBody =
      "After talking with @danielmewes a bit yesterday I tried to do a few benchmarks:\r\n\r\n1. Difference between single node vs multi-node cluster\r\n2. No full table scans, use a secondary index on all queries.\r\n3. Between vs Filter\r\n4. Threads vs Processes\r\n\r\nThis is my results:\r\n\r\n![screenshot from 2015-09-18 10 10 16](https://cloud.githubusercontent.com/assets/151924/9965789/90e835d8-5ded-11e5-8a06-19b465baeeed.png)\r\n\r\nYou can see it pretty much always slows down from 1 to 4 nodes, here is the actual data:\r\n\r\n```\r\n# Full table scan (SELECT * FROM ..)\r\nint query took 15740ms\r\nint query took 14524ms\r\nint query took 14466ms\r\nint query took 14285ms\r\n\r\n# Filter single query single node\r\nint query took 19005ms\r\nint query took 18482ms\r\nint query took 18220ms\r\nint query took 24998ms\r\n\r\n\r\n# Between single query single node\r\nint query took 15970ms\r\nint query took 15818ms\r\nint query took 15878ms\r\nint query took 15700ms\r\n\r\n\r\n# Multi-threaded single query\r\nint query took 11077ms\r\nint query took 11609ms\r\nint query took 10852ms\r\nint query took 11621ms\r\n\r\n\r\n# Multi-processes single query\r\nint query took 15251ms\r\nint query took 15907ms\r\nint query took 14677ms\r\nint query took 14374ms\r\n\r\n\r\n\r\n\r\n# Full table scan (SELECT * FROM ..) 4 nodes\r\nint query took 18882ms\r\nint query took 16515ms\r\nint query took 17638ms\r\nint query took 16100ms\r\n\r\n\r\n# Filter single query 4 nodes\r\nint query took 20862ms\r\nint query took 20141ms\r\nint query took 20481ms\r\nint query took 19868ms\r\n\r\n\r\n# Between single query 4 nodes\r\nint query took 23500ms\r\nint query took 24951ms\r\nint query took 24233ms\r\nint query took 24213ms\r\n\r\n\r\n# Multi-threaded 4 nodes\r\nint query took 12882ms\r\nint query took 12124ms\r\nint query took 12317ms\r\nint query took 11718ms\r\n\r\n# Multi-processes 4 nodes\r\nint query took 15131ms\r\nint query took 14618ms\r\nint query took 15284ms\r\nint query took 15193ms\r\n```\r\n\r\nThis is the code:\r\n\r\n```python\r\nimport rethinkdb as r\r\nimport random\r\nimport rapidjson\r\nimport pytz\r\nimport time\r\nimport yappi\r\nfrom datetime import datetime\r\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\r\nUTC = pytz.utc\r\n\r\nYAPPI = False\r\nDO_CREATE = False\r\nDO_INSERTS = False\r\nSERVER = \"mt1-rethinkd1c1\"\r\nTABLE = \"test2\"\r\n\r\nclass RapidJsonDecoder(object):\r\n    def __init__(self, reql_format_opts):\r\n        pass\r\n\r\n    def decode(self, s):\r\n        return rapidjson.loads(s)\r\n\r\ndef create_decoder(format_opts):\r\n    return RapidJsonDecoder(format_opts)\r\n\r\nconn = r.connect(SERVER)\r\nconn._get_json_decoder = create_decoder\r\n\r\n\r\nif DO_CREATE:\r\n    r.table_create(TABLE).run(conn)\r\n\r\nSTART_BIGINT = 100000000000000000\r\nEND_BIGINT = 999999999999999999\r\n\r\n\r\ndef utc_now():\r\n    now = datetime.utcnow()\r\n    tz_now = now.replace(tzinfo=UTC)\r\n    return tz_now.timestamp()\r\n\r\n\r\ndef get_rint(start=1000000, end=9999999):\r\n    \"\"\"\r\n    Generate a very large integer\r\n    :return:\r\n    \"\"\"\r\n    return random.randint(start, end)\r\n\r\n\r\ndef get_bigint():\r\n    \"\"\"\r\n    Generate a random BIGINT\r\n    :return:\r\n    \"\"\"\r\n    return get_rint(start=START_BIGINT, end=END_BIGINT)\r\n\r\n\r\nif DO_INSERTS:\r\n    objects_to_insert = []\r\n\r\n    for i in range(0, 1000000):\r\n        objects_to_insert.append({\r\n            'survey_id': get_rint(start=1, end=4),\r\n            'respondent_id': get_bigint(),\r\n            'row_id': get_bigint(),\r\n            'column_id': get_bigint(),\r\n            'value_id': get_bigint(),\r\n            'rid1': get_bigint(),\r\n            'rid2': get_bigint(),\r\n            'rid3': get_rint(),\r\n            'now': utc_now()\r\n        })\r\n\r\n        if i % 4000 == 0:\r\n            r.table(TABLE).insert(objects_to_insert).run(conn)\r\n            objects_to_insert = []\r\n\r\n    r.table(TABLE).insert(objects_to_insert).run(conn, noreply=True, durability=\"soft\")\r\n\r\nstart = time.time()\r\n\r\nif YAPPI:\r\n    yappi.set_clock_type('cpu')\r\n    yappi.start(builtins=True)\r\n\r\n\r\ndef query(survey_id):\r\n    data = []\r\n    tconn = r.connect(\"mt1-rethinkd1c1\")\r\n    tconn._get_json_decoder = create_decoder\r\n    results = r.table(TABLE).filter({'survey_id': survey_id}).run(tconn)\r\n\r\n    for result in results:\r\n        data.append(result)\r\n\r\n    return data\r\n\r\ndef get_multi_proc(use_threads=True):\r\n    futures = []\r\n\r\n    if use_threads:\r\n        klass = ThreadPoolExecutor\r\n    else:\r\n        klass = ProcessPoolExecutor\r\n\r\n    with klass(max_workers=4) as executor:\r\n        for i in range(1, 5):\r\n            future = executor.submit(query, i)\r\n            futures.append(future)\r\n\r\n        data = []\r\n\r\n    for future in futures:\r\n        data += future.result()\r\n\r\n    return data\r\n\r\ndef get_single():\r\n    # select all\r\n    data = []\r\n#    result = r.table(TABLE).run(conn)\r\n    result = r.table(TABLE).between(1, 5, index='survey_id').run(conn)\r\n#    result = r.table(TABLE).filter(\r\n#        (r.row['survey_id'] >= 1) & (r.row['survey_id'] <= 4)\r\n#    ).run(conn)\r\n\r\n    for row in result:\r\n        data.append(row)\r\n\r\n    return data\r\n\r\n\r\n#data = get_single()\r\n#data = get_multi_proc()\r\ndata = get_multi_proc(use_threads=False)\r\n\r\ncount = len(data)\r\nend = time.time()\r\n\r\nif YAPPI:\r\n    stats = yappi.get_func_stats()\r\n    stats.save('callgrind.out', type='callgrind')\r\n    print('checkout callgrind.out')\r\n\r\nprint(\"count is %s\" % count)\r\nduration = int(1000 * (end - start))\r\nprint(\"int query took %sms\" % duration)\r\n```\r\n\r\nI have a secondary index on `survey_id`.\r\n\r\n\r\nHere is a google drive excel document with the numbers in it:\r\n\r\nhttps://docs.google.com/spreadsheets/d/1kfyt9DjYrZwXowCVx4eSA7kLUrgK6oYvgbmtoOnXsTE/edit?usp=sharing\r\n\r\n"
  , issueCommentId = 141510548
  }