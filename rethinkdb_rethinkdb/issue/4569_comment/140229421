IssueComment
  { issueCommentUpdatedAt = 2015 (-09) (-14) 23 : 05 : 02 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 151924
        , simpleUserLogin = N "sontek"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/151924?v=3"
        , simpleUserUrl = "https://api.github.com/users/sontek"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/140229421"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/4569#issuecomment-140229421"
  , issueCommentCreatedAt = 2015 (-09) (-14) 23 : 05 : 02 UTC
  , issueCommentBody =
      "@danielmewes @coffeemug Do you have code example of how you were able to multiprocess and get down to 550ms per 100k?\r\n\r\nI setup a 4 node cluster and distributed 1 million rows evenly across them and I'm not seeing that performance, here is the code I wrote to test:\r\n\r\n\r\n```python\r\nimport rethinkdb as r\r\nimport random\r\nimport rapidjson\r\nimport pytz\r\nimport time\r\nimport yappi\r\nfrom datetime import datetime\r\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\r\nUTC = pytz.utc\r\n\r\nYAPPI = False\r\nDO_CREATE = False\r\nDO_INSERTS = False\r\n\r\nclass RapidJsonDecoder(object):\r\n    def __init__(self, reql_format_opts):\r\n        pass\r\n\r\n    def decode(self, s):\r\n        return rapidjson.loads(s)\r\n\r\ndef create_decoder(format_opts):\r\n    return RapidJsonDecoder(format_opts)\r\n\r\nconn = r.connect(\"mt1-rethinkd1c1\")\r\nconn._get_json_decoder = create_decoder\r\n\r\n\r\nif DO_CREATE:\r\n    r.table_create('test').run(conn)\r\n\r\nSTART_BIGINT = 100000000000000000\r\nEND_BIGINT = 999999999999999999\r\n\r\n\r\ndef utc_now():\r\n    now = datetime.utcnow()\r\n    tz_now = now.replace(tzinfo=UTC)\r\n    return tz_now\r\n\r\n\r\ndef get_rint(start=1000000, end=9999999):\r\n    \"\"\"\r\n    Generate a very large integer\r\n    :return:\r\n    \"\"\"\r\n    return random.randint(start, end)\r\n\r\n\r\ndef get_bigint():\r\n    \"\"\"\r\n    Generate a random BIGINT\r\n    :return:\r\n    \"\"\"\r\n    return get_rint(start=START_BIGINT, end=END_BIGINT)\r\n\r\n\r\nif DO_INSERTS:\r\n    objects_to_insert = []\r\n\r\n    for i in range(0, 1000000):\r\n        objects_to_insert.append({\r\n            'respondent_id': get_bigint(),\r\n            'row_id': get_bigint(),\r\n            'column_id': get_bigint(),\r\n            'value_id': get_bigint(),\r\n            'rid1': get_bigint(),\r\n            'rid2': get_bigint(),\r\n            'rid3': get_rint(),\r\n            'now': utc_now()\r\n        })\r\n\r\n        if i % 4000 == 0:\r\n            r.table('test').insert(objects_to_insert).run(conn)\r\n            objects_to_insert = []\r\n\r\n    r.table('test').insert(objects_to_insert).run(conn)\r\n\r\nstart = time.time()\r\n\r\nif YAPPI:\r\n    yappi.set_clock_type('cpu')\r\n    yappi.start(builtins=True)\r\n\r\n\r\ndef query(start, end):\r\n    data = []\r\n    tconn = r.connect(\"mt1-rethinkd1c1\")\r\n    tconn._get_json_decoder = create_decoder\r\n    print('starting...', start, end)\r\n    results = r.table('test').between(start, end, index='rid3').run(tconn)\r\n\r\n    for result in results:\r\n        data.append(result)\r\n    print('finished...', start, end)\r\n    return data\r\n\r\ndef get_multi_proc():\r\n    start_count = 1000000\r\n    end_count = 2000000\r\n    max_numbers = 9999999\r\n    incrementer = 1000000\r\n    futures = []\r\n\r\n    with ProcessPoolExecutor(max_workers=4) as executor:\r\n        while end_count < max_numbers:\r\n            future = executor.submit(query, start_count, end_count)\r\n            futures.append(future)\r\n            start_count = end_count\r\n            end_count = start_count + incrementer\r\n\r\n        future = executor.submit(query, start_count, end_count)\r\n        futures.append(future)\r\n\r\n\r\n        data = []\r\n\r\n    for future in futures:\r\n        data += future.result()\r\n\r\n    return data\r\n\r\ndef get_single():\r\n    # select all\r\n    data = []\r\n    result = r.table('test').run(conn)\r\n    for row in result:\r\n        data.append(row)\r\n\r\n    return data\r\n\r\n#data = get_single()\r\ndata = get_multi_proc()\r\ncount = len(data)\r\nend = time.time()\r\n\r\nif YAPPI:\r\n    stats = yappi.get_func_stats()\r\n    stats.save('callgrind.out', type='callgrind')\r\n    print('checkout callgrind.out')\r\n\r\nprint(\"count is %s\" % count)\r\nduration = int(1000 * (end - start))\r\nprint(\"int query took %sms\" % duration)\r\n```\r\n\r\nI also created a secondary index on `rid3` to be used for the between calls"
  , issueCommentId = 140229421
  }