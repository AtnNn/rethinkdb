IssueComment
  { issueCommentUpdatedAt = 2014 (-11) (-26) 22 : 54 : 23 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/64722573"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/3348#issuecomment-64722573"
  , issueCommentCreatedAt = 2014 (-11) (-26) 22 : 54 : 23 UTC
  , issueCommentBody =
      "I can get about 3x the disk write throughput by reducing the number of hash shards from 8 to 1.\r\n\r\nI wonder if the fact that each cache uses its own i/o account and that we switch between them when \r\nperforming disk writes might lead to the high number of random seeks... This is just an idea, I haven't verified anything so far.\r\nWe could probably use the same i/o account for all caches of a given table when writing."
  , issueCommentId = 64722573
  }