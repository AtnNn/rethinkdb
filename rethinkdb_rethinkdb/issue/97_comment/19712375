IssueComment
  { issueCommentUpdatedAt = 2013 (-06) (-19) 20 : 26 : 25 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/19712375"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/97#issuecomment-19712375"
  , issueCommentCreatedAt = 2013 (-06) (-19) 20 : 26 : 25 UTC
  , issueCommentBody =
      "Bump\r\n\r\nI believe that we really really should make the quota reconfigurable for existing tables. This should be very very easy to implement (see https://github.com/rethinkdb/rethinkdb/issues/97?source=cc#issuecomment-17124174). The \"real\" solution which @coffeemug seems to be waiting for to be implemented at some point in the potentially distant future is absolutely non-trivial. Despite that, people might actually prefer explicit control over per-table cache sizes over the magic proposed in the long-term solution.\r\n\r\nThis issue will have a huge impact on anyone who tries to use RethinkDB for a production system.\r\nThe problem being that once you reach a certain table size, your queries will become incredibly slow even on systems with much memory. The only way to make them fast again is to re-create the tables, resulting in downtime and frustration.\r\n\r\nOn the other hand, you might have a system running absolutely fine for months. Then you add another table and suddenly RethinkDB becomes unusable because it runs out of memory. Again, no chance to do anything about it except for re-creating *all* of the tables."
  , issueCommentId = 19712375
  }