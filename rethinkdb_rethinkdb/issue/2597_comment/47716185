IssueComment
  { issueCommentUpdatedAt = 2014 (-07) (-01) 22 : 17 : 08 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/47716185"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/2597#issuecomment-47716185"
  , issueCommentCreatedAt = 2014 (-07) (-01) 22 : 17 : 08 UTC
  , issueCommentBody =
      "One problem is that our driver (or some part of node.js) becomes slow when handling very complex objects like these. It works alright with 10 documents of 20,000 embedded fields each, but if I try to insert 50 at a time, the client takes a really long time just to encode the query. If I try to insert 100 at a time it begins to crash with allocation failures or runs into connection problems. Maybe because of the `JSON.stringify` call that @nickpoorman mentions?\r\n\r\nI don't know enough about node.js to debug or profile this. @neumino we should look into this together some time this week.\r\n\r\n@nickpoorman Is there a chance you could run multiple clients concurrently to insert the data? I expect that that could improve throughput quite a bit."
  , issueCommentId = 47716185
  }