Issue
  { issueClosedAt = Nothing
  , issueUpdatedAt = 2016 (-08) (-01) 17 : 15 : 31 UTC
  , issueEventsUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/5695/events"
  , issueHtmlUrl =
      Just "https://github.com/rethinkdb/rethinkdb/issues/5695"
  , issueClosedBy = Nothing
  , issueLabels = []
  , issueNumber = 5695
  , issueAssignee = Nothing
  , issueUser =
      SimpleUser
        { simpleUserId = Id 1430058
        , simpleUserLogin = N "ntquyen"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/1430058?v=3"
        , simpleUserUrl = "https://api.github.com/users/ntquyen"
        , simpleUserType = OwnerUser
        }
  , issueTitle =
      "Query is so slow in large database with big records (up to 7mb record)"
  , issuePullRequest = Nothing
  , issueUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/5695"
  , issueCreatedAt = 2016 (-04) (-20) 04 : 36 : 52 UTC
  , issueBody =
      Just
        "I'm using rethinkdb cluster with 3 instances inside docker containers and my database has some big tables: 27GB for each replicas (~85k records) and having up to 7MB for one record. The cluster is allocated 30GB of RAM and ~14 GB cache-size in total for the cluster. Each table has 3 shards and 3 replicas.\n\nThe problem is before the cluster was working quite happily with haft the RAM today, but after we added one big client as above info, the queries became so slow even for queries by indexes, causing timeout all the time along with heartbeat timeout. The cache-size filled up so quickly, from 8% to 74% for just one query. `.group({index: 'boolean_field'})`. Doubling the RAM and cache-size doesn't solve it. And during the query, disk read was very high and I suspect that it was reading from disk, not from RAM.\n\nRethinkdb version: 2.2.5\nDocker version : 1.9.1\nOS version: CoreOS stable 899.15.0\n\nI also have some questions on how rethinkdb uses cache. \n- Is rethinkdb using LRU cache strategy?\n- How much portion of memory is reserved for indexes?\n- Does the size of the whole record (may be up to 7GM) affect the size of index, given that the index fields are very small (some boolean fields)? In general, how to calculate the size of an index of a table?\n- Will all indexes be loaded in memory when the cluster start? What happen if the memory is not enough to fit all the cache?\n\nThanks for helping in investigating this issue. This is quite urgent for us because our rethinkdb is running in production.\n\nMore info in how our cluster is installed:\n\n**Instance 1:**\n- CoreOS bare metal, stable version 835.13.0, docker 1.8.3\n- Memory allocated: 12Gb\n- cache-size 5.5Gb\n\n**Instance 2:**\n- CoreOS VM under hyper-V, stable version 899.15.0, docker 1.9.1\n- Memory allocated: 10Gb\n- cache-size 4.5Gb\n\n**Instance 3:**\n- CoreOS VM under hyper-V, stable version 899.15.0, docker 1.9.1\n- Memory allocated: 9Gb\n- cache-size 4Gb\n\nEvery tables have 3 shards, 3 replicas, taking 174Gb disk used.\n"
  , issueState = "open"
  , issueId = Id 149656332
  , issueComments = 5
  , issueMilestone = Nothing
  }