IssueComment
  { issueCommentUpdatedAt = 2015 (-12) (-29) 22 : 17 : 03 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 1366
        , simpleUserLogin = N "deontologician"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/1366?v=3"
        , simpleUserUrl = "https://api.github.com/users/deontologician"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/167889283"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/5240#issuecomment-167889283"
  , issueCommentCreatedAt = 2015 (-12) (-29) 22 : 17 : 03 UTC
  , issueCommentBody =
      "For 1. ingesting the log file in the `rethinkdb_data` directory, I would say something like filebeat would be a good choice (or the file output for logstash or something). You could use the logstash plugin to watch the `rethinkdb.logs` table, but it doesn't guarantee it will have the entire contents of the logfile, so it isn't a good idea if you need to index the entire log. (Also, that table is just a facade for reading the log file anyway, so it's just overhead that's probably not needed).\r\n\r\nFor 2, syncing the documents themselves, the logstash input is the best way. It better allows you to manipulate the documents (you could strip out all fields but text fields and the id for example), and it allows you to decouple your input from your output, which would be helpful if you end up wanting to shoot your documents somewhere else, like rabbitmq or something."
  , issueCommentId = 167889283
  }