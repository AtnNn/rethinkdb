IssueComment
  { issueCommentUpdatedAt = 2013 (-08) (-30) 18 : 24 : 20 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 48436
        , simpleUserLogin = N "coffeemug"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/48436?v=3"
        , simpleUserUrl = "https://api.github.com/users/coffeemug"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/23580460"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1383#issuecomment-23580460"
  , issueCommentCreatedAt = 2013 (-08) (-30) 18 : 24 : 20 UTC
  , issueCommentBody =
      "@pixelspark -- I also think that treating the output of this command as a table would be weird (not to mention nearly impossible to do). However, it would be a RethinkDB array (assuming the server returns an array), so the commands that operate on sequences (e.g. map/filter/etc.) would work beautifully.\r\n\r\n@flipchart -- I strongly disagree that this feature would set us on a path of \"all things to all people\". When @mglukhovsky originally proposed this feature, I thought it's brilliant for an exact opposite reason. Here's what I mean.\r\n\r\nWe get a lot of pressure from people to implement all sorts of features -- geoindexing, full text search, publish/subscribe queues, a distributed file system/media store (like MongoDB's gridfs), etc. We don't want to do any of these because there are already existing products that handle these use cases phenomenally well. Instead, we want to add a couple of small features to RethinkDB that would offer a really nice experience integrating with other products. Triggers (#997) would allow getting data out of RethinkDB and into other systems on event-by-event basis. The `wget` feature is the missing piece on the other side -- getting data out of other systems and into RethinkDB. With this pair of features, we'd be left with a really small surface area -- we'll be able to focus on doing what we do exceptionally well, and let other products handle use cases we're not so good at.\r\n\r\nInformally, I think of RethinkDB as a sort of Mathematica for unstructured data. Any time I have some JSON data and I need to do some stuff on it, I fire up RethinkDB, get the data into it, open the data explorer, and type away until I get what I need. After I'm done, I just copy the queries into my program, and can then deploy it/scale it out. As of now the only complication I run into is that getting data into Rethink is a bit of a pain.\r\n\r\nThe `wget` feature has the following benefits:\r\n\r\n* It makes getting data into Rethink ridiculously, amazingly, spectacularly easy. It's relatively easy to do, but makes the \"Mathematica\"/interactive use case 100 times more pleasant.\r\n* It would make getting started with Rethink A LOT more pleasant, which I think would make a huge impact in bringing in more early adopters.\r\n* It would make initial tutorials on rethinkdb.com much easier and more relevant. Instead of inserting somewhat artificial data, we could tell people \"here's a one liner to import data from any API you want\". For example, we could correlate RethinkDB followers on GitHub and on Twitter with a ridiculously, amazingly short query and no pain.\r\n* On real production systems, it would make importing data a bit easier (though not by much). It has the caveat that the outbound port has to be open, so I don't know how useful it would be in this scenario."
  , issueCommentId = 23580460
  }