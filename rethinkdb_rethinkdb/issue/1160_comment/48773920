IssueComment
  { issueCommentUpdatedAt = 2014 (-07) (-11) 19 : 42 : 12 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 258437
        , simpleUserLogin = N "srh"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/258437?v=3"
        , simpleUserUrl = "https://api.github.com/users/srh"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/48773920"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1160#issuecomment-48773920"
  , issueCommentCreatedAt = 2014 (-07) (-11) 19 : 39 : 38 UTC
  , issueCommentBody =
      "@hardkrash: Consider the possibility of having a data set with points at times ..., t_i, t_{i+1}, t_{i+2}, ... where t_{i+1} - t_i != t_{i+2} - t_{i+1}.\r\n\r\nDo you find that this is common (in *your* experience, I guess)?  Based on my vague memory of another post you've made, I guess maybe not, (edit: never mind, now I read another post, and maybe so) but:\r\n\r\nIf so, and you wanted interpolation to fill in data, how would you want it to happen:  exactly X samples between each original data point?  Enough points between each original data point such that the difference between successive times is bounded above?  That, with the stipulation that the query errors if the distance between two successive time points is above some stipulated value (as a safety valve if one of the datums has a garbage time value)?  (Also, would you want some kind of downsample operation that takes a sample every *u* seconds, say, the first sample after t_0 + k*u for k = 0,1,2,...?)"
  , issueCommentId = 48773920
  }