IssueComment
  { issueCommentUpdatedAt = 2015 (-09) (-23) 00 : 22 : 41 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/142461097"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/4785#issuecomment-142461097"
  , issueCommentCreatedAt = 2015 (-09) (-23) 00 : 22 : 17 UTC
  , issueCommentBody =
      "@marshall007 That sounds like a great idea. I think the web UI plugin would essentially have that functionality. In fact all the specific queries it's running are already encoded in the browser-side JS code, not in the backend.\r\nWe could probably make it a library of some sort, so that you can very easily create plugins where you only need to provide the static HTML + browser JS code, without writing any additional code for the backend.\r\n\r\n@mlucy \r\n\r\n> I'm not 100% sure plugin state should live in a table. You can get into weird situations that way. Say, for example, you add a plugin while your cluster is one node, then later you grow it to a 16 node cluster. Unless we automatically reconfigure the plugin tables when we add nodes, which we probably shouldn't because of availability loss, the plugin is now dependent on one of your 16 servers being up, and if that server goes down then the plugin stops working even if the rest of the cluster is healthy. \r\n\r\nI thought about that too. I couldn't really come up with a great solution. I think for the moment it would be ok if we left reconfiguring the plugin tables to the administrator. Note that adding and removing replicas doesn't impair availability though (with the exception of currently running queries and changefeeds being interrupted). Additionally, many plugins could probably use outdated reads to make sure that the plugin remains available as long as at least one replica is still around.\r\n\r\nOverall I think storing all configuration and plugin state into a table makes for a nicer user experience.\r\n1. You can represent both per-server as well as global state conveniently. If a certain configuration option is per server, the plugin can use one document (or field or whatever) per server.\r\n2. It allows modifying and querying the plugin configuration through ReQL. I think this is a very powerful argument in favor of this solution, because it fits well with our system table approach to cluster management.\r\n3. The configuration can be easily replicated, using the existing replica machinery.\r\n\r\nI agree that it probably depends a bit on the use case. The main scenario I had in mind was the GraphQL library.\r\n\r\nConfiguration options would include:\r\n- the port to bind to, and maybe the interfaces to bind to (we could just take those from RethinkDB's server config though). The interfaces would need to be a per-server option, but the port in the simplest case could be cluster-wide.\r\n- the GraphQL schema. This should clearly be a cluster-wide setting\r\n- authentication and permission data. This also clearly needs to be a cluster-wide setting\r\n\r\nEspecially for the latter two, being able to modify those settings through normal ReQL queries would be extremely useful I think. And we don't need to worry about providing a special API for configuration changes.\r\n\r\nMy impression is that relying on tables as the only mean for storing state and configuration will be the best way to get us started. Once we have more experience about what plugins need, we can add additional storage options or add better abstractions around the tables. We can also think about how to automatically replica the state tables to additional servers then.\r\n\r\n> This seems unnecessarily complicated to me. I think that if we just pass the plugins the port to connect on and the auth key that would be easier (we wouldn't have to add anything new to the server besides the ability to launch the plugin). That would also making porting to windows easier (unless they have an equivalent of Unix domain sockets, in which case ignore this point).\r\n\r\nFair enough. I think that might be ok, though it feels less clean to me (especially the fact that we need to expose the auth key to plugins). We could make this an optional feature, depending on how much time we'll end up having left.\r\nI'm not sure about what Windows offers in this respect ( @AtnNn ?).\r\n\r\n\r\n> Some of the intended plugins (document expiration, triggers) seem like they would run on only one machine rather than on every machine in the cluster. Should we just leave it up to users to pick which machines plugins are launched on by editing their command-line options, or are we going to be starting and stopping plugins in a centralized way?\r\n\r\nI think for the first version, it's enough if you have to explicitly enable a plugin on a given server by providing a command line option. Long-term we should probably make that more convenient and handle the different cases you mentioned."
  , issueCommentId = 142461097
  }