IssueComment
  { issueCommentUpdatedAt = 2015 (-01) (-15) 23 : 47 : 13 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 265071
        , simpleUserLogin = N "kofalt"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/265071?v=3"
        , simpleUserUrl = "https://api.github.com/users/kofalt"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/70184961"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/3471#issuecomment-70184961"
  , issueCommentCreatedAt = 2015 (-01) (-15) 23 : 47 : 13 UTC
  , issueCommentBody =
      "This portion of @timmaxw's proposal makes me nervous:\r\n\r\n> then streams the entire table\r\n\r\n1) Wouldn't this make changefeed resumption basically ususable for large data sets? \r\n\r\nThe example @danielmewes just pointed out - a web app that broadcasts events - would like to pick up where it left off after a crash, but would be incapable of eating an entire table's worth of data, deduping it with what the client already has, sending 4 million websocket events, etc.\r\n\r\n2) What would the 'full table dump' events look like?\r\n\r\nIf `{ 'old_val': null, 'new_val': ..., 'token': OPAQUE}`, then it would appear identical to an insertion event, which is arguably reasonable but not idempotent. A system expecting the current change feed behaviour would react incorrectly - say, broadcasting 4 million websocket events to clients about how many new comments there were.\r\n\r\n"
  , issueCommentId = 70184961
  }