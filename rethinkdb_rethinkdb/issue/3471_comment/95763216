IssueComment
  { issueCommentUpdatedAt = 2015 (-04) (-24) 00 : 37 : 53 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 1777134
        , simpleUserLogin = N "mlucy"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/1777134?v=3"
        , simpleUserUrl = "https://api.github.com/users/mlucy"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/95763216"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/3471#issuecomment-95763216"
  , issueCommentCreatedAt = 2015 (-04) (-24) 00 : 37 : 53 UTC
  , issueCommentBody =
      "Building up a bunch of small queues on the shards is an interesting option, but deduplication would be a problem (we couldn't just have one queue per feed on the shards and request documents from them as needed to serve persistent feeds because intracluster network traffic would go up by a factor of `n` compared to the current implementation where `n` is the number of named feeds).  Although, thinking about it, I'm not sure how you'd avoid that in the case where the feed is centralized but on the wrong machine, either.  Maybe we should just give up and accept that we're going to have one intracluster message for every change in every named changefeed?  @danielmewes, do you have a sense for how bad that would be in a real network setup?  (In the future we can also implement some complicated solution where once you've caught up you go back to getting messages through the normal deduplicated channel, although I would guess that will basically never happen, and it would lead to higher memory usage anyway because you'd have a duplicate queue.)"
  , issueCommentId = 95763216
  }