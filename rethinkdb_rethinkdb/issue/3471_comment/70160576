IssueComment
  { issueCommentUpdatedAt = 2015 (-01) (-15) 20 : 59 : 43 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/70160576"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/3471#issuecomment-70160576"
  , issueCommentCreatedAt = 2015 (-01) (-15) 20 : 59 : 43 UTC
  , issueCommentBody =
      "I like @timmaxw 's proposal.\r\n\r\nThat way we could ship a first version using a ring buffer on the primary replica that:\r\n* allows resuming if the client (connection) dies\r\n* starts over feeding the table state from scratch if more than e.g. 100K changes accumulate\r\n* starts over feeding the table state from scratch if the primary dies\r\n\r\nWe could think about maintaining resumable changefeeds over table reconfiguration, though on first thought that seems like work we can better spend on the better implementation.\r\n\r\nLater we can follow up with an implementation that uses store timestamps, in order to\r\n* allow resuming even after missing more than 100K changes\r\n* allow resuming after primary changes\r\n\r\nI'm uncertain as to whether it's worth spending time on the first implementation rather than going for the second one right away. This will depend on how much work we expect the respective implementations to be."
  , issueCommentId = 70160576
  }