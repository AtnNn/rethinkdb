IssueComment
  { issueCommentUpdatedAt = 2013 (-05) (-14) 21 : 15 : 20 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 258437
        , simpleUserLogin = N "srh"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/258437?v=3"
        , simpleUserUrl = "https://api.github.com/users/srh"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/17905815"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/803#issuecomment-17905815"
  , issueCommentCreatedAt = 2013 (-05) (-14) 21 : 15 : 20 UTC
  , issueCommentBody =
      "Here's an example of how complicated code is with protocol_t templatization:\r\n\r\n```c++\r\ntemplate <class protocol_t>\r\nlistener_t<protocol_t>::listener_t(const base_path_t &base_path,\r\n                                   io_backender_t *io_backender,\r\n                                   mailbox_manager_t *mm,\r\n                                   clone_ptr_t<watchable_t<boost::optional<boost::optional<broadcaster_business_card_t<protocol_t> > > > > broadcaster_metadata,\r\n                                   branch_history_manager_t<protocol_t> *branch_history_manager,\r\n                                   store_view_t<protocol_t> *svs,\r\n                                   clone_ptr_t<watchable_t<boost::optional<boost::optional<replier_business_card_t<protocol_t> > > > > replier,\r\n                                   backfill_session_id_t backfill_session_id,\r\n                                   perfmon_collection_t *backfill_stats_parent,\r\n                                   signal_t *interruptor,\r\n                                   order_source_t *order_source)\r\n        THROWS_ONLY(interrupted_exc_t, backfiller_lost_exc_t, broadcaster_lost_exc_t) :\r\n\r\n    mailbox_manager_(mm),\r\n    svs_(svs),\r\n    uuid_(generate_uuid()),\r\n    perfmon_collection_(),\r\n    perfmon_collection_membership_(backfill_stats_parent, &perfmon_collection_, \"backfill-serialization-\" + uuid_to_str(uuid_)),\r\n    /* TODO: Put the file in the data directory, not here */\r\n    write_queue_(io_backender,\r\n                 serializer_filepath_t(base_path, \"backfill-serialization-\" + uuid_to_str(uuid_)),\r\n                 &perfmon_collection_),\r\n    write_queue_semaphore_(SEMAPHORE_NO_LIMIT,\r\n        WRITE_QUEUE_SEMAPHORE_TRICKLE_FRACTION),\r\n    enforce_max_outstanding_writes_from_broadcaster_(MAX_OUTSTANDING_WRITES_FROM_BROADCASTER),\r\n    write_mailbox_(mailbox_manager_,\r\n        boost::bind(&listener_t::on_write, this, _1, _2, _3, _4, _5),\r\n        mailbox_callback_mode_inline),\r\n    writeread_mailbox_(mailbox_manager_,\r\n        boost::bind(&listener_t::on_writeread, this, _1, _2, _3, _4, _5, _6),\r\n        mailbox_callback_mode_inline),\r\n    read_mailbox_(mailbox_manager_,\r\n        boost::bind(&listener_t::on_read, this, _1, _2, _3, _4, _5),\r\n        mailbox_callback_mode_inline)\r\n{\r\n    boost::optional<boost::optional<broadcaster_business_card_t<protocol_t> > > business_card =\r\n        broadcaster_metadata->get();\r\n    if (!business_card || !business_card.get()) {\r\n        throw broadcaster_lost_exc_t();\r\n    }\r\n\r\n    branch_id_ = business_card.get().get().branch_id;\r\n\r\n    branch_birth_certificate_t<protocol_t> this_branch_history;\r\n\r\n    {\r\n        cross_thread_signal_t ct_interruptor(interruptor, branch_history_manager->home_thread());\r\n        on_thread_t th(branch_history_manager->home_thread());\r\n        branch_history_manager->import_branch_history(business_card.get().get().branch_id_associated_branch_history, interruptor);\r\n        this_branch_history = branch_history_manager->get_branch(branch_id_);\r\n    }\r\n\r\n    our_branch_region_ = this_branch_history.region;\r\n\r\n#ifndef NDEBUG\r\n    /* Sanity-check to make sure we're on the same timeline as the thing\r\n       we're trying to join. The backfiller will perform an equivalent check,\r\n       but if there's an error it would be nice to catch it where the action\r\n       was initiated. */\r\n\r\n    rassert(region_is_superset(our_branch_region_, svs_->get_region()));\r\n\r\n    object_buffer_t<fifo_enforcer_sink_t::exit_read_t> read_token;\r\n    svs_->new_read_token(&read_token);\r\n    region_map_t<protocol_t, binary_blob_t> start_point_blob;\r\n    svs_->do_get_metainfo(order_source->check_in(\"listener_t(A)\").with_read_mode(), &read_token, interruptor, &start_point_blob);\r\n    region_map_t<protocol_t, version_range_t> start_point = to_version_range_map(start_point_blob);\r\n\r\n    {\r\n        on_thread_t th(branch_history_manager->home_thread());\r\n        for (typename region_map_t<protocol_t, version_range_t>::const_iterator it = start_point.begin();\r\n             it != start_point.end();\r\n             ++it) {\r\n\r\n            version_t version = it->second.latest;\r\n            rassert(version.branch == branch_id_ ||\r\n                    version_is_ancestor(branch_history_manager,\r\n                                        version,\r\n                                        version_t(branch_id_, this_branch_history.initial_timestamp),\r\n                                        it->first));\r\n        }\r\n    }\r\n#endif\r\n\r\n    /* Attempt to register for reads and writes */\r\n    try_start_receiving_writes(broadcaster_metadata, interruptor);\r\n    listener_intro_t<protocol_t> listener_intro;\r\n    bool registration_is_done = registration_done_cond_.try_get_value(&listener_intro);\r\n    guarantee(registration_is_done);\r\n\r\n    state_timestamp_t streaming_begin_point = listener_intro.broadcaster_begin_timestamp;\r\n\r\n    try {\r\n        /* Go through a little song and dance to make sure that the\r\n         * backfiller will at least get us to the point that we will being\r\n         * live streaming from. */\r\n\r\n        cond_t backfiller_is_up_to_date;\r\n        mailbox_t<void()> ack_mbox(\r\n            mailbox_manager_,\r\n            boost::bind(&cond_t::pulse, &backfiller_is_up_to_date),\r\n            mailbox_callback_mode_inline);\r\n\r\n        resource_access_t<replier_business_card_t<protocol_t> > replier_access(replier);\r\n        send(mailbox_manager_, replier_access.access().synchronize_mailbox, streaming_begin_point, ack_mbox.get_address());\r\n\r\n        wait_any_t interruptor2(interruptor, replier_access.get_failed_signal());\r\n        wait_interruptible(&backfiller_is_up_to_date, &interruptor2);\r\n\r\n        /* Backfill */\r\n        backfillee<protocol_t>(mailbox_manager_,\r\n                               branch_history_manager,\r\n                               svs_,\r\n                               svs_->get_region(),\r\n                               replier->subview(&listener_t<protocol_t>::get_backfiller_from_replier_bcard),\r\n                               backfill_session_id,\r\n                               interruptor);\r\n    } catch (const resource_lost_exc_t &) {\r\n        throw backfiller_lost_exc_t();\r\n    }\r\n\r\n    object_buffer_t<fifo_enforcer_sink_t::exit_read_t> read_token2;\r\n    svs_->new_read_token(&read_token2);\r\n\r\n    region_map_t<protocol_t, binary_blob_t> backfill_end_point_blob;\r\n    svs_->do_get_metainfo(order_source->check_in(\"listener_t(B)\").with_read_mode(), &read_token2, interruptor, &backfill_end_point_blob);\r\n\r\n    region_map_t<protocol_t, version_range_t> backfill_end_point = to_version_range_map(backfill_end_point_blob);\r\n\r\n    /* Sanity checking. */\r\n\r\n    /* Make sure the region is not empty. */\r\n    guarantee(backfill_end_point.begin() != backfill_end_point.end());\r\n\r\n    /* The end timestamp is the maximum of the timestamps we've seen. If you've\r\n    been following closely (which you probably haven't because this is\r\n    confusing) you should be thinking \"How is that correct? If the region map\r\n    says that we're at timestamp 900 on region A and 901 on region B, then that\r\n    means we're missing the updates for the write with timestamp 900->901 on\r\n    region A. If the region map has different timestamps for different regions,\r\n    then it's not possible to boil down the current timestamp to a single\r\n    number, right?\" But this is wrong. Our backfill was, in fact, serialized\r\n    with respect to all the writes. In fact, the reason why region A has\r\n    timestamp 900 is that the write with timestamp 900->901 didn't touch any\r\n    keys in region A. We didn't update the timestamp for region A because\r\n    regions A and B were on separate threads, and we didn't want to send the\r\n    operation to region A unnecessarily if we weren't actually updating any keys\r\n    there. That's why it's OK to just take the maximum of all the timestamps\r\n    that we see.\r\n    TODO: If we change the way we shard such that each listener_t maps to a\r\n    single B-tree on the same machine, then replace this loop with a strict\r\n    assertion that requires everything to be at the same timestamp. */\r\n    state_timestamp_t backfill_end_timestamp = backfill_end_point.begin()->second.earliest.timestamp;\r\n    for (typename region_map_t<protocol_t, version_range_t>::const_iterator it = backfill_end_point.begin();\r\n         it != backfill_end_point.end();\r\n         ++it) {\r\n        guarantee(it->second.is_coherent());\r\n        guarantee(it->second.earliest.branch == branch_id_);\r\n        backfill_end_timestamp = std::max(backfill_end_timestamp, it->second.earliest.timestamp);\r\n    }\r\n\r\n    guarantee(backfill_end_timestamp >= streaming_begin_point);\r\n\r\n    current_timestamp_ = backfill_end_timestamp;\r\n    write_queue_coro_pool_callback_.init(new boost_function_callback_t<write_queue_entry_t>(\r\n            boost::bind(&listener_t<protocol_t>::perform_enqueued_write, this, _1, backfill_end_timestamp, _2)));\r\n    write_queue_coro_pool_.init(new coro_pool_t<write_queue_entry_t>(\r\n            WRITE_QUEUE_CORO_POOL_SIZE, &write_queue_, write_queue_coro_pool_callback_.get()));\r\n    write_queue_semaphore_.set_capacity(WRITE_QUEUE_SEMAPHORE_LONG_TERM_CAPACITY);\r\n\r\n    if (write_queue_.size() <= WRITE_QUEUE_SEMAPHORE_LONG_TERM_CAPACITY) {\r\n        write_queue_has_drained_.pulse_if_not_already_pulsed();\r\n    }\r\n\r\n    wait_interruptible(&write_queue_has_drained_, interruptor);\r\n}\r\n```\r\n\r\nHere's the \"simplified\" code that isn't templated:\r\n\r\n```c++\r\nlistener_t::listener_t(const base_path_t &base_path,\r\n                       io_backender_t *io_backender,\r\n                       mailbox_manager_t *mm,\r\n                       clone_ptr_t<watchable_t<boost::optional<boost::optional<broadcaster_business_card_t > > > > broadcaster_metadata,\r\n                       branch_history_manager_t *branch_history_manager,\r\n                       store_view_t *svs,\r\n                       clone_ptr_t<watchable_t<boost::optional<boost::optional<replier_business_card_t> > > > replier,\r\n                       backfill_session_id_t backfill_session_id,\r\n                       perfmon_collection_t *backfill_stats_parent,\r\n                       signal_t *interruptor,\r\n                       order_source_t *order_source)\r\n        THROWS_ONLY(interrupted_exc_t, backfiller_lost_exc_t, broadcaster_lost_exc_t) :\r\n\r\n    mailbox_manager_(mm),\r\n    svs_(svs),\r\n    uuid_(generate_uuid()),\r\n    perfmon_collection_(),\r\n    perfmon_collection_membership_(backfill_stats_parent, &perfmon_collection_, \"backfill-serialization-\" + uuid_to_str(uuid_)),\r\n    /* TODO: Put the file in the data directory, not here */\r\n    write_queue_(io_backender,\r\n                 serializer_filepath_t(base_path, \"backfill-serialization-\" + uuid_to_str(uuid_)),\r\n                 &perfmon_collection_),\r\n    write_queue_semaphore_(SEMAPHORE_NO_LIMIT,\r\n        WRITE_QUEUE_SEMAPHORE_TRICKLE_FRACTION),\r\n    enforce_max_outstanding_writes_from_broadcaster_(MAX_OUTSTANDING_WRITES_FROM_BROADCASTER),\r\n    write_mailbox_(mailbox_manager_,\r\n        boost::bind(&listener_t::on_write, this, _1, _2, _3, _4, _5),\r\n        mailbox_callback_mode_inline),\r\n    writeread_mailbox_(mailbox_manager_,\r\n        boost::bind(&listener_t::on_writeread, this, _1, _2, _3, _4, _5, _6),\r\n        mailbox_callback_mode_inline),\r\n    read_mailbox_(mailbox_manager_,\r\n        boost::bind(&listener_t::on_read, this, _1, _2, _3, _4, _5),\r\n        mailbox_callback_mode_inline)\r\n{\r\n    boost::optional<boost::optional<broadcaster_business_card_t> > business_card =\r\n        broadcaster_metadata->get();\r\n    if (!business_card || !business_card.get()) {\r\n        throw broadcaster_lost_exc_t();\r\n    }\r\n\r\n    branch_id_ = business_card.get().get().branch_id;\r\n\r\n    branch_birth_certificate_t this_branch_history;\r\n\r\n    {\r\n        cross_thread_signal_t ct_interruptor(interruptor, branch_history_manager->home_thread());\r\n        on_thread_t th(branch_history_manager->home_thread());\r\n        branch_history_manager->import_branch_history(business_card.get().get().branch_id_associated_branch_history, interruptor);\r\n        this_branch_history = branch_history_manager->get_branch(branch_id_);\r\n    }\r\n\r\n    our_branch_region_ = this_branch_history.region;\r\n\r\n#ifndef NDEBUG\r\n    /* Sanity-check to make sure we're on the same timeline as the thing\r\n       we're trying to join. The backfiller will perform an equivalent check,\r\n       but if there's an error it would be nice to catch it where the action\r\n       was initiated. */\r\n\r\n    rassert(region_is_superset(our_branch_region_, svs_->get_region()));\r\n\r\n    object_buffer_t<fifo_enforcer_sink_t::exit_read_t> read_token;\r\n    svs_->new_read_token(&read_token);\r\n    region_map_t<binary_blob_t> start_point_blob;\r\n    svs_->do_get_metainfo(order_source->check_in(\"listener_t(A)\").with_read_mode(), &read_token, interruptor, &start_point_blob);\r\n    region_map_t<version_range_t> start_point = to_version_range_map(start_point_blob);\r\n\r\n    {\r\n        on_thread_t th(branch_history_manager->home_thread());\r\n        for (typename region_map_t<version_range_t>::const_iterator it = start_point.begin();\r\n             it != start_point.end();\r\n             ++it) {\r\n\r\n            version_t version = it->second.latest;\r\n            rassert(version.branch == branch_id_ ||\r\n                    version_is_ancestor(branch_history_manager,\r\n                                        version,\r\n                                        version_t(branch_id_, this_branch_history.initial_timestamp),\r\n                                        it->first));\r\n        }\r\n    }\r\n#endif\r\n\r\n    /* Attempt to register for reads and writes */\r\n    try_start_receiving_writes(broadcaster_metadata, interruptor);\r\n    listener_intro_t listener_intro;\r\n    bool registration_is_done = registration_done_cond_.try_get_value(&listener_intro);\r\n    guarantee(registration_is_done);\r\n\r\n    state_timestamp_t streaming_begin_point = listener_intro.broadcaster_begin_timestamp;\r\n\r\n    try {\r\n        /* Go through a little song and dance to make sure that the\r\n         * backfiller will at least get us to the point that we will being\r\n         * live streaming from. */\r\n\r\n        cond_t backfiller_is_up_to_date;\r\n        mailbox_t<void()> ack_mbox(\r\n            mailbox_manager_,\r\n            boost::bind(&cond_t::pulse, &backfiller_is_up_to_date),\r\n            mailbox_callback_mode_inline);\r\n\r\n        resource_access_t<replier_business_card_t> replier_access(replier);\r\n        send(mailbox_manager_, replier_access.access().synchronize_mailbox, streaming_begin_point, ack_mbox.get_address());\r\n\r\n        wait_any_t interruptor2(interruptor, replier_access.get_failed_signal());\r\n        wait_interruptible(&backfiller_is_up_to_date, &interruptor2);\r\n\r\n        /* Backfill */\r\n        backfillee(mailbox_manager_,\r\n                   branch_history_manager,\r\n                   svs_,\r\n                   svs_->get_region(),\r\n                   replier->subview(&listener_t::get_backfiller_from_replier_bcard),\r\n                   backfill_session_id,\r\n                   interruptor);\r\n    } catch (const resource_lost_exc_t &) {\r\n        throw backfiller_lost_exc_t();\r\n    }\r\n\r\n    object_buffer_t<fifo_enforcer_sink_t::exit_read_t> read_token2;\r\n    svs_->new_read_token(&read_token2);\r\n\r\n    region_map_t<binary_blob_t> backfill_end_point_blob;\r\n    svs_->do_get_metainfo(order_source->check_in(\"listener_t(B)\").with_read_mode(), &read_token2, interruptor, &backfill_end_point_blob);\r\n\r\n    region_map_t<version_range_t> backfill_end_point = to_version_range_map(backfill_end_point_blob);\r\n\r\n    /* Sanity checking. */\r\n\r\n    /* Make sure the region is not empty. */\r\n    guarantee(backfill_end_point.begin() != backfill_end_point.end());\r\n\r\n    /* The end timestamp is the maximum of the timestamps we've seen. If you've\r\n    been following closely (which you probably haven't because this is\r\n    confusing) you should be thinking \"How is that correct? If the region map\r\n    says that we're at timestamp 900 on region A and 901 on region B, then that\r\n    means we're missing the updates for the write with timestamp 900->901 on\r\n    region A. If the region map has different timestamps for different regions,\r\n    then it's not possible to boil down the current timestamp to a single\r\n    number, right?\" But this is wrong. Our backfill was, in fact, serialized\r\n    with respect to all the writes. In fact, the reason why region A has\r\n    timestamp 900 is that the write with timestamp 900->901 didn't touch any\r\n    keys in region A. We didn't update the timestamp for region A because\r\n    regions A and B were on separate threads, and we didn't want to send the\r\n    operation to region A unnecessarily if we weren't actually updating any keys\r\n    there. That's why it's OK to just take the maximum of all the timestamps\r\n    that we see.\r\n    TODO: If we change the way we shard such that each listener_t maps to a\r\n    single B-tree on the same machine, then replace this loop with a strict\r\n    assertion that requires everything to be at the same timestamp. */\r\n    state_timestamp_t backfill_end_timestamp = backfill_end_point.begin()->second.earliest.timestamp;\r\n    for (typename region_map_t<version_range_t>::const_iterator it = backfill_end_point.begin();\r\n         it != backfill_end_point.end();\r\n         ++it) {\r\n        guarantee(it->second.is_coherent());\r\n        guarantee(it->second.earliest.branch == branch_id_);\r\n        backfill_end_timestamp = std::max(backfill_end_timestamp, it->second.earliest.timestamp);\r\n    }\r\n\r\n    guarantee(backfill_end_timestamp >= streaming_begin_point);\r\n\r\n    current_timestamp_ = backfill_end_timestamp;\r\n    write_queue_coro_pool_callback_.init(new boost_function_callback_t<write_queue_entry_t>(\r\n            boost::bind(&listener_t::perform_enqueued_write, this, _1, backfill_end_timestamp, _2)));\r\n    write_queue_coro_pool_.init(new coro_pool_t<write_queue_entry_t>(\r\n            WRITE_QUEUE_CORO_POOL_SIZE, &write_queue_, write_queue_coro_pool_callback_.get()));\r\n    write_queue_semaphore_.set_capacity(WRITE_QUEUE_SEMAPHORE_LONG_TERM_CAPACITY);\r\n\r\n    if (write_queue_.size() <= WRITE_QUEUE_SEMAPHORE_LONG_TERM_CAPACITY) {\r\n        write_queue_has_drained_.pulse_if_not_already_pulsed();\r\n    }\r\n\r\n    wait_interruptible(&write_queue_has_drained_, interruptor);\r\n}\r\n```\r\n\r\nRemoving protocol_t doesn't make the clustering layer substantially simpler."
  , issueCommentId = 17905815
  }