IssueComment
  { issueCommentUpdatedAt = 2016 (-04) (-21) 18 : 09 : 54 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/213042161"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/2646#issuecomment-213042161"
  , issueCommentCreatedAt = 2016 (-04) (-21) 18 : 09 : 54 UTC
  , issueCommentBody =
      "@lucasjans This ticket was about aggregation queries slowing down in the presence of concurrent write operations. Unless you also have lots of background writes and the `count` is fast without the writes, but extremely slow with the writes, those are different issues.\r\nThe issue to follow is https://github.com/rethinkdb/rethinkdb/issues/3949 , which will include making `count` efficient.\r\n\r\nWe currently don't store table sizes in the b-tree index, and there are some complications to this due to sharding that make the solution non-trivial. So `count()` has no other way but to traverse the whole index (though it will skip loading the individual documents). This can be especially slow if the index doesn't fit into the cache, and things need to be fetched from disk."
  , issueCommentId = 213042161
  }