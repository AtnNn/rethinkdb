IssueComment
  { issueCommentUpdatedAt = 2014 (-10) (-22) 01 : 59 : 24 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 316661
        , simpleUserLogin = N "timmaxw"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/316661?v=3"
        , simpleUserUrl = "https://api.github.com/users/timmaxw"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/60026590"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/2646#issuecomment-60026590"
  , issueCommentCreatedAt = 2014 (-10) (-22) 01 : 59 : 24 UTC
  , issueCommentBody =
      "I was unable to reproduce this. I inserted 1,000,000 documents of approximately 100 bytes each into a table on a single server with a 10MB cache. Then I ran a count query with and without a concurrent workload of writes. I found that the concurrent workload slowed down the count query by up to a factor of ~5x, but only for heavy workloads of at least several hundred writes per second. For light write workloads I found no measurable slowdown. I tried with 1.15 and 1.13, on SSDs and rotational drives, with point updates, point inserts, batch updates, and batch inserts.\r\n\r\nThe worst result I got was a slowdown of ~5x in `.count()` when the write workload consisted of inserting batches of 100 new documents, empty except for autogenerated primary key, with `no_reply = True`, on a single connection. It averaged 2 batches per second on a rotation drive and 30 batches per second on a SSD."
  , issueCommentId = 60026590
  }