IssueComment
  { issueCommentUpdatedAt = 2014 (-07) (-23) 23 : 15 : 24 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 316661
        , simpleUserLogin = N "timmaxw"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/316661?v=3"
        , simpleUserUrl = "https://api.github.com/users/timmaxw"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/49948919"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/2738#issuecomment-49948919"
  , issueCommentCreatedAt = 2014 (-07) (-23) 23 : 15 : 24 UTC
  , issueCommentBody =
      "> I'm trying to find a solution that involves as little production downtime as possible. Since it seems like there is a node with a problem rethinkdb_data directory, I'm planning to just blow that away and bring up a new node. It should then backfill the replicas from the other nodes.\r\n\r\nThis will not solve the vector clock conflict issue. The metadata is replicated across all of the nodes in the cluster. If you bring up a new server, the vector clock conflict will propagate to it.\r\n\r\nWould you mind emailing a copy of one of your metadata files at tim@rethinkdb.com so I can look at what it actually contains? We don't really understand why the metadata conflict is not showing up as an issue. If you send me the file, then we can figure out what's actually going on, and that might suggest a way to fix it.\r\n\r\nIf I understand correctly, your cluster is actively serving queries, so you want as little downtime as possible; but the corrupted metadata isn't interfering with those queries, so fixing the corruption is not an emergency. Is my understanding correct?"
  , issueCommentId = 49948919
  }