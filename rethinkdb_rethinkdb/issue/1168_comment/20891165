IssueComment
  { issueCommentUpdatedAt = 2013 (-07) (-12) 17 : 29 : 17 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 43867
        , simpleUserLogin = N "jdoliner"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/43867?v=3"
        , simpleUserUrl = "https://api.github.com/users/jdoliner"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/20891165"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1168#issuecomment-20891165"
  , issueCommentCreatedAt = 2013 (-07) (-12) 17 : 29 : 17 UTC
  , issueCommentBody =
      "This has the serious downside that it makes it very easy for people  to write queries which work on their test data but fail once they get a larger dataset because they construct oversized arrays. This is arguably a problem in #1096 but I think people generally understand that aggregation can be memory intensive. Common aggregations like word counting already exhibit similar performance. My sense is that people generally expect joins to be scalable (something we highlight heavily in our marketing) and it would be really confusing if joins suddenly started failing in your production environment and you had to rewrite your code to use the other join syntax. I'd just prefer not to have such a fundamental operation be vulnerable to this. Also with the syntax in #1096 this is fairly straightforward to do, you just say:\r\n\r\n```Python\r\njoin.group_by(\"left\", r.rows(\"right\"))\r\n```\r\n\r\nreally isn't *that* bad but I think it makes it a lot more obvious that this could potentially be a memory intensive operation."
  , issueCommentId = 20891165
  }