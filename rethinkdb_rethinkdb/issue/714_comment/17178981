IssueComment
  { issueCommentUpdatedAt = 2013 (-04) (-29) 16 : 51 : 47 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 43867
        , simpleUserLogin = N "jdoliner"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/43867?v=3"
        , simpleUserUrl = "https://api.github.com/users/jdoliner"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/17178981"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/714#issuecomment-17178981"
  , issueCommentCreatedAt = 2013 (-04) (-29) 16 : 51 : 47 UTC
  , issueCommentBody =
      "I think limiting to a fixed number of rows is only sometimes sane. The thing we care about is the ratio of encoding time to database time. Limiting to 100 rows is limiting to between ~500 Bytes (I'm guessing here) and a Gigabyte. If someone tries something at the high end of the range we're still going to have a bottleneck in the protobuf libraries. It's hopefully easy to do too protobuf libraries give you a way to check the size of the encoded message."
  , issueCommentId = 17178981
  }