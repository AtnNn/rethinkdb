IssueComment
  { issueCommentUpdatedAt = 2013 (-10) (-11) 06 : 46 : 45 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 48436
        , simpleUserLogin = N "coffeemug"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/48436?v=3"
        , simpleUserUrl = "https://api.github.com/users/coffeemug"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/26117653"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/175#issuecomment-26117653"
  , issueCommentCreatedAt = 2013 (-10) (-11) 06 : 46 : 45 UTC
  , issueCommentBody =
      "I think the Unix philosophy of \"do one thing and do it well\" breaks down for profilers in practice. The profilers end up dumping too much data, which is useful in theory (if it could be analyzed), but is too voluminous to analyze in practice. Second-order analysis tools tend to leave a lot to be desired, so people end up feeling like they have enough information, but no immediately obvious actions they can take based on it.\r\n\r\nI can easily think of a heuristic that would probably do a good job: group low constant time deterministic operations. Basically all the math, all the operations on fields, etc, but not point reads, math with nondeterministic components, array ops, etc. This isn't ideal (random isn't deterministic but should be grouped anyway), but I think it's pretty good. We might need to tweak this, but I think we could arrive at good heuristics pretty quickly. It doesn't seem hard.\r\n\r\nAnyway, this is a bit hypothetical. I think we'll be able to tell very quickly what's worth doing and what isn't by running it on actual queries and examining the results."
  , issueCommentId = 26117653
  }