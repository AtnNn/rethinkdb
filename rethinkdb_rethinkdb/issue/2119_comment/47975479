IssueComment
  { issueCommentUpdatedAt = 2014 (-07) (-04) 06 : 21 : 05 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 1461947
        , simpleUserLogin = N "neumino"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/1461947?v=3"
        , simpleUserUrl = "https://api.github.com/users/neumino"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/47975479"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/2119#issuecomment-47975479"
  , issueCommentCreatedAt = 2014 (-07) (-03) 19 : 41 : 51 UTC
  , issueCommentBody =
      "So I wrote a little about it. I haven't proven that the proposal actually really works, but I couldn't prove that it was not too. On a \"simple\" example, it seems to work fine.\r\n\r\nI also know that there is actually tons of work involved in this proposal since it kind of tackle more than just scaling reads. So while I like the proposal because I think it will make things easier for users (it gives a simple definition of ack/replica), we may want to find another solution that is more doable in a short time.\r\n\r\nAlso, there are probably flaws in my proposal, it has been years since I have read anything about distributed systems.\r\n\r\n@coffeemug -- feel free to kill this proposal.\r\nIf you do, hopefully there are some nice idea that we may help for later.\r\n\r\n\r\nHere's the proposal:\r\n\r\n========================\r\n\r\n# User interface\r\n\r\nThe users set \r\n- a number X of up to date replicas (not including the master)\r\n- a number Y of replicas\r\n\r\nThe X replicas and the master are all up to date and can serve up to date read without having to route queries to the master\r\n\r\n\r\n## What we guarantee:\r\n\r\nAny write that is acknowledge by a client (and therefore by the master) will be seen by any up to date read that is served by an up to date replica.\r\n\r\n## What we don't guarantee:\r\nAn up to date read may see a write that has not been acknowledged.\r\nThis is because in a single master (no replica) configuration, the master may acknowledge a write, send the acknowledgment to the client, receive a new read before the acknowledgement make it to the client.\r\n\r\n\r\n\r\n\r\n# How it works\r\n\r\n## In normal conditions\r\n\r\nWhen a write is received:\r\n- the write is routed to the master\r\n- the master send it to all the replicas\r\n- the master write it on disk\r\n- the master wait for the X replicas to acknowledge the write (let's call this acknowledgement Ack1) - A replica will send an acknowledgment only when it has written the write to disk\r\n- the master weakly acknowledge the write (let's call this acknowledgment Ack2) and broadcast it to the X replicas\r\n- the X replicas send back another acknowledgement (Ack3)\r\n- the master definitively acknowledge the write (let's call it Ack4) and send it back to the client.\r\n\r\n\r\nWhen a client send a read to an up to date replicas, it directly reads from the up to date replicas without having to go to the master.\r\n\r\n\r\n## In case of failures, netsplit etc.\r\n\r\nThe master and the X replicas ping each other every T seconds (let's talk about the value of T later).\r\n\r\n\r\nSuppose that we have a netsplit, that creates two groups of connected servers. Group `1` has the master, group `2` doesn't have the master.\r\n\r\n\r\n\r\n### Case 1: Group `2` has less than (X+1)/2 machines_\r\n\r\nGroup `2` cannot elect a new master, and this is what happens.\r\n\r\nLet's note `A` a machine in group `2`\r\n- From group `1` point of view:  \r\nAfter `2*T` seconds (see race condition to read why `2*T` and not `T`, the master and the replicas in group `1` know that `A` is down/disconnected, and remove the \"up to date\" property from it.\r\nThe master can wait for 2*T seconds and acknowledge the write once the replicas in group `2` are flagged out of date.\r\n\r\nWrite availability is not lost (except for the writes that first hit a server in group 2). some writes will just be slower (have a latency of T seconds).\r\n\r\n- From group `2` point of view  \r\nFrom `A` point of view, after T seconds it will know that it is disconnected from the master and flag itself as out of date and will not serve any up to date read. \r\nSo if a client reach A and ask for up to date data, the read will fail (since it cannot reach an up to date replica).\r\n\r\n\r\n_Race condition_\r\n\r\nWe need to make sure that a machine in group `2` will flag itself out of date __before__ the master does the same.\r\n\r\nLet's note \952 the time at which the split is done, and let's suppose that the machines were connected before.\r\n\r\n- So the master could ping `A` at time `t1` where `\952-T<t1<\952` (`t1` is when the master got confirmation that `A` received the ping)  \r\n- So `A` could ping the master at time `t2` where `\952-T<t2<\952` (`t2` is when `A` got confirmation that the master received the ping)\r\n\r\n`A` is going to flag itself out of date at `max(t1-epsilon, t2)+T`  \r\nThe master is going to flag `A` out of date at `max(t2-epsilon, t1)+2*T`  \r\nWhere epsilon is roughly the latency between the two servers.  \r\n\r\nAnd we can guarantee that `A` will flag itself out of date before the master as long as we can guarantee `epsilon<T`.  \r\nThat's the flaw of the proposal. We can't make a hard guarantee about that I think.\r\n\r\n\r\n\r\n### Case 2: Group `2` has strictly more than (X+1)/2 machines_\r\n\r\nThen group `2` will do two things:\r\n- elect a new master (not sure which one we pick)\r\n- decide what is the last write that should be acknowledge.\r\n\r\nThe last write that group `2` should acknowledge is the most recent write for which every server in group `2` sent an Ack3.\r\nFor this write, the master may or may not have sent an Ack4.\r\n\r\nIf for a write, one of the server in group `2` didn't send an Ack3, it will never send it, so the master cannot acknowledge the write.\r\n\r\nThen the server in group `2` need to sync, and maybe do some rollback (or backfill?). It's hard to do (according to Tim and Slava), but from my understanding of our system, it should be reasonably possible (we \"just\" need not to garbage collect a write as long as the write was not acknowledge by the master). Again, this is just from my shallow understanding of the system.\r\n\r\n\r\n\r\nFrom a server in group `1`, after T seconds, they will flag themselves as out of date.\r\n\r\n\r\n\r\n# The reason behind the proposal\r\n\r\n## Scaling reads\r\n\r\nThere are two things to scale, writes and reads.\r\n\r\nCurrently, to:\r\n- Scale writes, you have to shard.\r\n- Scale reads, you have to shard\r\n\r\nThe proposal should allow people to:\r\n- Scale writes by sharding like we do now\r\n- Scale reads by adding up to date replicas\r\n\r\nSharding increase latency for queries. \r\n\r\nThe more shards you have, the less chance you have for `r.table('test').get(1)` to hit the good shard - So you will have to forward the query to the appropriate shard. \r\n\r\nWith up to date replicas, you don't need to shard that much, and you have more chance for the read query to hit a server that has the data you need.  \r\nIt would be really nice to be have a connection pool in the client that roughly knows what server to query to prevent intra cluster traffic.\r\n\r\nI believe that overall, this proposal will increase intra cluster traffic for writes, but reduce traffic for reads. (I also think that the most common workload is closer to 90/10 reads/writes than 10/90 reads/writes).\r\n\r\n\r\n## Give a sense to ack/replica\r\n\r\nIt gives a meaning to ack, replicas.\r\nNow ack is marginally useful from my point of view (since you don't know who actually acknowledged a write.\r\n\r\n\r\n## Simple yet useful guarantee\r\n\r\nWe keep the guarantee that any write acknowledge will be seen by an up to date read.\r\nThis is important to me, way more than scaling things\r\n\r\n## It solves other problems\r\n\r\n- It includes failover\r\n- It includes Tim's proposal to have a group of machines responsible for a table which would reduce the directory size.\r\n\r\n"
  , issueCommentId = 47975479
  }