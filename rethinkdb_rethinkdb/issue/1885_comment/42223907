IssueComment
  { issueCommentUpdatedAt = 2014 (-05) (-05) 18 : 55 : 13 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 139396
        , simpleUserLogin = N "wojons"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/139396?v=3"
        , simpleUserUrl = "https://api.github.com/users/wojons"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/42223907"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1885#issuecomment-42223907"
  , issueCommentCreatedAt = 2014 (-05) (-05) 18 : 55 : 13 UTC
  , issueCommentBody =
      "@nergdron i totally agree about testing on under powered hardware i built most of my app on a spinning disk cluster. So I was talking with @srh and my best guess before he makes his rolling on this is If you have a super large data set like the one i think i remeber you having is if your index goes out of memory your going to end up paging a lot. When you have just the primary index then your only paging for 1 index but if your have lots of secondray indexes it compands the problem. From what @srh said it could take 4 to 5 block reads to find the right spot in the index to write. and this could express the reason why your times are between 50-450 writes at once when you have a lot of blocks to read then its 50 a sec when u have few its 450. my guess is that its going to keep getting slower and slower. I would suggest that you build just pirmary key and then since indexes can build in the background well now you can let them build one at a time and should be a lot faster. Something you may want to do is have the node read the import file remortly if you have a lot of network badwith it will just lower the random io."
  , issueCommentId = 42223907
  }