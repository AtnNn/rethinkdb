IssueComment
  { issueCommentUpdatedAt = 2013 (-06) (-19) 20 : 31 : 22 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 48436
        , simpleUserLogin = N "coffeemug"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/48436?v=3"
        , simpleUserUrl = "https://api.github.com/users/coffeemug"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/19712682"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1026#issuecomment-19712682"
  , issueCommentCreatedAt = 2013 (-06) (-19) 20 : 31 : 22 UTC
  , issueCommentBody =
      "Awesome description, thanks!\r\n\r\n\r\nOn Wed, Jun 19, 2013 at 11:46 AM, wmrowan <notifications@github.com> wrote:\r\n\r\n> Sorry, I should have been more clear. There are 4 things I'm comparing\r\n> here.\r\n>\r\n>    1. Google's Python protobuf implementation\r\n>    2. Google's Python protobuf library with a C extension wrapping\r\n>    Google's C++ protobuf implementation\r\n>    3. Palm, Bump's Python protobuf implementation\r\n>    4. Google's C++ protobuf library and implementation (i.e. what we have\r\n>    on the server) independent of the Python interface.\r\n>\r\n> I wrote a test that constructs a large, highly nested object and then\r\n> repeatedly serializes and deserializes it and times this process. Options\r\n> 1-3 all use the same script while option 4 is a simple C++ program that\r\n> attempts to replicate the test in the Python script. I wouldn't say that\r\n> the result for option 4 is directly comparable to the others though due to\r\n> potentially subtle differences in the way the test is implemented. Here are\r\n> my conclusions:\r\n>\r\n> Option 1 is terrible. This issue is all about replacing it with something\r\n> faster, namely either option 2 or 3. I included option 4 as a control on\r\n> option 2. Given that option 4 is basically the backend for option 2, I\r\n> wanted to make sure the numbers were similar.\r\n>\r\n> While option 3 seems to be slightly faster than option 2 at\r\n> deserialization (~40%) it is almost 30x slower at serialization. With this\r\n> trade off, I would recommend using option 2 given our focus this release on\r\n> improving insert speed which is highly sensitive to serialization\r\n> performance but not deserialization performance. Option 2 does suffer from\r\n> deserialization errors beyond a nesting depth of ~63 in the test. Changing\r\n> the size of the document at each level doesn't affect this limit. Neither\r\n> option 3 or 4 suffer from this drawback (suggesting that the problem is in\r\n> the Python interface). I haven't run such a test on option 1 to see if the\r\n> limit exists there because the test would be too slow. If it is there\r\n> though than we're already living with this limitation. @coffeemug<https://github.com/coffeemug>suggests that such a nesting limit is high enough that is isn't a problem\r\n> for us. Option 2 also uses code that is already well tested th roughout our\r\n> code base (the Python interface code is already what we use and the C++\r\n> backend is the same as what we use in the server). Implementing option 3\r\n> would not only involve the extra work of swapping out the protobuf\r\n> interfacing code but would imply much more reliability testing and would\r\n> potentially introduce more bugs. On balance then I would still recommend\r\n> option 2. This is what I currently have implemented in the branch\r\n> fast_python_protobuf.\r\n>\r\n> The numbers for option 4 are actually slightly slower than the numbers for\r\n> option 2. While this is unexpected, I imagine the answer lies somewhere in\r\n> the subtle details of how the tests differ given that they are not run in\r\n> the same framework. The real point is that option 2 is not obviously slower\r\n> than option 4 suggesting that option 2 really does bring Python protobuf\r\n> serialization up to the same speed as the C++ version.\r\n>\r\n> The 38x improvement vs. 130x improvement numbers I mentioned before were\r\n> for different tests. The better looking numbers are for the current test\r\n> that constructs deeply nested objects (with a nesting depth of 63). Since\r\n> the pure Python code is probably quadratic in nesting depth while the C\r\n> extension code is linear in nesting depth the numbers look better the\r\n> deeper you go.\r\n>\r\n> Everything I've mentioned above applies to a test that only looks at\r\n> serialization and deserialization of a simple, if highly nested, message.\r\n> The real question of course is how this affects the performance of the\r\n> RethindDB Python driver. The simple test @coffeemug<https://github.com/coffeemug>and I did yesterday showed that even with the new protobuf backend running\r\n> the python based stress client on my laptop pegged all the cores with\r\n> python processes when run with large documents. We did not verify that this\r\n> was time spent within the serializer code or anything else about the\r\n> test.My next task will be to more robustly test driver performance with and\r\n> without the new backend and figure out why python still uses so much cpu.\r\n>\r\n> \8212\r\n> Reply to this email directly or view it on GitHub<https://github.com/rethinkdb/rethinkdb/issues/1026#issuecomment-19705050>\r\n> .\r\n>"
  , issueCommentId = 19712682
  }