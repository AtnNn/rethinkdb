IssueComment
  { issueCommentUpdatedAt = 2013 (-06) (-19) 18 : 46 : 35 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 646357
        , simpleUserLogin = N "wmrowan"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/646357?v=3"
        , simpleUserUrl = "https://api.github.com/users/wmrowan"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/19705050"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1026#issuecomment-19705050"
  , issueCommentCreatedAt = 2013 (-06) (-19) 18 : 46 : 35 UTC
  , issueCommentBody =
      "Sorry, I should have been more clear. There are 4 things I'm comparing here.\r\n\r\n1. Google's Python protobuf implementation\r\n2. Google's Python protobuf library with a C extension wrapping Google's C++ protobuf implementation\r\n3. Palm, Bump's Python protobuf implementation\r\n4. Google's C++ protobuf library and implementation (i.e. what we have on the server) independent of the Python interface.\r\n\r\nI wrote a test that constructs a large, highly nested object and then repeatedly serializes and deserializes it and times this process. Options 1-3 all use the same script while option 4 is a simple C++ program that attempts to replicate the test in the Python script. I wouldn't say that the result for option 4 is directly comparable to the others though due to potentially subtle differences in the way the test is implemented. Here are my conclusions:\r\n\r\nOption 1 is terrible. This issue is all about replacing it with something faster, namely either option 2 or 3. I included option 4 as a control on option 2. Given that option 4 is basically the backend for option 2, I wanted to make sure the numbers were similar.\r\n\r\nWhile option 3 seems to be slightly faster than option 2 at deserialization (~40%) it is almost 30x slower at serialization. With this trade off, I would recommend using option 2 given our focus this release on improving insert speed which is highly sensitive to serialization performance but not deserialization performance. Option 2 does suffer from deserialization errors beyond a nesting depth of ~63 in the test. Changing the size of the document at each level doesn't affect this limit. Neither option 3 or 4 suffer from this drawback (suggesting that the problem is in the Python interface). I haven't run such a test on option 1 to see if the limit exists there because the test would be too slow. If it is there though than we're already living with this limitation. @coffeemug suggests that such a nesting limit is high enough that is isn't a problem for us. Option 2 also uses code that is already well tested throughout our code base (the Python interface code is already what we use and the C++ backend is the same as what we use in the server). Implementing option 3 would not only involve the extra work of swapping out the protobuf interfacing code but would imply much more reliability testing and would potentially introduce more bugs. On balance then I would still recommend option 2. This is what I currently have implemented in the branch fast_python_protobuf.\r\n\r\nThe numbers for option 4 are actually slightly slower than the numbers for option 2. While this is unexpected, I imagine the answer lies somewhere in the subtle details of how the tests differ given that they are not run in the same framework. The real point is that option 2 is not obviously slower than option 4 suggesting that option 2 really does bring Python protobuf serialization up to the same speed as the C++ version.\r\n\r\nThe 38x improvement vs. 130x improvement numbers I mentioned before were for different tests. The better looking numbers are for the current test that constructs deeply nested objects (with a nesting depth of 63). Since the pure Python code is probably quadratic in nesting depth while the C extension code is linear in nesting depth the numbers look better the deeper you go.\r\n\r\nEverything I've mentioned above applies to a test that only looks at serialization and deserialization of a simple, if highly nested, message. The real question of course is how this affects the performance of the RethindDB Python driver. The simple test @coffeemug and I did yesterday showed that even with the new protobuf backend running the python based stress client on my laptop pegged all the cores with python processes when run with large documents. We did not verify that this was time spent within the serializer code or anything else about the test.My next task will be to more robustly test driver performance with and without the new backend and figure out why python still uses so much cpu."
  , issueCommentId = 19705050
  }