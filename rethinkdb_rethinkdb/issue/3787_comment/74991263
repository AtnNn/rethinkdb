IssueComment
  { issueCommentUpdatedAt = 2015 (-02) (-19) 02 : 52 : 01 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/74991263"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/3787#issuecomment-74991263"
  , issueCommentCreatedAt = 2015 (-02) (-19) 02 : 52 : 01 UTC
  , issueCommentBody =
      "@apa512 , thank you for providing the data. I'm still downloading it, but we just had a roughly comparable case and it turns out that the problem there was that reads have to queue up behind writes. So if you have lots of secondary indexes and do certain types of inserts (especially batched inserts) the reads might get blocked for an relatively long time waiting for all the writes to finish up.\r\n\r\nThe idea behind this blocking is to guarantee ordering between reads and writes, specifically that each read always sees the effect of any previous write. This is unnecessarily strict in its current implementation however, and we're working on a fix for that which will hopefully ship with RethinkDB 2.0 in a few weeks.\r\n\r\nSo far the speculative part. I yet have to actually verify that on your data. Will update you shortly once I have done some testing."
  , issueCommentId = 74991263
  }