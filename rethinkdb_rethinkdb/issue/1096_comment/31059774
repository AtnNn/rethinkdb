IssueComment
  { issueCommentUpdatedAt = 2013 (-12) (-21) 09 : 13 : 30 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 48436
        , simpleUserLogin = N "coffeemug"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/48436?v=3"
        , simpleUserUrl = "https://api.github.com/users/coffeemug"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/31059774"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1096#issuecomment-31059774"
  , issueCommentCreatedAt = 2013 (-12) (-21) 09 : 13 : 30 UTC
  , issueCommentBody =
      "Oh, I was referring to the number of events in a given group, not the number of groups.\r\n\r\nThe trouble with the grouping operation (and old `groupBy`/`groupedMapReduce`) is that you have to consume all the input to finish the aggregation for any given group (since you don't know if you need to update a group until you're done), which means you have to hold all the groups in memory. That's why we have to return an array.\r\n\r\nIf the user wants more than 100k groups, they could use `skip`/`limit` to effectively \"paginate\" grouping calculations."
  , issueCommentId = 31059774
  }