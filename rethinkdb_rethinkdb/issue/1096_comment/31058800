IssueComment
  { issueCommentUpdatedAt = 2013 (-12) (-21) 07 : 48 : 05 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 1461947
        , simpleUserLogin = N "neumino"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/1461947?v=3"
        , simpleUserUrl = "https://api.github.com/users/neumino"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/31058800"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1096#issuecomment-31058800"
  , issueCommentCreatedAt = 2013 (-12) (-21) 07 : 48 : 05 UTC
  , issueCommentBody =
      "Having `merge` be polymorphic on array makes the syntax shorter, but not so nicer.\r\nIf we add an option to do join and get back the results as arrays, I'm good with that (and that would be really better).\r\n\r\n\r\nI have one more question. Can we use `group` to aggregate lot of data (like in the case of time series data).\r\n\r\nThe question was\r\nhttps://twitter.com/bencevans/status/410578630956171264\r\n\r\nOne way to do it with groupedMapReduce now is with\r\n```\r\nr.db(\"ScrobbleGraph\").table(\"scrobbles\").groupedMapReduce(\r\n  function(doc) { return doc(\"date\").date() },\r\n  function(doc) { return [doc] },\r\n  function(left, right) { return left.add(right)}\r\n)\r\n```\r\n\r\nWhich doesn't work if you aggregate one more than 100k dates (which can easily happen if you aggregate data per second).\r\nPeople now can just use `orderBy` and build their structure in the client, but that's not really friendly."
  , issueCommentId = 31058800
  }