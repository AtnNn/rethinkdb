IssueComment
  { issueCommentUpdatedAt = 2014 (-12) (-19) 21 : 41 : 51 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/67701704"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/3462#issuecomment-67701704"
  , issueCommentCreatedAt = 2014 (-12) (-19) 21 : 41 : 51 UTC
  , issueCommentBody =
      "That's an interesting point, thanks for bringing it up @nviennot.\r\n\r\nYou can work around it by using `foreach` after the `distinct`:\r\n`r.table(:table).get_all(1,2,3, :index => :field1).distinct.foreach(|x| r.table(:table).get(x[:id]).update(:field2 => 2) )`\r\n\r\nReturning the number of *distinct* documents updated would require us to keep their primary keys in memory, which could blow the memory on very large tables. We could possibly have an opt arg for `update`, `delete` and `replace` that enables this behavior in cases where you expect a smallish set of matches.\r\n\r\nPinging @mlucy for opinions"
  , issueCommentId = 67701704
  }