Issue
  { issueClosedAt = Just 2015 (-08) (-13) 00 : 32 : 16 UTC
  , issueUpdatedAt = 2015 (-08) (-21) 23 : 07 : 20 UTC
  , issueEventsUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/4679/events"
  , issueHtmlUrl =
      Just "https://github.com/rethinkdb/rethinkdb/issues/4679"
  , issueClosedBy = Nothing
  , issueLabels =
      [ IssueLabel
          { labelColor = "e10c02"
          , labelUrl =
              "https://api.github.com/repos/rethinkdb/rethinkdb/labels/pr:high"
          , labelName = "pr:high"
          }
      , IssueLabel
          { labelColor = "e102d8"
          , labelUrl =
              "https://api.github.com/repos/rethinkdb/rethinkdb/labels/tp:bug"
          , labelName = "tp:bug"
          }
      ]
  , issueNumber = 4679
  , issueAssignee =
      Just
        SimpleUser
          { simpleUserId = Id 1777134
          , simpleUserLogin = N "mlucy"
          , simpleUserAvatarUrl =
              "https://avatars.githubusercontent.com/u/1777134?v=3"
          , simpleUserUrl = "https://api.github.com/users/mlucy"
          , simpleUserType = OwnerUser
          }
  , issueUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueTitle =
      "Changefeed failed sanity checks and server crashes in 2.1"
  , issuePullRequest = Nothing
  , issueUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/4679"
  , issueCreatedAt = 2015 (-08) (-13) 00 : 28 : 20 UTC
  , issueBody =
      Just
        "This was reported on the mailing list: https://groups.google.com/forum/#!topic/rethinkdb/A4lb8ViV7DY\n\nHere's the typical sort of error i was getting (tens of thousands of times per hour) when using changefeeds.  The part that says \"(server is buggy)\" is buggy is what makes me think that there is a bug in RethinkDB:\n\n```\nFEED 30288716-0d35-41e0-acbf-a81be2ae9527 is broken, so canceling -- {\"message\":\"SANITY CHECK FAILED: [batch.size() != 0 || (parent->coro_env->return_empty_normal_batches == return_empty_normal_batches_t::YES) || (bs.get_batch_type() == batch_type_t::NORMAL_FIRST)] at `src/rdb_protocol/datum_stream.cc:1286` (server is buggy).  Backtrace:\nWed Aug 12 21:11:16 2015\n\n1: backtrace_t::backtrace_t() at 0xa4acb0 (/usr/bin/rethinkdb)\n2: lazy_backtrace_formatter_t::lazy_backtrace_formatter_t() at 0xa4afae (/usr/bin/rethinkdb)\n3: ql::runtime_sanity_check_failed(char const*, int, char const*, std::string const&) at 0x789736 (/usr/bin/rethinkdb)\n4: ql::coro_stream_t::cb(auto_drainer_t::lock_t) at 0x7cdd57 (/usr/bin/rethinkdb)\n5: callable_action_instance_t<ql::coro_stream_t::maybe_launch_read()::{lambda()#2}>::run_action() at 0x7ce120 (/usr/bin/rethinkdb)\n6: coro_t::run() at 0x954418 (/usr/bin/rethinkdb)\n in:\nr.db(\"smc\").table(\"file_use\").getAll(\"0ab77792-c73f-4173-b3f8-3ee9b62b265e\", ...\nEtc.\n```\n\nI was running 3 nodes with 3 shards and replication factor of 3 for each table (on Ubuntu 15.04 on Google Compute Engine, with 8-core nodes on SSD's).  With this setup one of three rethinkdb servers itself would often crash completely (with errors about inconsistencies in the log_file). Here's one such crash log error:\n\n```\n2015-08-12T22:21:30.086420124 47.641620s notice: Connected to server \"rethink1\" 5c0f5431-e72d-4e30-bda3-a4804f19d281\n2015-08-12T22:23:19.046464266 156.601664s error: Error in src/rdb_protocol/changefeed.cc at line 1739:\n2015-08-12T22:23:19.046499149 156.601698s error: Guarantee failed: [active()]\n2015-08-12T22:23:19.046508241 156.601707s error: Backtrace:\n2015-08-12T22:23:19.200879225 156.756080s error: Error in src/rdb_protocol/changefeed.cc at line 1739:\n2015-08-12T22:23:19.200918701 156.756118s error: Guarantee failed: [active()]\n2015-08-12T22:23:19.200928371 156.756127s error: Backtrace:\n2015-08-12T22:23:19.286724209 156.841926s error: Wed Aug 12 22:23:19 2015\\n\\n1: backtrace_t::backtrace_t() at ??:?\\n2: format_backtrace(bool) at ??:?\\n3: report_fatal_error\n(char const*, int, char const*, ...) at ??:?\\n4: ql::changefeed::range_sub_t::apply_ops(ql::datum_t) at ??:?\\n5: ql::changefeed::msg_visitor_t::operator()(ql::changefeed::m\nsg_t::change_t const&) const::{lambda(ql::changefeed::range_sub_t*)#1}::operator()(ql::changefeed::range_sub_t*) const at ??:?\\n6: void ql::changefeed::feed_t::each_sub_in_\nvec_cb<ql::changefeed::range_sub_t>(std::function<void (ql::changefeed::range_sub_t*)> const&, std::vector<std::set<ql::changefeed::range_sub_t*, std::less<ql::changefeed::\nrange_sub_t*>, std::allocator<ql::changefeed::range_sub_t*> >, std::allocator<std::set<ql::changefeed::range_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::allocator\n<ql::changefeed::range_sub_t*> > > > const&, std::vector<int, std::allocator<int> > const&, int) at ??:?\\n7: callable_action_instance_t<pmap_runner_one_arg_t<std::_Bind<std\n::_Mem_fn<void (ql::changefeed::feed_t::*)(std::function<void (ql::changefeed::range_sub_t*)> const&, std::vector<std::set<ql::changefeed::range_sub_t*, std::less<ql::chang\nefeed::range_sub_t*>, std::allocator<ql::changefeed::range_sub_t*> >, std::allocator<std::set<ql::changefeed::range_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::al\nlocator<ql::changefeed::range_sub_t*> > > > const&, std::vector<int, std::allocator<int> > const&, int)> (ql::changefeed::feed_t*, std::reference_wrapper<std::function<void\n (ql::changefeed::range_sub_t*)> const>, std::reference_wrapper<std::vector<std::set<ql::changefeed::range_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::allocator<q\nl::changefeed::range_sub_t*> >, std::allocator<std::set<ql::changefeed::range_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::allocator<ql::changefeed::range_sub_t*>\n> > > const>, std::reference_wrapper<std::vector<int, std::allocator<int> > const>, std::_Placeholder<1>)>, long> >::run_action() at ??:?\\n8: coro_t::run() at ??:?\n2015-08-12T22:23:19.286882340 156.842082s error: Exiting.\n```\n\nHere's another:\n\n```\n2015-08-12T22:53:20.199052864 1406.264295s error: Error in src/rdb_protocol/changefeed.cc at line 1739:\n2015-08-12T22:53:20.199113755 1406.264355s error: Guarantee failed: [active()]\n2015-08-12T22:53:20.199124440 1406.264365s error: Backtrace:\n2015-08-12T22:53:20.468995729 1406.534238s error: Wed Aug 12 22:53:20 2015\\n\\n1: backtrace_t::backtrace_t() at ??:?\\n2: format_backtrace(bool) at ??:?\\n3: report_fatal_erro\nr(char const*, int, char const*, ...) at ??:?\\n4: ql::changefeed::range_sub_t::apply_ops(ql::datum_t) at ??:?\\n5: ql::changefeed::msg_visitor_t::operator()(ql::changefeed::\nmsg_t::change_t const&) const::{lambda(ql::changefeed::range_sub_t*)#1}::operator()(ql::changefeed::range_sub_t*) const at ??:?\\n6: void ql::changefeed::feed_t::each_sub_in\n_vec_cb<ql::changefeed::range_sub_t>(std::function<void (ql::changefeed::range_sub_t*)> const&, std::vector<std::set<ql::changefeed::range_sub_t*, std::less<ql::changefeed:\n:range_sub_t*>, std::allocator<ql::changefeed::range_sub_t*> >, std::allocator<std::set<ql::changefeed::range_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::allocato\nr<ql::changefeed::range_sub_t*> > > > const&, std::vector<int, std::allocator<int> > const&, int) at ??:?\\n7: pmap_runner_one_arg_t<std::_Bind<std::_Mem_fn<void (ql::change\nfeed::feed_t::*)(std::function<void (ql::changefeed::range_sub_t*)> const&, std::vector<std::set<ql::changefeed::range_sub_t*, std::less<ql::changefeed::range_sub_t*>, std:\n:allocator<ql::changefeed::range_sub_t*> >, std::allocator<std::set<ql::changefeed::range_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::allocator<ql::changefeed::ra\nnge_sub_t*> > > > const&, std::vector<int, std::allocator<int> > const&, int)> (ql::changefeed::feed_t*, std::reference_wrapper<std::function<void (ql::changefeed::range_su\nb_t*)> const>, std::reference_wrapper<std::vector<std::set<ql::changefeed::range_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::allocator<ql::changefeed::range_sub_t\n*> >, std::allocator<std::set<ql::changefeed::range_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::allocator<ql::changefeed::range_sub_t*> > > > const>, std::referen\nce_wrapper<std::vector<int, std::allocator<int> > const>, std::_Placeholder<1>)>, long>::operator()() at ??:?\\n8: void pmap<std::_Bind<std::_Mem_fn<void (ql::changefeed::fe\ned_t::*)(std::function<void (ql::changefeed::range_sub_t*)> const&, std::vector<std::set<ql::changefeed::range_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::allocat\nor<ql::changefeed::range_sub_t*> >, std::allocator<std::set<ql::changefeed::range_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::allocator<ql::changefeed::range_sub_\nt*> > > > const&, std::vector<int, std::allocator<int> > const&, int)> (ql::changefeed::feed_t*, std::reference_wrapper<std::function<void (ql::changefeed::range_sub_t*)> c\nonst>, std::reference_wrapper<std::vector<std::set<ql::changefeed::range_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::allocator<ql::changefeed::range_sub_t*> >, st\nd::allocator<std::set<ql::changefeed::range_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::allocator<ql::changefeed::range_sub_t*> > > > const>, std::reference_wrapp\ner<std::vector<int, std::allocator<int> > const>, std::_Placeholder<1>)> >(long, long, std::_Bind<std::_Mem_fn<void (ql::changefeed::feed_t::*)(std::function<void (ql::chan\ngefeed::range_sub_t*)> const&, std::vector<std::set<ql::changefeed::range_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::allocator<ql::changefeed::range_sub_t*> >, s\ntd::allocator<std::set<ql::changefeed::range_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::allocator<ql::changefeed::range_sub_t*> > > > const&, std::vector<int, st\nd::allocator<int> > const&, int)> (ql::changefeed::feed_t*, std::reference_wrapper<std::function<void (ql::changefeed::range_sub_t*)> const>, std::reference_wrapper<std::ve\nctor<std::set<ql::changefeed::range_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::allocator<ql::changefeed::range_sub_t*> >, std::allocator<std::set<ql::changefeed:\n:range_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::allocator<ql::changefeed::range_sub_t*> > > > const>, std::reference_wrapper<std::vector<int, std::allocator<in\nt> > const>, std::_Placeholder<1>)> const&) at ??:?\\n9: void ql::changefeed::feed_t::each_sub_in_vec<ql::changefeed::range_sub_t>(std::vector<std::set<ql::changefeed::range\n_sub_t*, std::less<ql::changefeed::range_sub_t*>, std::allocator<ql::changefeed::range_sub_t*> >, std::allocator<std::set<ql::changefeed::range_sub_t*, std::less<ql::change\nfeed::range_sub_t*>, std::allocator<ql::changefeed::range_sub_t*> > > > const&, rwlock_in_line_t*, auto_drainer_t::lock_t const&, std::function<void (ql::changefeed::range_\nsub_t*)> const&) at ??:?\\n10: ql::changefeed::feed_t::each_active_range_sub(auto_drainer_t::lock_t const&, std::function<void (ql::changefeed::range_sub_t*)> const&) at ??:\n?\\n11: ql::changefeed::msg_visitor_t::operator()(ql::changefeed::msg_t::change_t const&) const at ??:?\\n12: ql::changefeed::real_feed_t::mailbox_cb(signal_t*, ql::changefee\nd::stamped_msg_t) at ??:?\\n13: void std::_Mem_fn<void (ql::changefeed::real_feed_t::*)(signal_t*, ql::changefeed::stamped_msg_t)>::operator()<signal_t*, ql::changefeed::sta\nmped_msg_t, void>(ql::changefeed::real_feed_t*, signal_t*&&, ql::changefeed::stamped_msg_t&&) const at ??:?\\n14: std::_Function_handler<void (signal_t*, ql::changefeed::sta\nmped_msg_t), std::_Bind<std::_Mem_fn<void (ql::changefeed::real_feed_t::*)(signal_t*, ql::changefeed::stamped_msg_t)> (ql::changefeed::real_feed_t*, std::_Placeholder<1>, s\ntd::_Placeholder<2>)> >::_M_invoke(std::_Any_data const&, signal_t*, ql::changefeed::stamped_msg_t) at ??:?\\n15: mailbox_t<void (ql::changefeed::stamped_msg_t)>::read_impl_\nt::read(read_stream_t*, signal_t*) at ??:?\\n16: mailbox_manager_t::mailbox_read_coroutine(connectivity_cluster_t::connection_t*, auto_drainer_t::lock_t, threadnum_t, unsign\ned long, std::vector<char, std::allocator<char> >*, long, mailbox_manager_t::force_yield_t) at ??:?\\n17: /usr/bin/rethinkdb() [0x975660] at 0x975660 ()\\n18: coro_t::run() a\nt ??:?\n```\n---\n\nAfter this I turned off sharding for all tables, and waited a while, but still got a lot of errors\nrethinkdb servers crashing.   (NOTE: when using rethinkdb2.1 beta last month for testing, it would always\ncrash on my data when sharded.)  \n\nI then dumped the entire database using rethinkdb's dump command, and imported it into \na single-node clean RethinkDB 2.1 server.  This eliminated any possibility of the failover\nstuff being involved directly.   Also, it meant that the data was cleanly read back in from\nJSON.\n\nI'm still continuing to get changefeed internal errors, even with only one node.\nThe error rate went down by a factor of about 100 though, so now the servers\ncan at least somewhat keep up with user demand so I can try to take a breath\nand see if I can understand other mistakes I'm making that could be leading to\ntoo much load, etc. \n\nBut any ideas?  Can you guys let me know if the above log messages ring any bells? \n"
  , issueState = "closed"
  , issueId = Id 100664105
  , issueComments = 2
  , issueMilestone =
      Just
        Milestone
          { milestoneCreator =
              SimpleUser
                { simpleUserId = Id 706854
                , simpleUserLogin = N "AtnNn"
                , simpleUserAvatarUrl =
                    "https://avatars.githubusercontent.com/u/706854?v=3"
                , simpleUserUrl = "https://api.github.com/users/AtnNn"
                , simpleUserType = OwnerUser
                }
          , milestoneDueOn = Nothing
          , milestoneOpenIssues = 1
          , milestoneNumber = 17
          , milestoneClosedIssues = 595
          , milestoneDescription =
              Just
                "The scope of this issue is covered by another issue. The closing comment should link to the other issue."
          , milestoneTitle = "duplicate"
          , milestoneUrl =
              "https://api.github.com/repos/rethinkdb/rethinkdb/milestones/17"
          , milestoneCreatedAt = 2013 (-03) (-29) 20 : 23 : 12 UTC
          , milestoneState = "closed"
          }
  }