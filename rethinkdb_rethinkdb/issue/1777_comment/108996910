IssueComment
  { issueCommentUpdatedAt = 2015 (-06) (-04) 18 : 16 : 06 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/108996910"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1777#issuecomment-108996910"
  , issueCommentCreatedAt = 2015 (-06) (-04) 18 : 16 : 06 UTC
  , issueCommentBody =
      "I think MongoDB might be using a larger write buffer to temporarily absorb unwritten data, though I'm not sure how this works with WiredTiger (have you been using that storage engine or the traditional one?).\r\nAs a consequence, RethinkDB throttles down to what the storage system can actually absorb earlier, which might explain why it becomes comparably slower in the 100k insert case.\r\nThat or the batch becomes too large and must be split up into a couple smaller batches for optimal efficiency (not sure)."
  , issueCommentId = 108996910
  }