IssueComment
  { issueCommentUpdatedAt = 2013 (-01) (-22) 19 : 00 : 44 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 646357
        , simpleUserLogin = N "wmrowan"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/646357?v=3"
        , simpleUserUrl = "https://api.github.com/users/wmrowan"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/12560114"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/252#issuecomment-12560114"
  , issueCommentCreatedAt = 2013 (-01) (-22) 19 : 00 : 44 UTC
  , issueCommentBody =
      "This reminds me of a similar question about bulk imports I received before. As you can see above, it's not just the JSON parsing that forces us to load all data from a large JSON file into memory at once. As is there may be several copies of your data in memory at once before it's actually sent to the server. While this would be a substantial project (and would even require important changes to the server) it might make sense to eventually support the streaming of queries to the server without ever loading the full query in memory so as to support arbitrarily large bulk inserts."
  , issueCommentId = 12560114
  }