IssueComment
  { issueCommentUpdatedAt = 2014 (-02) (-13) 19 : 52 : 22 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 552910
        , simpleUserLogin = N "Tryneus"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/552910?v=3"
        , simpleUserUrl = "https://api.github.com/users/Tryneus"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/35018356"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1911#issuecomment-35018356"
  , issueCommentCreatedAt = 2014 (-02) (-13) 19 : 52 : 22 UTC
  , issueCommentBody =
      "So I went through and identified the biggest problems, prioritized them, and have a rough outline of a proposal in 6 phases:\r\n\r\n---\r\n\r\nPhase 1: Directory and Blueprint optimization\r\n - Remove the NOTHING role from the Blueprint\r\n  - Will drastically reduce the size of the blueprint in large clusters\r\n - Remove the `nothing` role from the Directory\r\n  - Will drastically reduce the size of the directory in large clusters\r\n  - Leave the `nothing_when_safe` and `nothing_when_done_erasing` roles\r\n - Need to know when to spawn nothing roles to perform backfilling and deletion\r\n\r\n---\r\n\r\nPhase 2: Auto-Failover\r\n - Integrate a Paxos or Raft (or some other consensus) library as a new service in the connectivity layer\r\n  - As opposed to a daemon, which would add more dependencies and more complicated cluster management for users\r\n  - BSD or otherwise free library we can build straight in or staticly link would be preferred\r\n    - libpaxos3 (https://bitbucket.org/sciascid/libpaxos) - should be able to work this into our architecture without much trouble\r\n    - CRaft (https://github.com/willemt/CRaft) - simpler interface, but missing some key features, we could contribute\r\n  - Alternative is to implement a consensus algorithm ourselves, but that is a last resort\r\n - Consensus library will be used to synchronize a Master Failover Table\r\n  - When the master failover table changes, reactor roles need to be reevaluated\r\n - There appears to be a window for data divergence in how writes are committed\r\n  - When recovering from a failover, we need the new master to eclipse the old master, if divergence is detected.\r\n  - This should not be an error from the user's perspective, as any rollbacked writes will not have been acked to any client\r\n\r\nProcess to elect a new master:\r\nOn peer disconnect\r\n> For each shard the peer was master of (known by checking the blueprint and master failover table):\r\n>   1. If the table has a number of write acks greater than half the number of replicas (rounded up), we can attempt failover without worrying about data divergence\r\n>   2. Propose a new master from the list of connected peers hosting a replica for that shard\r\n>   3. Leader polls each replica for the shard to ensure that contact to the peer is gone\r\n>   4. If at least half the replicas (rounded up) cannot contact the peer, commit the change to the master failover table\r\n\r\nOn peer reconnect\r\n> For each shard the peer should be master of, but isn't (known by checking the blueprint and master failover table):\r\n>   1. Propose that the peer becomes master\r\n>   2. Leader polls each replica for the shard to ensure that contact to the peer is restored\r\n>   3. If at least half the replicas (rounded up) can contact the peer, commit the change to the master failover table\r\n\r\n---\r\n\r\nPhase 3: ReQL Cluster API\r\n - r.cluster() operations to change semilattice metadata\r\n - Ideally, treat the semilattice metadata as a virtual table or tables\r\n - Change the admin CLI and web UI to use this interface\r\n\r\n---\r\n\r\nPhase 4: Minimal downtime on blueprint change\r\n - Change reactor shutdown such that a machine will continue serving queries until a new master is available\r\n - Could probably reuse a lot of the auto failover logic \r\n\r\n---\r\n\r\nPhase 5: Blueprint generation without full connectivity\r\n - Not sure if this is possible, but we need it\r\n\r\n---\r\n\r\nPhase 6: More Directory and Blueprint optimization\r\n - Use hash maps instead of trees for table and mailbox lookup\r\n   - Important if we want to scale to hundreds of thousands of tables\r\n - Consider using a single mailbox per core for all tables, rather than one for each table\r\n   - This would make the directory much smaller, at the cost of complicating reactor transitions\r\n - Deterministic blueprints\r\n   - Move the blueprint out of the cluster semilattice metadata\r\n   - Save time distributing blueprints, which could feasibly be dozens of MB in size\r\n   - Instead, pass around blueprint hashes, and only send the entire blueprint if there is a desync\r\n\r\n---\r\n\r\nAs you can see, there are a number of open questions.  The biggest are:\r\nPhase 2: Which consensus library to use and how to integrate it into our clustering system\r\nPhase 3: ReQL Cluster API description\r\nPhase 3: How to interface a virtual table with the rest of the ReQL protocol\r\nPhase 5: Feasibility and architecture design\r\n\r\nPhases 1 and 6 are optimization changes, but phase 1 is there because it should be relatively easy and hits one of the biggest bottlenecks in the current state of things.  Phases 2 and 3 are necessary for a production-ready product.  I wouldn't say phases 4 and 5 are *necessary* for production-readiness, but I would strongly recommend them."
  , issueCommentId = 35018356
  }