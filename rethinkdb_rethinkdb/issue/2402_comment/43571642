IssueComment
  { issueCommentUpdatedAt = 2014 (-05) (-19) 23 : 42 : 01 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 8194
        , simpleUserLogin = N "chrisguidry"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/8194?v=3"
        , simpleUserUrl = "https://api.github.com/users/chrisguidry"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/43571642"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/2402#issuecomment-43571642"
  , issueCommentCreatedAt = 2014 (-05) (-19) 23 : 42 : 01 UTC
  , issueCommentBody =
      "Those are all good to know, thanks @danielmewes and @coffeemug.  Can you guys comment on the pros/cons of us adding an additional node of the same size or sticking with two nodes with more memory?\r\n\r\nFor our use case, we're adding data at a greater-than-linear rate; we're collecting hourly/daily statistics on a set of objects, and the objects we track are increasing linearly on a weekly basis.  (Sorry, can't be more specific).  For that main \"fact\" table capturing the statistics, we're storing 2 replicas of 16 shards partitioned on a rolling date window which we adjust daily.\r\n\r\n>>> Memory for storing intermediate data for each running query and background operation (e.g. backfills, secondary index postconstruction)\r\n\r\nI think we may have hit memory exhaustion for this during a few intense index creation events when we were running 1.11 (sorry, we didn't report the issues).  I think we're okay for now to leave some RAM on the table for the OS and these administrative events, but ideally, RethinkDB would just read both our and our users' minds and adjust it's memory allocation accordingly."
  , issueCommentId = 43571642
  }