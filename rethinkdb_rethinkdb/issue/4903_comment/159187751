IssueComment
  { issueCommentUpdatedAt = 2015 (-11) (-24) 07 : 51 : 40 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 48936
        , simpleUserLogin = N "thedrow"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/48936?v=3"
        , simpleUserUrl = "https://api.github.com/users/thedrow"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/159187751"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/4903#issuecomment-159187751"
  , issueCommentCreatedAt = 2015 (-11) (-24) 07 : 51 : 40 UTC
  , issueCommentBody =
      "There are multiple optimizations I can come up with that will help utilize CPU more correctly. Some of them involve using the (Leader/Follower pattern)[http://kircher-schwanninger.de/michael/publications/lf.pdf] which I assume RethinkDB is already implementing at least partially since a quick search in your code came up with a threadpool, an event queue and polling mechanisms such as epoll, poll and select which are the exact building blocks for the pattern.\r\nStatic pinning does not occur in the classic implementation of the L/F pattern which may help distribute CPU more correctly but there is a proposed variant named by the paper as \"Hybrid thread associations\" that does just that.\r\nThe document I linked to describes the pattern quite well but I'll summarize myself for those who are not interested in reading it entirely.\r\nIn the original proposal the thread pool is divided into two categories: Leader and Followers.\r\nThe Leader is responsible for listening to I/O events and the followers are responsible for processing I/O events.\r\nThe variant I'd recommend using is to allow multiple Leaders to coexist and this is briefly mentioned in the paper.\r\nWhen a Leader receives an I/O event it promotes an available follower thread to be a Leader and demotes itself to be a follower.\r\nThis allows us to scale the amount of I/O processing we are able to perform at a given moment of time by adding more Leaders or Followers accordingly.\r\nI'm assuming that intracluster communication doesn't happen 100% of the time and that there are times where it is completely idle. One of the Leader threads (or more) can watch the intracluster file descriptor among other file handles it might handle which will allow us to process other events when that thread is idle. Furthermore, if intracluster chatter is high, more threads will be able to process those and move on to processing other events. That can happen if the order of messages coming from intracluster commnication doesn't matter.\r\nCPU intensive tasks such as building indexes will pop the threads from the L/F pool into a dedicated pool of workers. As such I/O operations will be processed more slowly and infrequently (assuming the same rate of events is maintained) but since we're pinning threads to CPUs we can automatically scale up the L/F thread pool which is usually I/O bound so we'll be still able to at the worst case initiate the same amount of I/O events processing as before.\r\nWe can also add more Leaders based on current CPU utilization. If currently each follower thread uses 33% and the work it is performing can be paralleled we can assign two worker threads to that follower thread (new or already idle) to parallelize the work. If the work is not parallelizable we can spawn two new Leader threads that will expect that same kind of event (or alternatively same kind of CPU intensiveness) based on previous requests made to the node. \r\nThis requires a priority queue which always attempts to keep the less CPU intensive tasks at the top of the queue (so that more work can be performed in parallel).\r\n\r\nThis way you can utilize all your CPU cores and dynamically balance between I/O and CPU bound tasks.\r\n\r\nThere are other optimization opportunities regarding b-tree indexes that may or may not depend on the architecture and hardware of your servers.\r\n- We can distribute the building of B-Tree indexes between the available read replicas which will free the busy nodes in the cluster to respond to other events. The penalty of serialization and de-serialization of the B-Tree may or may not exceed the the value provided by this approach.\r\n- We can optimize the building and searching of B-Trees for processors that support SIMD instructions (which are available on AWS btw) using methods like (FAST)[http://cs.nyu.edu/~lerner/spring12/Preso06-SIMDTree.pdf]. This will increase the availability of other workers to other tasks since it requires less time to build a B-Tree index this way.\r\n- We can use a combination of SIMD instructions and GPUs to search and build the indexes as described in http://www.umiacs.umd.edu/~joseph/Indexing_IPDPS2011.pdf and http://www.mayankdaga.com/wp-content/uploads/2012/11/paper.pdf\r\nNote: I have to read these myself, but by just skimming through them I can see that they are relevant.\r\n- We can introduce other index types which are more efficient for memory-first databases and are suitable for hot data that is constantly being read like in http://www3.informatik.tu-muenchen.de/~leis/papers/ART.pdf\r\n\r\nI'm not familiar with RethinkDB internals so my suggestions may be irrelevant but I'd love to hear what you guys think.\r\n"
  , issueCommentId = 159187751
  }