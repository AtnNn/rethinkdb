IssueComment
  { issueCommentUpdatedAt = 2014 (-11) (-05) 04 : 23 : 43 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 1777134
        , simpleUserLogin = N "mlucy"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/1777134?v=3"
        , simpleUserUrl = "https://api.github.com/users/mlucy"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/61758187"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/3264#issuecomment-61758187"
  , issueCommentCreatedAt = 2014 (-11) (-05) 04 : 23 : 43 UTC
  , issueCommentBody =
      "Alright, found it.  The fix is in CR 2278 by @danielmewes .  It turned out to be a sort-of-subtle batching bug that only shows up if unsharding discards more rows than we expect (it's also the second bug caused by our \"batchspec_t::all() actually reads everything\" assumption -- we should consider changing the code to not rely on that, since we treat batchspecs as hints in all other situations).\r\n\r\nThanks for your patience @sctb !  The data files really helped with tracking this down.\r\n\r\n@AtnNn -- we should do a point-release for this bug once the CR is done.\r\n\r\n@sctb -- is this bug blocking anything on your end?  If so, we can put together a custom package for you so you don't have to wait for the official release."
  , issueCommentId = 61758187
  }