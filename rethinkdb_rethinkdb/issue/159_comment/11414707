IssueComment
  { issueCommentUpdatedAt = 2012 (-12) (-16) 06 : 11 : 08 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 48436
        , simpleUserLogin = N "coffeemug"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/48436?v=3"
        , simpleUserUrl = "https://api.github.com/users/coffeemug"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/11414707"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/159#issuecomment-11414707"
  , issueCommentCreatedAt = 2012 (-12) (-16) 06 : 11 : 08 UTC
  , issueCommentBody =
      "@neumino -- there are no fundamental technical reasons why we should use so much space in this case, nor why we can't use a heap sort. The reason here is that sorting this out (pun intended) is work, and it hasn't made it high enough on the list of priorities yet.\r\n\r\n@mlucy -- I think we should limit the product to sorts that fit into RAM for the time being. There are much better products that can do sorts on very large datasets, and investing time into doing this sort of thing with RethinkDB isn't worth it right now. Once we sort out the primary key issue, I'd do two things for this:\r\n\r\n* Work out why we use so much RAM and minimize it if possible\r\n* If the sort requires *too* much RAM, exit with an error\r\n\r\nEventually we can also support proper distributed merge sorts, and possibly even serialize interim datasets to disk to allow sorting huge datasets, but for the time being returning an error on sorts that require too much RAM is ok."
  , issueCommentId = 11414707
  }