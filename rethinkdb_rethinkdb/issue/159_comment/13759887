IssueComment
  { issueCommentUpdatedAt = 2013 (-02) (-19) 07 : 15 : 28 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 43867
        , simpleUserLogin = N "jdoliner"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/43867?v=3"
        , simpleUserUrl = "https://api.github.com/users/jdoliner"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/13759887"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/159#issuecomment-13759887"
  , issueCommentCreatedAt = 2013 (-02) (-19) 07 : 15 : 28 UTC
  , issueCommentBody =
      "It seems like serializing to a BTree is actually a great solution to this then and it's easy enough that we could even do it for 1.4. If we just do orderBy on streams by creating a BTree with a cache size of 30 MB then it seems like we get the memory cap we want without having to fail queries that go over. We can set up a memory cap for queries in general but if there's no way at all to do orderBy on datasets over 30 MB then that's actually really limiting. This gives us a way to order basically any data set that will fit on disk.\r\n\r\nI actually think we should definitely do this for 1.4."
  , issueCommentId = 13759887
  }