IssueComment
  { issueCommentUpdatedAt = 2012 (-12) (-16) 05 : 28 : 44 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 1777134
        , simpleUserLogin = N "mlucy"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/1777134?v=3"
        , simpleUserUrl = "https://api.github.com/users/mlucy"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/11414467"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/159#issuecomment-11414467"
  , issueCommentCreatedAt = 2012 (-12) (-16) 05 : 28 : 44 UTC
  , issueCommentBody =
      "The first is clearly a problem.\r\n\r\nThe second seems murkier to me.  If we aren't sorting by a primary key, then our only real options (in the general case) are to go to disk or kill the query.  It wouldn't be at all unreasonable for a 3GB dataset to use up 6GB of memory when we load it up, and with an extra 2GB or so for RethinkDB to do everything else that would get us up to 8GB.  I would guess we get even worse than a factor of two right now, and that we could improve the ratio, but the general problem wouldn't go away.\r\n\r\nClearly writing data to disk ourselves is better than going to swap (not least because most people don't give servers much, if any, swap), so we should probably do that when we sort large streams, but that's about the best we can do.  Once we have secondary indices, that would be another solution to this problem that we could suggest to people."
  , issueCommentId = 11414467
  }