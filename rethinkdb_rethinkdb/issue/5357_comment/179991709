IssueComment
  { issueCommentUpdatedAt = 2016 (-02) (-04) 18 : 38 : 41 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/179991709"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/5357#issuecomment-179991709"
  , issueCommentCreatedAt = 2016 (-02) (-04) 18 : 38 : 41 UTC
  , issueCommentBody =
      "I suspect that you can emulate this in most (if not all) cases by inserting `.limit(100000)` terms into your query at the right places (before the command that converts the data into an array). Though I can see that becoming a bit annoying if you're building ad-hoc queries from user requests.\r\n\r\nOne issue with truncating automatically is that things might no longer behave in the expected way.\r\nFor example if you have a non-indexed `orderBy(\"val\")`, and we simply truncate the input to 100k elements, then the first result in the output is not actually going to be the one with the smallest `val`."
  , issueCommentId = 179991709
  }