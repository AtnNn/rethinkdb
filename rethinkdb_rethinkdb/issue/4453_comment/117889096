IssueComment
  { issueCommentUpdatedAt = 2015 (-07) (-02) 03 : 06 : 44 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/117889096"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/4453#issuecomment-117889096"
  , issueCommentCreatedAt = 2015 (-07) (-02) 03 : 06 : 05 UTC
  , issueCommentBody =
      "In a first quick test I could reproduce the slow read. The latency seems to be essentially proportional to the number of concurrent write clients. At 500 concurrent writes I'm getting execution times of about 2.5 seconds for the read. At 50 writes it's about 300-400 ms.\r\n\r\n@behrad is it an option to reduce the number of concurrent update clients?\r\nWithout having looked in detail at what's going on, I suspect that RethinkDB more or less processes the queries from the different connected clients with equal priority, leaving only about 1/500 of the total throughput to the read query. I still have to check where exactly the read is blocking though."
  , issueCommentId = 117889096
  }