IssueComment
  { issueCommentUpdatedAt = 2013 (-11) (-18) 18 : 57 : 28 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/28726506"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1648#issuecomment-28726506"
  , issueCommentCreatedAt = 2013 (-11) (-18) 18 : 56 : 04 UTC
  , issueCommentBody =
      "Here's a small example to demonstrate the severity of the issue: Based on next, I set up a cluster of 32 nodes. 16 running on magneto and electro each.\r\nI set off a script to create 200 tables (just 1 shard / 1 replica each) to create a realistic usage scenario on Friday.\r\nWhen I came in this Monday, the script had timed out. The script's timeout was set to 48 hours!\r\n\r\nGoing to the web interface, I can see that 85 out of the 200 tables were successfully created before the timeout occurred. Interestingly, just pointing my browser to the web interface is enough to max out one core on the respective node. I can see the web interface for a few seconds before it states that I've been disconnected from the server. Guess even just retrieving the current cluster metadata for rendering in the web interface is too much.\r\n\r\nThere is hope though. In branch 'daniel_directory_efficiency', which incorporates optimization done by @jdoliner and myself, creating those 200 tables is no problem. Creating an additional table afterwards takes ~7 seconds (edit: actually 7s is for creating and deleting the table again. Just creating could be faster)."
  , issueCommentId = 28726506
  }