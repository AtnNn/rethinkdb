Issue
  { issueClosedAt = Just 2014 (-02) (-19) 05 : 38 : 54 UTC
  , issueUpdatedAt = 2014 (-03) (-26) 01 : 10 : 17 UTC
  , issueEventsUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/1806/events"
  , issueHtmlUrl =
      Just "https://github.com/rethinkdb/rethinkdb/issues/1806"
  , issueClosedBy = Nothing
  , issueLabels =
      [ IssueLabel
          { labelColor = "ededed"
          , labelUrl =
              "https://api.github.com/repos/rethinkdb/rethinkdb/labels/cp:testing"
          , labelName = "cp:testing"
          }
      ]
  , issueNumber = 1806
  , issueAssignee =
      Just
        SimpleUser
          { simpleUserId = Id 1461947
          , simpleUserLogin = N "neumino"
          , simpleUserAvatarUrl =
              "https://avatars.githubusercontent.com/u/1461947?v=3"
          , simpleUserUrl = "https://api.github.com/users/neumino"
          , simpleUserType = OwnerUser
          }
  , issueUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueTitle = "Automated performance regression tests"
  , issuePullRequest = Nothing
  , issueUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/1806"
  , issueCreatedAt = 2013 (-12) (-31) 05 : 15 : 17 UTC
  , issueBody =
      Just
        "We have had a couple of performance regressions. Most recently https://github.com/rethinkdb/rethinkdb/issues/1733\n\nThe new cache https://github.com/rethinkdb/rethinkdb/issues/1642 is likely to also have some impact on performance.\n\nI think it is fair to assume that we are currently missing a lot of performance problems and regressions, and that we will continue to do so unless we do something about it.\nTherefore I believe that we should get some automated performance regression tests together asap.\n\nI propose that we do something quick&dirty at first. Then, post LTS (or once we hire a second reliability engineer), we can design a better system.\n\nI imagine something of the following kind:\n- Run workload X for ~15-30 min as a quick but incomplete test of overall system performance\n- Have the following four basic scenarios:\n  - in-memory data set [1] with small (inlined) documents\n  - in-memory data set [1] with large documents\n  - out-of-memory[2] data set [1] with small documents\n  - out-of-memory[2] data set [1] with large documents\n- For each scenario, have approximately one test per ReQL term. E.g. check the performance of `count`, of `map`, of `pluck`, of `getAll` etc.\n- Maybe also some very basic tests with for each client, e.g. a table scan, inserts, and point reads or something like that in each of Ruby, Python and JavaScript\n\nThese tests should be enough to tell us when we break (performance-wise) the implementation of some term. It also implicitly tests the cache and serializer.\nFurthermore it is simple enough that we can put it together and have it running in 2-3 days. I'm thinking of a set of very simple scripts, no fancy framework, no boiler plate.\n\nWhat I think we should _NOT_ do now\n- clustering performance\n- compositions of queries (other than what's in workload X)\n- meta data things\n- web interface performance  \n- lots of different data sets\n- a comprehensive and extensible testing framework\n- fancy reporting. Just a file with QPS and/or average latency as its output.\n\nI'm literally thinking of an implementation which works for this specific set of tasks, and which we can throw away once we do it properly.\nDoing so has the following advantages:\n- We have something that avoids 70% of performance regressions.\n- We have that something very quickly.\n- It allows us to tag LTS with a clean consciousness when it comes to performance\n- It allows us to learn more about what we really need from an automated performance testing framework, so we can hopefully get it right when we design a proper one later.\n\nThoughts, suggestions?\n\n[1] The data set should be large enough to span multiple internal batches when we run range queries on them.\n[2] The out-of-memory data sets would be the same as the in-memory ones, just with a smaller cache size limit.\n"
  , issueState = "closed"
  , issueId = Id 24911884
  , issueComments = 9
  , issueMilestone =
      Just
        Milestone
          { milestoneCreator =
              SimpleUser
                { simpleUserId = Id 48436
                , simpleUserLogin = N "coffeemug"
                , simpleUserAvatarUrl =
                    "https://avatars.githubusercontent.com/u/48436?v=3"
                , simpleUserUrl = "https://api.github.com/users/coffeemug"
                , simpleUserType = OwnerUser
                }
          , milestoneDueOn = Just 2014 (-03) (-13) 07 : 00 : 00 UTC
          , milestoneOpenIssues = 0
          , milestoneNumber = 53
          , milestoneClosedIssues = 203
          , milestoneDescription = Just ""
          , milestoneTitle = "1.12"
          , milestoneUrl =
              "https://api.github.com/repos/rethinkdb/rethinkdb/milestones/53"
          , milestoneCreatedAt = 2013 (-11) (-19) 09 : 47 : 10 UTC
          , milestoneState = "closed"
          }
  }