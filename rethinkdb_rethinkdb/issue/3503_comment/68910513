IssueComment
  { issueCommentUpdatedAt = 2015 (-01) (-06) 18 : 42 : 30 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 552910
        , simpleUserLogin = N "Tryneus"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/552910?v=3"
        , simpleUserUrl = "https://api.github.com/users/Tryneus"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/68910513"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/3503#issuecomment-68910513"
  , issueCommentCreatedAt = 2015 (-01) (-06) 18 : 39 : 42 UTC
  , issueCommentBody =
      "There is no 'official' export format, but I can describe what the `rethinkdb export` tool produces.  It may be useful to add this to our docs.  Note that all code examples are given in Python.\r\n\r\nThis should be fairly clear, but if you see any mistakes or have questions or comments, let me know.\r\n\r\n#### Overview\r\n\r\nThe `rethinkdb export` tool by default will export all tables in a cluster, each to a separate `.json` file, accompanied by a similarly-named `.info` file.  Both files are in a plain UTF-8 JSON format, with no additions or extensions.  The `.json` file contains each row from the table, and the `.info` file contains table metadata.\r\n\r\nStarting with 1.16, the backup tools will ignore tables in the system database 'rethinkdb'.\r\n\r\nUsing the option `--format csv` (along with `--fields`), a `.csv` (comma-separated value) file will be produced instead.  `--format csv` can only be used to export a single table at a time.\r\n\r\n#### Directory Structure\r\n\r\nBy default, `rethinkdb export` will choose a directory name in the current working directory based on the system time, e.g. `./rethinkdb_export_2015-01-06T09:01:11`.  The export directory will have `_part` appended to the name until the export is completed, upon which it will be renamed.\r\n\r\nInside the export directory, there will be one directory for each exported database.  Each directory will have two files for each exported table, depending on the format (detailed below).  Here is an example directory structure for exporting three tables: `foo.bar`, `foo.baz`, and `test.data`:\r\n\r\n```\r\nrethinkdb_export_2015-01-06T09:01:11/\r\n    foo/\r\n        bar.info\r\n        bar.json\r\n        baz.info\r\n        baz.json\r\n    test/\r\n        data.info\r\n        data.json\r\n```\r\n\r\n#### `.info` File\r\n\r\nThe `.info` file contains the table metadata available from the `info` ReQL term.  Before 1.16, the contents of this file were equivalent to a JSON dump of `r.table(...).info()`:\r\n```json\r\n{\r\n    \"db\": {\"name\": \"foo\", \"type\": \"DB\"},\r\n    \"indexes\": [\"value\"],\r\n    \"name\": \"bar\",\r\n    \"primary_key\": \"id\",\r\n    \"type\": \"TABLE\"\r\n}\r\n``` \r\n\r\nFollowing 1.16, the indexes field was changed to use the output of the `index_status` ReQL term, which includes a binary representation of the function used to build the secondary index.  The output here is equivalent to `r.table(...).info().merge({'indexes': r.db(r.row['db']['name']).table(r.row['name']).index_status()}).run(..., binary_format='raw')`:\r\n```json\r\n{\r\n    \"db\": {\"name\": \"foo\", \"type\": \"DB\"},\r\n    \"doc_count_estimates\": [2],\r\n    \"indexes\": [\r\n        {\r\n            \"function\": { \"$reql_type$\": \"BINARY\", \"data\": \"JHJlcWxfaW5kZXhfZnVuY3Rpb24kBAICAgAAAAAAAAHt/////////3IAAAAIHxo5CAoaIggBEgsI\\r\\nAxkAAAAAAAAzwILxBA8KDQgBEP7//////////wGC8QQPCg0IARD+//////////8BGiAIARIJCAQi\\r\\nBXZhbHVlgvEEDwoNCAEQ/v//////////AYLxBA8KDQgBEP7//////////wEPAAAACg0IARD+////\\r\\n//////8BAAA=\"},\r\n            \"geo\": false,\r\n            \"index\": \"value\",\r\n            \"multi\": false,\r\n            \"outdated\": false,\r\n            \"ready\": true\r\n        }\r\n    ],\r\n    \"name\": \"bar\",\r\n    \"primary_key\": \"id\",\r\n    \"type\": \"TABLE\"\r\n}\r\n```\r\n\r\nNote that we specify `binary_format='raw'`, see the `.json` format description for why.\r\n\r\n#### `.json` File\r\nWhen exporting to the JSON format, we produce an array of dicts, each dict corresponding to a row.\r\n\r\nThe query used to dump a table is `r.db(...).table(...).order_by(index=...).run(..., time_format='raw', binary_format='raw')`\r\n\r\nThis file is ordered by the primary key of the table (particularly to allow us to resume in the case of an interrupted connection).  In addition, we specify `time_format='raw'` and `binary_format='raw'`.  This is for proper JSON serialization of types that are not supported by the JSON specification.  The 'raw' format is equivalent to what we send on the wire between the server and client, and allows us to later send the data back to the server with no modifications to recreate the original data structure.  There should be API docs detailing these formats, but to summarize: binary objects use base64, and time objects use a float for seconds since the UNIX epoch.\r\n\r\nAside from these caveats, the JSON file format is the most straightforward when importing/exporting.  The file produced is encoded using UTF-8.  With an import we can simply parse the JSON into native objects and write the rows directly to the table.\r\n\r\n#### `.csv` File\r\nCSV export/import is not meant as a backup path so much as migrating to or from other tools, due to the limitations of the format.  Because of these limitations, we have the following behavior:\r\n\r\n* When exporting, any fields which are not a string or number will be dumped as inline-JSON (e.g. objects, arrays, times, nulls, binary).\r\n* When importing, all fields will be read as a string\r\n\r\nThis means we always produce and always accept valid CSV, and we do not do anything 'unexpected' with the data.  This also means users may need to add a post-processing step when importing CSV data.\r\n\r\nExample CSV file:\r\n```\r\nid,value\r\nfoo,524\r\n```\r\n\r\nWould result in one row:\r\n```json\r\n{\r\n    \"id\": \"foo\",\r\n    \"value\": \"524\"\r\n}\r\n```\r\n\r\nThis could be post-processed to turn all `value` fields into numbers using `r.table(...).update({\"value\": r.row[\"value\"].coerce_to(\"NUMBER\")})`, resulting in:\r\n```json\r\n{\r\n    \"id\": \"foo\",\r\n    \"value\": 524\r\n}\r\n```\r\n\r\nSimilarly, more-complex fields that were saved as inline-JSON could be converted using `r.table(...).update({\"value\": r.json(r.row[\"value\"])})`.\r\n\r\nAs for the file format itself, the user must explicitly specify which fields will be exported using the `--fields` option.  Any rows that do not have the field will result in an empty field in the file.  The file produced is encoded using UTF-8.  CSV is not a well-standardized format, but we use the default `csv.writer` serializer in Python (https://docs.python.org/2/library/csv.html).\r\n\r\nExample row to be exported:\r\n```json\r\n{\r\n    \"id\": \"foo\",\r\n    \"value\": 524,\r\n    \"array\": [ 1, 2, 3 ],\r\n    \"dummy\": \"string\",\r\n    \"none\": null\r\n}\r\n```\r\n\r\nWhen exported using `--fields value,none,missing,array`, the resulting CSV file should be:\r\n```\r\nvalue,none,missing,array\r\n524,null,,\"[1, 2, 3]\"\r\n```\r\n"
  , issueCommentId = 68910513
  }