IssueComment
  { issueCommentUpdatedAt = 2014 (-07) (-22) 20 : 45 : 24 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 1777134
        , simpleUserLogin = N "mlucy"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/1777134?v=3"
        , simpleUserUrl = "https://api.github.com/users/mlucy"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/49798165"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/2721#issuecomment-49798165"
  , issueCommentCreatedAt = 2014 (-07) (-22) 20 : 45 : 24 UTC
  , issueCommentBody =
      "> So would processing the results when using a cursor.\r\n\r\nNot if you're doing work for each row.  (For example, our backup script.)\r\n\r\nAlso, just from a user-experience perspective, not blocking forever is nice during interactive use.\r\n\r\n> The main use case I think where first batch latency matters, and why we try to keep that low, is the data explorer.\r\n\r\nI might be an outlier, but I use `irb` in place of the data explorer.  I think a lot of people who are developing in a language use that same language interactively to play with their data.  (This is especially true if they're trying to debug a query, because they can paste code from their actual program.)"
  , issueCommentId = 49798165
  }