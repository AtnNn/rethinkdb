IssueComment
  { issueCommentUpdatedAt = 2014 (-08) (-13) 22 : 19 : 56 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 1777134
        , simpleUserLogin = N "mlucy"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/1777134?v=3"
        , simpleUserUrl = "https://api.github.com/users/mlucy"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/52119815"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/2721#issuecomment-52119815"
  , issueCommentCreatedAt = 2014 (-08) (-13) 22 : 19 : 56 UTC
  , issueCommentBody =
      "@coffeemug -- I still think we should do this, but I think we have to improve the array size limit at the same time.  After sleeping on this I remembered one of my original objections, which is that the array size limit is totally inappropriate when you have rows with large documents.  If someone tries to write `r([r.table('test')]).run` and they have big documents, the server is going to blow up.  As in, OOM error, boom.\r\n\r\nMarking as unsettled again because I think we need to work out how to handle cases like that before doing this.  (We might have to move off of the array size limit entirely and go based off the serialized size of the data.)"
  , issueCommentId = 52119815
  }