IssueComment
  { issueCommentUpdatedAt = 2016 (-09) (-08) 00 : 28 : 12 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/245460229"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/5494#issuecomment-245460229"
  , issueCommentCreatedAt = 2016 (-09) (-08) 00 : 28 : 12 UTC
  , issueCommentBody =
      "Here are a few open questions for the JSON-Schema support:\r\n\r\n## Setting table-level schema validation ##\r\n\r\nOptions:\r\n\r\n1. The simplest option is to just provide the `r.model(\8230).validate(\8230)` function. Users can set a write hook that contains the model and calls `validate`.\r\n2. A bit more convenient, and still very easy to do, is to add a shortcut for generating the validation write-hook function automatically. We could overload `table.setWriteHook(\8230)` to accept an `r.model` object. If it's passed a model instead of a function, it will set the write hook to a function that validates the given model.\r\n3. Finally, we could have a completely separate interface for configuring a table's schema, that doesn't rely on write hooks at all (internally it would probably still work similarly). \r\n\r\nI think 2 is going to be an easy addition, while 3 is obviously more work and also adds more complexity to the set of administrative ReQL commands.\r\n\r\n## Automatic value coercion ##\r\n\r\nAs an available option, we could support automatic coercion of some types into the type required by the schema. I think primarily this would be from strings to more specific types, namely bool, numbers and date/times. Note that the coercion rules would probably differ from our usual `coerceTo` rule when it comes to conversion to `bool`. See https://github.com/epoberezkin/ajv/blob/master/COERCION.md for a set of rules that we can consider.\r\n\r\nGenerally, a field with a specified type would first be checked against the specified type. If it is of the right type, the field is accepted. If it is of a different type, we attempt to coerce it to the appropriate type (using the reversible coercion rules). If that fails, we reject the field.\r\n\r\nIf an input validates with type coercion enabled, `model.validate(x)` will return a datum which is equivalent to `x` but with types converted as necessary.\r\n\r\nThe idea behind this is that you can pass user input directly into RethinkDB without having to convert many fields explicitly in the application first. User input will often arrive without a specific type, usually in the form of strings. So this would make that easier.\r\n\r\n## Pseudo-type handling ##\r\n\r\nJSON-schema works with the usual JSON types, but RethinkDB also has the date/time, binary, and geospatial types.\r\n\r\nThere are different ways in which we can represent these in a JSON schema.\r\n\r\n1. Add our own `type` values, e.g. `\"reql_date\"`, `\"reql_binary\"`, `\"reql_geo\"` (and probably `\"reql_geo_point\"`, `\"reql_geo_line\"`, `\"reql_geo_polygon\"`). This is probably convenient to use, but those JSON schemas couldn't be interpreted by any other JSON-Schema validator.\r\n2. Using JSON Schema references ($ref), like what @mbroadst's thinkagain does: https://github.com/mbroadst/thinkagain/tree/master/lib/types\r\n3. Using the `format` attribute for pseudo-type validation. For example one would write:\r\n    \r\n    ```\r\n    type: \"object\",\r\n    format: \"reql_geo\"\r\n    ```\r\n    \r\n    The `format` attribute is intended for \"semantic validation\": http://json-schema.org/latest/json-schema-validation.html#anchor104\r\n    At least some JSON Schema validators also allow you to register your own formats, so this would allow performing the same validation outside of ReQL (e.g. https://github.com/epoberezkin/ajv#api-addformat ).\r\n    Normally formats are bound to an associated type. On the ReQL-side, it would make most sense to use the `\"object\"` type, since pseudo-types are essentially objects. However if the schema is to be used for pre-validating data on the client or in the application, the data will not have been converted to the ReQL pseudo-type yet. To allow additional flexibility, we could simply allow any type to be used with the `\"reql_*\"` formats. For example if dates are represented as ISO8601-formatted strings in the application until they get inserted into the database, users could specify the date field with type `\"string\"` and with format `\"reql_date\"`, and then register a date format handler on the client that validates the ISO8601 format. On the RethinkDB server, the very same JSON schema would instead validate that the field carries a valid date/time pseudo-type (and would ignore the `type` value).\r\n\t\r\nBeing able to use the same schema definition for validation in another environment (outside of ReQL) is nice for example if you want to perform pre-validation of input data in your application code, or even on the client-side.\r\n\r\n## Schema migration ##\r\n\r\nWhat happens if you change the schema validation of an existing table?\r\n\r\nOptions:\r\n\r\n1. Do nothing. Existing documents might violate the new schema. You will be unable to update existing documents unless the update also makes them conform to the new schema. Any newly inserted document must adhere to the new schema\r\n2. Implement a check and refuse to apply a new schema until all documents in the table adhere to it. In practice you could switch to a \"transitional\" schema that allows both the old and the new schema, then migrate all your existing data, and finally switch to the new schema.\r\n3. Do the same as 2, but perform the transitional schema switch and the migration automatically. This would require users to provide a \"migration\" function.\r\n\r\n1 is by far the easiest to implement.\r\n\r\nBenefits of having the stricter requirement imposed by 2 and 3 are that we can potentially utilize the information from the schema in the future to optimize data access or storage (for example we can perform data compression or more efficient lookups if we know the names and types of all allowed fields in advance).\r\n\r\n--\r\n\r\nPinging @mbroadst @marshall007 @deontologician @VeXocide @mlucy @dalanmiller (and everyone else with thoughts on this)\r\n\r\nI'll add my opinion on this later."
  , issueCommentId = 245460229
  }