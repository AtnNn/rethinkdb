IssueComment
  { issueCommentUpdatedAt = 2014 (-01) (-04) 02 : 38 : 40 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/31569386"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1239#issuecomment-31569386"
  , issueCommentCreatedAt = 2014 (-01) (-04) 02 : 38 : 40 UTC
  , issueCommentBody =
      "Well, to be fair inserts used to be a lot slower too, didn't they? And backfilling is not hand-tuned to different value sizes the way the inserts are in this test. But sure, might be useful to find out what the bottlenecks in backfilling actually are.\r\n\r\nI've only tested on SSD so far. (2) is slow because there are a lot of values compared to the large-value data set. For small values you have to load ~1 block from the SSD whenever you access a document, for 16 KB values ~5. However the table with small documents has ~100 times as many of those."
  , issueCommentId = 31569386
  }