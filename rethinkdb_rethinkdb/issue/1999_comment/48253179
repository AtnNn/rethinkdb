IssueComment
  { issueCommentUpdatedAt = 2014 (-07) (-07) 23 : 10 : 19 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/48253179"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1999#issuecomment-48253179"
  , issueCommentCreatedAt = 2014 (-07) (-07) 23 : 10 : 19 UTC
  , issueCommentBody =
      "> but then couldn't I divide one by the other to get an estimate of the total number of blocks? :)\r\n\r\nHmm you are right. Though this endeavour would be partially hindered if we forced the percentage to be monotonic.\r\nWe could randomly perturbate either the percentage or the number of processed blocks to make it more difficult to reconstruct the estimated total with a high precision. We would have to be careful to pick the perturbation at the beginning of a backfill, so people don't query the progress 1,000 times in a row and reconstruct the unperturbated value by averaging.\r\nAlso there is a problem of known plaintext attacks. Let's say we add a random number to the percentage. Then at the very beginning and at the very end of the process, users could reconstruct the random number because they know the unperturbated  percentage must be 0 and 100 respectively. We would have to do something smarter than addition, though some type of statistical attack would almost always still be possible (as far as I can think).\r\n\r\n"
  , issueCommentId = 48253179
  }