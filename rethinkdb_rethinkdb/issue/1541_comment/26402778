IssueComment
  { issueCommentUpdatedAt = 2013 (-10) (-16) 09 : 07 : 53 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 1777134
        , simpleUserLogin = N "mlucy"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/1777134?v=3"
        , simpleUserUrl = "https://api.github.com/users/mlucy"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/26402778"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1541#issuecomment-26402778"
  , issueCommentCreatedAt = 2013 (-10) (-16) 09 : 07 : 53 UTC
  , issueCommentBody =
      "@presidentbeef -- turns out this is a bug, not a performance issue; when the volume of data in a single secondary-index-using `get_all` exceeds our intracluster batch size we send duplicate rows to the clients.  There are ~240 rows in that `get_all`, and our client batch size is 1000 (we're going to make this smarter -- see #1543), so you're getting ~4x as much data as you should back in the first batch.\r\n\r\nI have no idea how this issue made it into next.  We'll fix this bug and do a point release.\r\n\r\nWe should also improve our tests.  In most places, we use tiny batch sizes in debug mode, but intracluster batch sizes are fixed, so our tests never run into this case."
  , issueCommentId = 26402778
  }