IssueComment
  { issueCommentUpdatedAt = 2013 (-06) (-23) 23 : 27 : 28 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/19883640"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1064#issuecomment-19883640"
  , issueCommentCreatedAt = 2013 (-06) (-23) 23 : 21 : 53 UTC
  , issueCommentBody =
      "Note: Streaming 9 GiB of data = 2,359,296 blocks of 4 KB each will take in the order of 3.3 hours if every read involves a random access, assuming a very fast disk with a mean access time of 5 ms.\r\n\r\n3 hours can be considered close to \"never returning\".\r\n\r\nFrankly, RethinkDB is simply not a good choice for out-of-memory workloads that involve range scans on rotational drives. This is by design. The storage backend was designed for SSD, so this was an intentional sacrifice (trading table scan data locality for write data locality) anchored deep in the very concept of a log-structured data store.\r\nIt is fine for out-of-memory point gets on rotational disks, it is fine for anything on in-memory working sets, it is perfect for SSD-based systems. It is no good for out-of-memory range gets on rotational disks. Probably this should be made clear in the FAQ."
  , issueCommentId = 19883640
  }