IssueComment
  { issueCommentUpdatedAt = 2015 (-09) (-17) 20 : 31 : 26 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/141218901"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/4851#issuecomment-141218901"
  , issueCommentCreatedAt = 2015 (-09) (-17) 20 : 31 : 26 UTC
  , issueCommentBody =
      "@dalanmiller The implementation is very different. With the limit, we keep a copy of the results within the limit in memory on the server. If you do something like this for a huge table, you'll get a lot of memory overhead. It's possible that we're also enforcing the array size limit (default: 100,000 elements) for these queries, but I'm not sure."
  , issueCommentId = 141218901
  }