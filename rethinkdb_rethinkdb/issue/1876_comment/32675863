IssueComment
  { issueCommentUpdatedAt = 2014 (-01) (-18) 06 : 27 : 23 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 297060
        , simpleUserLogin = N "nviennot"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/297060?v=3"
        , simpleUserUrl = "https://api.github.com/users/nviennot"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/32675863"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1876#issuecomment-32675863"
  , issueCommentCreatedAt = 2014 (-01) (-18) 06 : 27 : 23 UTC
  , issueCommentBody =
      "@coffeemug I was just handwaving some high levels to get a discussion started. I guess I should start thinking. Note that I don't have much experience in building these sorts of protocols.\r\n\r\nThe goal is to have a RethinkDB cluster that stays available without human intervention.\r\n\r\nSuppose you have a system like Chubby: http://research.google.com/archive/chubby-osdi06.pdf or whatever name server to store all the state of rethinkdb's server/table/shards locations.\r\nTo help for the discussion, it's helpful to read http://research.google.com/archive/bigtable-osdi06.pdf (or just search for \"Chubby\" in that paper).\r\nThe doozerd API is useful too: https://github.com/ha/doozerd/blob/master/doc/proto.md\r\n\r\nSuppose that each server maintains a connection to the name server, and send a heartbeat every 10 seconds. If the connection to the name server is unresponsive for more than 10s, the server must commit suicide.\r\n\r\nSuppose you have N replicas of a shard, among them a master.\r\nSuppose we have a write ack of M.\r\nSuppose M-1 servers died / can't reach, among them is the master.\r\nWe should be able to 1) recover all the previously acknowledge writes 2) get a new master up and running.\r\nSuppose each replica maintains a connection with the name server to get events asynchronously, like \"master unreachable\".\r\nWe should elect a replica that have the most up to date data as the new master.\r\n\r\nThe first step is to detect the master failure. This can happen for two reasons:\r\n* This can be done with a lost connection to the master: In this case, a replica would register its loss by setting a path like \"db/table_name/shard_name/lost_master/replica_ip\" with a timestamp. The replica then would walk the path \"db/table_name/shard_name/lost_master\" and count how many replicas are seeing a recent lost connection issue. If it's more than N/2+1, it means that a majority of the cluster is indeed seeing the master as dead. In this case we unregister the current master.\r\n* Getting an event from the name server saying that the connection to the master was lost.\r\n\r\nAfter detecting a failure, the replica waits 20 seconds to make sure the master is no longer operating and stopped serving requests.\r\n\r\nThen, the replicas must agree on what is the latest data version. (Suppose each shard is replicated with a lampard clock so replicas could compare who's the most up to date). Each replicas write on \"db/table_name/shard_name/latest_version/replica_ip\" their version. Once written, each replica walk db/table_name/shard_name/latest_version to deduce the latest version. If the number of latest_version read is bigger than N-M+1, it means that the computed latest version is correct and everybody would agree on it.\r\nIf we never reach N-M+1, we don't assume anything, some data may be lost and manual intervention is needed.\r\n\r\nAt this point, replicas that don't have the latest version step out, because they can't be a master candidate.\r\nThe remaining candidates compete on \"db/table_name/shard_name/master\" to set their IP. The winner assumes the role of master. Other replicas learn the value of \"db/table_name/shard_name/master\" and connect to it.\r\n\r\nThis protocol is far from complete, but we should get something that is resilient to network partitions as long as M>N/2 and at least N/2+1 machines can talk together and also to the name servers. The algorithm would have to be implemented for real to have a feel of what's missing.\r\n\r\n---\r\n\r\nSo that's just master election, but it would be a good idea to use the name servers for server discovery. On an elastic setup with lots of servers (https://github.com/rethinkdb/rethinkdb/issues/1861), machines come and go, things fail often. When a new rethinkdb instance is up, it advertises itself to the name server, and when it dies the name server can do some cleanup after some timeout, or when the connection is dropped.\r\n\r\nKeeping the global state of the system in the name server is useful for application servers (like web server).\r\nThey can get the IP of a live rethinkdb instance without having to go through a bunch of hops. I'm not sure how would that translate with something like CoreOS/docker and its container approach.\r\n\r\nEven though the rethinkdb web UI is really nice, I want a very low maintenance system so I don't have to pay attention to the system too much."
  , issueCommentId = 32675863
  }