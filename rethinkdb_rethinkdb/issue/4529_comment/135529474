IssueComment
  { issueCommentUpdatedAt = 2015 (-08) (-27) 19 : 30 : 31 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/135529474"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/4529#issuecomment-135529474"
  , issueCommentCreatedAt = 2015 (-08) (-27) 19 : 24 : 43 UTC
  , issueCommentBody =
      "@VeXocide complexity I guess. But I'm not opposed to making it configurable in principle. Unfortunately we don't have anything in the protocol that allows configuring the current connection at the moment. We could either add a new command to our protocol for these purposes, or make it a cluster setting configurable through `r.db('rethinkdb').table('cluster_config')`. I feel like either option might be too much work for this at the moment.\r\n\r\n@deontologician none that I know of. It's pretty vague. I think the main risk in setting the limit too high is that someone tries doing a batch insert of a full data set at once, and it will knock out their entire cluster (because it will run out of memory, or get heartbeat timeouts because we don't yield during parsing, etc.). We know that 64 MB queries are pretty safe in this respect. I think 128 MB are still ok. I wouldn't go much higher than that from my experience."
  , issueCommentId = 135529474
  }