IssueComment
  { issueCommentUpdatedAt = 2013 (-11) (-05) 21 : 11 : 47 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 552910
        , simpleUserLogin = N "Tryneus"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/552910?v=3"
        , simpleUserUrl = "https://api.github.com/users/Tryneus"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/27812687"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1389#issuecomment-27812687"
  , issueCommentCreatedAt = 2013 (-11) (-05) 21 : 11 : 47 UTC
  , issueCommentBody =
      "I have a similar crash with different steps to reproduce.\r\n\r\n1. Start new server\r\n2. Run the stress client for some time to populate the table\r\n3. Create a sindex \"customer_id\" (for workload x data)\r\n4. While the sindex is being post-constructed, ctrl-c the server\r\n5. Restart the server and wait for post-construction to complete\r\n6. Run a getAll query on the table\r\n\r\nThe following crash is observed (and will continue to happen if the server is restarted and step 6 is executed):\r\n```c\r\nersion: rethinkdb 1.10.1-234-gfecdcd (debug) (GCC 4.6.3)\r\nerror: Error in ../src/rdb_protocol/lazy_json.cc at line 19:\r\nerror: Guarantee failed: [res == 0] disk corruption (or programmer error) detected\r\nerror: Backtrace:\r\nerror: Tue Nov  5 13:04:35 2013\r\n       \r\n       1: rethinkdb_backtrace(void**, int) at thread_stack_pcs.cc:151\r\n       2: lazy_backtrace_t::lazy_backtrace_t() at backtrace.cc:250\r\n       3: format_backtrace(bool) at backtrace.cc:197\r\n       4: report_fatal_error(char const*, int, char const*, ...) at errors.cc:68\r\n       5: get_data(rdb_value_t const*, scc_transaction_t<mc_cache_t>*) at lazy_json.cc:19\r\n       6: lazy_json_t::get() const at lazy_json.cc:26\r\n       7: rdb_rget_depth_first_traversal_callback_t::handle_pair(scoped_key_value_t&&, concurrent_traversal_fifo_enforcer_signal_t) at btree.cc:729\r\n       8: concurrent_traversal_adapter_t::handle_pair_coro(scoped_key_value_t*, adjustable_semaphore_acq_t*, fifo_enforcer_write_token_t, auto_drainer_t::lock_t) at concurrent_traversal.cc:56\r\n       9: std::_Mem_fn<void (concurrent_traversal_adapter_t::*)(scoped_key_value_t*, adjustable_semaphore_acq_t*, fifo_enforcer_write_token_t, auto_drainer_t::lock_t)>::operator()(concurrent_traversal_adapter_t*, scoped_key_value_t*, adjustable_semaphore_acq_t*, fifo_enforcer_write_token_t, auto_drainer_t::lock_t) const at functional:551\r\n       10: void std::_Bind<std::_Mem_fn<void (concurrent_traversal_adapter_t::*)(scoped_key_value_t*, adjustable_semaphore_acq_t*, fifo_enforcer_write_token_t, auto_drainer_t::lock_t)> (concurrent_traversal_adapter_t*, scoped_key_value_t*, adjustable_semaphore_acq_t*, fifo_enforcer_write_token_t, auto_drainer_t::lock_t)>::__call<void, , 0, 1, 2, 3, 4>(std::tuple<>&&, std::_Index_tuple<0, 1, 2, 3, 4>) at functional:1146\r\n       11: void std::_Bind<std::_Mem_fn<void (concurrent_traversal_adapter_t::*)(scoped_key_value_t*, adjustable_semaphore_acq_t*, fifo_enforcer_write_token_t, auto_drainer_t::lock_t)> (concurrent_traversal_adapter_t*, scoped_key_value_t*, adjustable_semaphore_acq_t*, fifo_enforcer_write_token_t, auto_drainer_t::lock_t)>::operator()<, void>() at functional:1206\r\n       12: callable_action_instance_t<std::_Bind<std::_Mem_fn<void (concurrent_traversal_adapter_t::*)(scoped_key_value_t*, adjustable_semaphore_acq_t*, fifo_enforcer_write_token_t, auto_drainer_t::lock_t)> (concurrent_traversal_adapter_t*, scoped_key_value_t*, adjustable_semaphore_acq_t*, fifo_enforcer_write_token_t, auto_drainer_t::lock_t)> >::run_action() at callable_action.hpp:28\r\n       13: callable_action_wrapper_t::run() at runtime_utils.cc:67\r\n       14: coro_t::run() at coroutines.cc:178\r\n```\r\n\r\nThis may be from the same root cause, though I'm not positive."
  , issueCommentId = 27812687
  }