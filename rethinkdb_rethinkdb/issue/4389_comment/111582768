IssueComment
  { issueCommentUpdatedAt = 2015 (-06) (-12) 18 : 31 : 59 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/111582768"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/4389#issuecomment-111582768"
  , issueCommentCreatedAt = 2015 (-06) (-12) 18 : 31 : 59 UTC
  , issueCommentBody =
      "@gebrits I think this is possible, but not super well supported at the moment.\r\n\r\nThere's a memory overhead of about 8-12 MB per table, regardless of its size.\r\n\r\nIn RethinkDB 2.0, you will also see a slowdown of meta operations (such as table creation, reconfiguration, restarting a server etc.) as you get to a lot of tables. That part is going to improve somewhat with the upcoming release (2.1), but the memory overhead remains for the time being.\r\n\r\nIf you keep the overhead in mind, you'll probably be fine with a few thousand tables in RethinkDB 2.1, especially if they are not (heavily) sharded. Note though that we haven't heavily tested this scenario, so there might be some other issues we're not aware of.\r\n\r\nYou might want to follow https://github.com/rethinkdb/rethinkdb/issues/1861 , which is basically about this scenario. Closing this as a duplicate of #1861, but feel free to ask further specific questions here."
  , issueCommentId = 111582768
  }