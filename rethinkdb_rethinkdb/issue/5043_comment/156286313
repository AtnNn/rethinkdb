IssueComment
  { issueCommentUpdatedAt = 2015 (-11) (-13) 01 : 07 : 16 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 151924
        , simpleUserLogin = N "sontek"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/151924?v=3"
        , simpleUserUrl = "https://api.github.com/users/sontek"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/156286313"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/5043#issuecomment-156286313"
  , issueCommentCreatedAt = 2015 (-11) (-13) 01 : 07 : 16 UTC
  , issueCommentBody =
      "There is something interesting going on with the asyncio stuff because it seems to think its querying ton more data than it actually is as well:\r\n\r\n![screenshot from 2015-11-12 17-02-40](https://cloud.githubusercontent.com/assets/151924/11135998/73a899d4-895f-11e5-8417-a81209cb72ba.png)\r\n\r\nThe first small line that is on there is doing a single query and it detected that it was getting 60k reads per second.   The big boost at 1.5 million/sec is using async IO, the end result from both queries is 1 million records but its odd to see such a difference.   Especially since if it thinks its getting 1.5 million rows per second the whole thing should take 1 second not 12.\r\n\r\nHere is the code:\r\n\r\n```python\r\n\r\nimport rethinkdb as r\r\nimport random\r\nimport rapidjson\r\nimport pytz\r\nimport time\r\nimport yappi\r\nfrom datetime import datetime\r\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\r\nUTC = pytz.utc\r\n\r\nYAPPI = False\r\nDO_CREATE = False\r\nDO_INSERTS = False\r\nSERVER = \"mt1-rethinkd1c1\"\r\n#SERVER = \"localhost\"\r\nTABLE = \"test5\"\r\n\r\nclass RapidJsonDecoder(object):\r\n    def __init__(self, reql_format_opts):\r\n        pass\r\n\r\n    def decode(self, s):\r\n        return rapidjson.loads(s, precise_float=False)\r\n\r\ndef create_decoder(format_opts):\r\n    return RapidJsonDecoder(format_opts)\r\n\r\nconn = r.connect(SERVER)\r\nconn._get_json_decoder = create_decoder\r\n\r\n\r\nif DO_CREATE:\r\n    r.table_create(TABLE).run(conn)\r\n\r\nSTART_BIGINT = 100000000000000000\r\nEND_BIGINT = 999999999999999999\r\n\r\n\r\ndef utc_now():\r\n    now = datetime.utcnow()\r\n    tz_now = now.replace(tzinfo=UTC)\r\n    return tz_now.timestamp()\r\n\r\n\r\ndef get_rint(start=1000000, end=9999999):\r\n    \"\"\"\r\n    Generate a very large integer\r\n    :return:\r\n    \"\"\"\r\n    return random.randint(start, end)\r\n\r\n\r\ndef get_bigint():\r\n    \"\"\"\r\n    Generate a random BIGINT\r\n    :return:\r\n    \"\"\"\r\n    return get_rint(start=START_BIGINT, end=END_BIGINT)\r\n\r\n\r\nif DO_INSERTS:\r\n    objects_to_insert = []\r\n\r\n    for i in range(0, 1000000):\r\n        objects_to_insert.append({\r\n            'survey_id': get_rint(start=1, end=15),\r\n            'respondent_id': get_bigint(),\r\n            'row_id': get_bigint(),\r\n            'column_id': get_bigint(),\r\n            'value_id': get_bigint(),\r\n            'rid1': get_bigint(),\r\n            'rid2': get_bigint(),\r\n            'rid3': get_rint(),\r\n            'now': utc_now()\r\n        })\r\n\r\n        if i % 5000 == 0:\r\n            print('writing %s' % i)\r\n            r.table(TABLE).insert(objects_to_insert).run(conn, durability=\"soft\")\r\n            objects_to_insert = []\r\n\r\n    r.table(TABLE).insert(objects_to_insert).run(conn, durability=\"soft\")\r\n\r\nstart = time.time()\r\n\r\nif YAPPI:\r\n    yappi.set_clock_type('cpu')\r\n    yappi.start(builtins=True)\r\n\r\n\r\ndef query(survey_id):\r\n    data = []\r\n    tconn = r.connect(\"mt1-rethinkd1c1\")\r\n    tconn._get_json_decoder = create_decoder\r\n    q = r.table(TABLE).filter({'survey_id': survey_id})\r\n    results = q.pluck(\r\n        \"survey_id\", \"respondent_id\", \"row_id\", \"column_id\", \"value_id\"\r\n    ).map(r.row.values()).run(tconn)\r\n\r\n    for row in results:\r\n        data.append(row)\r\n\r\n    return data\r\n\r\ndef get_multi_proc(use_threads=True):\r\n    futures = []\r\n\r\n    if use_threads:\r\n        klass = ThreadPoolExecutor\r\n    else:\r\n        klass = ProcessPoolExecutor\r\n\r\n    with klass(max_workers=4) as executor:\r\n        for i in range(1, 16):\r\n            future = executor.submit(query, i)\r\n            futures.append(future)\r\n\r\n        data = []\r\n\r\n    for future in futures:\r\n        data += future.result()\r\n\r\n    return data\r\n\r\ndef get_single():\r\n    # select all\r\n    data = []\r\n#    q = r.table(TABLE)\r\n    q = r.table(TABLE).between(1, 16, index='survey_id')\r\n#    q = r.table(TABLE).filter(\r\n#        (r.row['survey_id'] >= 1) & (r.row['survey_id'] <= 16)\r\n#    )\r\n    result = q.pluck(\r\n        \"survey_id\", \"respondent_id\", \"row_id\", \"column_id\", \"value_id\"\r\n    ).map(r.row.values()).run(conn)\r\n\r\n    for row in result:\r\n        data.append(row)\r\n\r\n    return data\r\n\r\nimport asyncio\r\n\r\n@asyncio.coroutine\r\ndef asyncio_query(future, survey_id):\r\n    data = []\r\n    tconn = yield from r.connect(\"mt1-rethinkd1c1\")\r\n    tconn._get_json_decoder = create_decoder\r\n    print(\"survey\", survey_id)\r\n    q = r.table(TABLE).filter({'survey_id': survey_id})\r\n    cursor = yield from q.pluck(\r\n        \"survey_id\", \"respondent_id\", \"row_id\", \"column_id\", \"value_id\"\r\n    ).map(r.row.values()).run(tconn)\r\n\r\n    while(yield from cursor.fetch_next()):\r\n        row = yield from cursor.next()\r\n        data.append(row)\r\n\r\n    cursor.close()\r\n    tconn.close()\r\n    print(\"survey: %s, data: %s\" % (survey_id, len(data)))\r\n    future.set_result(data)\r\n\r\ndef get_async_io():\r\n    r.set_loop_type(\"asyncio\")\r\n    loop = asyncio.get_event_loop()\r\n\r\n    futures = []\r\n    for i in range(1, 16):\r\n        future = asyncio.Future()\r\n        asyncio.async(asyncio_query(future, i))\r\n        futures.append(future)\r\n\r\n    final_futures = asyncio.wait(futures)\r\n    loop.run_until_complete(final_futures)\r\n\r\n    data = []\r\n\r\n    for future in futures:\r\n        data += future.result()\r\n    print('got all data')\r\n    loop.stop()\r\n    # hack to work around bug in rethinkdb driver\r\n    loop.run_forever()\r\n    loop.close()\r\n    return data\r\n\r\ndata = get_single()\r\n#data = get_multi_proc()\r\n#data = get_multi_proc(use_threads=False)\r\ndata = get_async_io()\r\ncount = len(data)\r\nend = time.time()\r\n\r\nif YAPPI:\r\n    stats = yappi.get_func_stats()\r\n    stats.save('callgrind.out', type='callgrind')\r\n    print('checkout callgrind.out')\r\n\r\nprint(\"count is %s\" % count)\r\nduration = int(1000 * (end - start))\r\nprint(\"int query took %sms\" % duration)\r\n\r\n\r\n```"
  , issueCommentId = 156286313
  }