IssueComment
  { issueCommentUpdatedAt = 2014 (-02) (-20) 21 : 20 : 52 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/35670279"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/1971#issuecomment-35670279"
  , issueCommentCreatedAt = 2014 (-02) (-20) 21 : 20 : 52 UTC
  , issueCommentBody =
      "@wojons: Thanks for sharing your thoughts. Something like what you describe would work very well I imagine.\r\nIn the case of backfilling I think we can get away with something easier for now. There are basically two limitations for the batch size in backfilling:\r\n- The way our code is currently structured, we can send at most all the documents in a single leaf node of our btree in one batch. This is typically something like 30-40 documents.\r\n- The other limitation is size. If each of those documents is 16 MB large, then that's already a huge chunk of data. Probably too much to send all of it at once. So we have to limit that a little."
  , issueCommentId = 35670279
  }