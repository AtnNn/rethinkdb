IssueComment
  { issueCommentUpdatedAt = 2013 (-07) (-02) 20 : 10 : 03 UTC
  , issueCommentUser =
      SimpleUser
        { simpleUserId = Id 505365
        , simpleUserLogin = N "danielmewes"
        , simpleUserAvatarUrl =
            "https://avatars.githubusercontent.com/u/505365?v=3"
        , simpleUserUrl = "https://api.github.com/users/danielmewes"
        , simpleUserType = OwnerUser
        }
  , issueCommentUrl =
      "https://api.github.com/repos/rethinkdb/rethinkdb/issues/comments/20372308"
  , issueCommentHtmlUrl =
      "https://github.com/rethinkdb/rethinkdb/issues/971#issuecomment-20372308"
  , issueCommentCreatedAt = 2013 (-07) (-02) 20 : 10 : 03 UTC
  , issueCommentBody =
      "I think if this gets implemented, it should definitely be configurable. \r\nThere are two things which I don't like about limiting the size of arrays statically:\r\n- It is difficult to rule out that there is any use case in which a really huge array might be useful. Even if it goes into the higher GBs.\r\n- People will likely not think too much about the size limit up to the point where they hit it and their queries very suddenly start failing. I see a real chance that if somebody ever hits this limit, it is going to happen in a production system. At that point, solving the issue will likely require some fundamental changes to the scheme of the data. The consequence would be very significant downtime.\r\nNow if the limit can be adjusted by the user, this wouldn't actually be a problem. The admin could raise the limit to temporarily work around the issue. While this will lead to more memory consumption, it would remove the sudden \"cliff\" between a system which is working, and a system which is just failing completely."
  , issueCommentId = 20372308
  }